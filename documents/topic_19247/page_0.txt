stefan-kaestle | 2023-04-03 08:03:42 UTC | #1

## TLDR

Currently the IC does not charge canisters for executed queries simply because it was not implemented at Genesis due to technical challenges. We would like to start exploring query charging in order to ensure that the IC charges for all consumed resources fairly.

We are aware that introducing query charging may have a large impact on dapps with heavy query traffic. We would like to proceed carefully to avoid breaking such dapps and the first step is to ask the community for feedback.

## 1. Status Quo

Currently, the IC executes [more query messages than update messages](https://dashboard.internetcomputer.org/). However, canisters pay only for updates and do not pay for queries. This contradicts the principle of fairly charging for consumed resources and introduces disbalances to the economic model of the IC. Effectively, canisters with heavy update traffic have to carry the cost of canisters with heavy query traffic. This also introduces wrong incentives to canister developers because they don’t see the cost of slow queries. Since having such disbalances is not sustainable in the long run, we would like to start exploring ways of fixing this issue.

![Screenshot from 2023-03-21 10-30-18|690x373](upload://6D4jdiR2QYkQW8QYFlJDTTuoQhJ.png)

The reason why the IC does not have query charging currently is purely technical: it was not implemented at Genesis due to technical challenges. Queries are executed in non-replicated fashion by a single node machine. This means that in order to charge a canister for executed queries, all nodes need to deterministically agree on the amount of cycles to charge, which is a difficult technical problem.

## 2. What we are asking the community

First of all, we are curious to learn what you think about the idea of introducing query charges in general. Do you agree that it is a bug in the economic model that needs to be fixed or do you think that it is better for the IC to keep the status quo? Note that if the IC community decides to keep the status quo, since it is not economically sustainable right now, it might be necessary to increase fees elsewhere, for example, fees for update calls.

Introducing query charging may drain some canisters that didn’t account for queries in their cycle burn rate. One of the goals of this post is to find ways to minimize the impact. One idea would be to introduce query charging gradually over several months such that the fees for queries increase gradually from 0% to 100%. 

If you have more such ideas, please let us know. We would also like to hear from you if you have concerns specific to your canister.

-------------------------

ZackDS | 2023-04-03 08:52:43 UTC | #2

I personally am completely fine with query charges instead of other fee increase since as I see it a new fee is always easier to justify than increasing existing ones ppl got used to, traffic is traffic after all and node resources usage should be divided equally. This would be good for existing cannisters :  
[quote="stefan-kaestle, post:1, topic:19247"]
One idea would be to introduce query charging gradually over several months such that the fees for queries increase gradually from 0% to 100%.
[/quote]
Also it would be good to do it before May's Bootcamp so it would be crucial for new developers to learn about this.

-------------------------

bytesun | 2023-04-03 15:06:49 UTC | #3

it doesn't help the eco gowning, at least now

-------------------------

lastmjs | 2023-04-03 18:56:51 UTC | #4

Seems pretty important to me to get query charging into the protocol in the long run, and the longer we wait the more we push the problem off to the future and will have to deal with that debt then.

I think some kind of gradual rollout sounds great, maybe increasing by 5-10% of the true price per month.

-------------------------

xpung | 2023-04-04 00:27:52 UTC | #5

this sounds like it would enable financial ddos etc. if i place a collect call the recipient isn't charged unless they agree

-------------------------

skilesare | 2023-04-04 00:34:34 UTC | #6

Would the queries get inspect message?

-------------------------

rckprtr | 2023-04-04 02:56:54 UTC | #7

To have this conversation we would need to have a break down of historical trends of the amount of queries a canister has received. Without this we cant estimate costs and understand impact.

-------------------------

stefan-kaestle | 2023-04-04 06:41:57 UTC | #8

[quote="xpung, post:5, topic:19247, full:true"]
this sounds like it would enable financial ddos etc. if i place a collect call the recipient isn’t charged unless they agree
[/quote]

Hi @xpung, we are aware of the issues with cycle drain attacks and are coming up with options to mitigate.

The key difference between query calls and update calls of course is that query calls are stateless. So even basic "rate limiting" the number of calls isn't possible to implement in the canister itself (as no counter value can be incremented). One option might be that the IC itself has a mechanisms for canister controllers to configure a per-machine per-canister rate limiter.

@skilesare - that's also why inspect message is not terribly useful. What logic would be in that inspect message if the query calls we want to protect cannot persist any state?

-------------------------

stefan-kaestle | 2023-04-04 06:39:57 UTC | #9

[quote="rckprtr, post:7, topic:19247, full:true"]
To have this conversation we would need to have a break down of historical trends of the amount of queries a canister has received. Without this we cant estimate costs and understand impact.
[/quote]

Hi @rckprtr - such statistics would definitely be very helpful. Making them available externally to canister developers unfortunately is also not very easy: the IC currently doesn't have a mechanisms to make such per-node statistics available to the controller.

One option might be to roll out the feature in "shadow-mode", where accounting is being done, but no cycles will be charged. With that, at least, we could aggregate per-node statistics into a single value that is deterministic on each node. That would allow some insights into the actual consumption.

This is also something we are collecting options for and we would like to address.

-------------------------

quinto | 2023-04-04 08:20:19 UTC | #10

[quote="stefan-kaestle, post:1, topic:19247"]
Effectively, canisters with heavy update traffic have to carry the cost of canisters with heavy query traffic.
[/quote]

I would disagree. At the moment nodes are paid by inflation, so essentially all ICP token holders are bearing the cost (of inflation). It is not the (developers of) canisters. In fact, we don't even know if update calls are paid sufficiently either.

[quote="stefan-kaestle, post:1, topic:19247"]
This means that in order to charge a canister for executed queries, all nodes need to deterministically agree on the amount of cycles to charge, which is a difficult technical problem.
[/quote]

Do we have any candidate solutions to this problem?

-------------------------

stefan-kaestle | 2023-04-04 08:27:49 UTC | #11

[quote="quinto, post:10, topic:19247, full:true"]

I would disagree. At the moment nodes are paid by inflation, so essentially all ICP token holders are bearing the cost (of inflation). It is not the (developers of) canisters. In fact, we don’t even know if update calls are paid sufficiently either.
[/quote]

While this might be an accurate picture of what is currently happening, there is a question on whether this needs to change. That's a big part in why we started this discussion.

[quote="quinto, post:10, topic:19247, full:true"]
Do we have any candidate solutions to this problem?
[/quote]

We have some ideas and are in the process of writing it up so that we can share it here.

-------------------------

timo | 2023-04-04 10:30:37 UTC | #12

It doesn't seem like a pressing issue currently, so I would postpone query charging. Currently it will hinder adoption. It is just one more thing that developers have to learn and manage. 

If we are not getting close to the limits anywhere where rate limiting would be required then query charging is just a distraction.

-------------------------

LightningLad91 | 2023-04-04 10:55:13 UTC | #13

I'm interested in hearing from NFT marketplace operators like @bob11. I know they've uploaded many GBs worth of assets to the network and I'm not sure if they have the infrastructure in place to keep all of these asset canisters topped up.  

Also want to plug CycleOps as an option for anyone looking to automate canister top-ups. https://twitter.com/CycleOps/status/1642813392317710338?s=20

-------------------------

stefan-kaestle | 2023-04-04 10:58:03 UTC | #14

[quote="timo, post:12, topic:19247, full:true"]
It doesn’t seem like a pressing issue currently, so I would postpone query charging. Currently it will hinder adoption. It is just one more thing that developers have to learn and manage.

If we are not getting close to the limits anywhere where rate limiting would be required then query charging is just a distraction.
[/quote]

In the designs we have envisioned so far, we have paid attention to ease of use for programmers. Previously, for example, we have considered approaches where query charging had to be configured by canister developers, but found them too complicated to use. Instead, we think it is desirable that no extra configuration is necessary from the perspective of developers. 

There have already been cases where long running queries by some canisters did affect other canisters on the same subnetwork. The reasons is that the number of query execution threads is limited and we would like to avoid wasteful computation in query calls, on order to provide fairness to other developers on the same subnetwork.

Given the long incremental roll-out, even in the best case we expect it takes at least 6 months to roll out that feature, which is why we have started to think about it now.

-------------------------

stefan-kaestle | 2023-04-04 11:00:27 UTC | #15

[quote="LightningLad91, post:13, topic:19247"]
I’m not sure if they have the infrastructure in place to keep all of these asset canisters topped up.
[/quote]

It seems very risky to operate a canister without a mechanisms to top up cycle balances regularly. Of course, with query charging, there more events that consume cycles. But even without eat, the cycle balance will reduce, e.g. for storage cost. Am I missing something?

-------------------------

LightningLad91 | 2023-04-04 11:03:19 UTC | #16

Oh, no, I don't think you're missing anything. I was just curious to hear Bob's take on the matter. I'm not familiar with how Toniq manages asset canisters. All of our (poked) NFT storage canisters are monitored and topped up by our heartbeat canister.

-------------------------

skilesare | 2023-04-04 12:27:34 UTC | #17

[quote="stefan-kaestle, post:8, topic:19247"]
@skilesare - that’s also why inspect message is not terribly useful. What logic would be in that inspect message if the query calls we want to protect cannot persist any state?
[/quote]

Inspect message would give you a place to deploy anti dos code and reject without cost(or maybe just less cost) Blocking a user abusing a heavy query based on certain criteria would be useful.  I don't honk we have access to much besides principa(which can be regenerated), but perhaps we give more info tot his function? IP, region, etc?

-------------------------

Severin | 2023-04-04 12:34:56 UTC | #18

I also think it may be useful to only allow only a select few users to run a specific query. E.g. you could guard something like `get_very_expensive_but_detailed_stats` with an `is_admin` guard function so people can't use it as an amplified DoS, even if it is only a query

-------------------------

stefan-kaestle | 2023-04-04 12:53:35 UTC | #19

Yes, similarly to what @Severin  and @skilesare have said, we could perhaps provide a per-node rate limiter where canister controllers could rate limit certain principals (or the anonymous user) to a certain maximum query rate for each function of the canister.

That seems much easier and more plausible than a `inspect_message` for query calls, where I am not sure what useful logic you could build without carrying over state from one call to another. You could basically only realize binary filters, where you always allow or always reject calls based on user, ip, geo location. What you could *not* do is count how often it happened before, so you can't even build a rate limiter in canister code yourself.

-------------------------

bob11 | 2023-04-04 16:33:43 UTC | #20

The last time I looked at it (I think I was chatting with Manu or Jan about it) it appears there are 2x as many queries as updates for us (I looked at a few subnets where the majority of canisters on the subnet are Entrepot). And query calls were going to be about half as expensive (rough estimate), which would mean charging for queries would result in a 2x in our cycles cost.

We're also still waiting on a protocol upgrade to further reduce cycles consumption (composite queries aka intercanister query calls within a subnet). And then we are working to reduce cycles consumption on our end with strategic timer setups.

My other concern (as others mentioned) is rate limiting. We have 500+ NFT canisters at this point, with many ICP services hitting these NFT canisters to check for NFT ownership, pull assets, or pull transaction history. Some of these ICP dApps hit our canisters every second (or more).

Happy to continue the discussion of course, and eventually we know this will get turned on.

-------------------------

stefan-kaestle | 2023-04-04 17:31:07 UTC | #21

[quote="bob11, post:20, topic:19247"]
which would mean charging for queries would result in a 2x in our cycles cost.
[/quote]

I would expect the cost of queries to be significantly lower than 1/2 the cost of update calls (because only one instead of 13 machines for common subnetworks is needed to execute the call). We should have more details about this very soon.

-------------------------

lastmjs | 2023-07-05 16:59:10 UTC | #22

Any updates on the latest thinking around query charging?

-------------------------

stefan-kaestle | 2023-07-06 07:13:08 UTC | #23

Hi Jordan

We are currently re-scoping this feature and are now proposing to:

1. Build a feature that allows developers to get access to query-related statistics via the `canister status` API. This is basically the entire mechanism that we envision a possible solution for query charging (a way to deterministically aggregate per-node query statistics), without actually charging for anything.

2. Hold back query charging for now.

Step 1. will hopefully help developers to learn something about the usage of query calls in their canisters and help optimize code, also in preparation for query charging.

Does this make sense?

We are planning to put up a motion proposal for voting soon.

-------------------------

stefan-kaestle | 2023-07-11 07:24:34 UTC | #24

Hi all,

here is the motion that we are thinking to put up for voting. What do you all think about it? Does this sound reasonable to everybody?

--------------------------------

# Background

Many dapps execute a significant number of query calls. Since queries are read-only and do not persist the state changes, it is difficult for the developers to keep track of executed queries. This means that currently developers do not know how many queries and instructions their canisters are executing.

# Proposal

This proposal aims to add a mechanism to the IC to deterministically aggregate per-node statistics for query execution and periodically update each canister’s status with those aggregates.

Aggregates statistics will appear in the canister status in a delayed and approximate manner due to limitations of data processing in byzantine distributed systems.

We propose to collect query statistics for both, replicated and non-replicated query calls.

# Details

This proposal will add a quadruple of query statistics to each canister’s state:

* Approximate sum of number of query calls executed
* Approximate sum of number of instructions executed by query calls
* Approximate sum of request payload size
* Approximate sum of response payload size

Counters are incremented periodically and will be initialized with 0 during canister creation. Developers can determine the rates for those counters by periodically querying them and analyzing the difference over time.

# What we are asking the community

* Vote accept or reject on NNS Motion.
* Participate in technical discussions as the motion moves forward.

-------------------------

lastmjs | 2023-07-13 16:23:23 UTC | #25

This sounds like a reasonable first step to me.

-------------------------

stefan-kaestle | 2023-07-17 13:24:20 UTC | #26

I put the motion of for voting [here](https://nns.ic0.app/proposal/?u=qoctq-giaaa-aaaaa-aaaea-cai&proposal=123481).

Thank you all for your input, here on the forum and offline and happy voting :slight_smile:

-------------------------

lastmjs | 2023-08-07 16:44:57 UTC | #27

Any update on the implementation now that the motion has passed?

-------------------------

stefan-kaestle | 2023-08-08 06:36:04 UTC | #28

Yes, we have started the implementation and the first partial MR will land on master soon.

As the changes are quite intricate and span multiple components, it will be a few months until the feature can ultimately be enabled on mainnet.

-------------------------

lastmjs | 2023-10-18 15:31:50 UTC | #29

Any updates on this? What are the next steps towards cycle charging?

-------------------------

stefan-kaestle | 2023-10-19 12:25:47 UTC | #30

Hi Jordan

The query statistics feature is coming along nicely. We have so far managed to put most of the required code in the replica, but it is still disabled by means of a feature flag.

We are now adding more tests and make sure to provide support in CDKs and dfx.

After that has all been merged, we are planning to incrementally enable it on subnets. With that, developers will be able to gain insights into query statistics. 

Note that we are not currently planning to actually charge for queries. If we ever do, most likely we would just charge cycles directly proportional to the metrics collected by this feature.

Thanks for the patience :slight_smile:

-------------------------

icpp | 2024-01-06 03:06:08 UTC | #31

I like to crosslink to this discussion, where it is requested by @jeshli to enable query charging while also enabling an increase in the instructions limit, which is blocking to run AI on chain.


https://forum.dfinity.org/t/proposal-composite-queries/15979/75?u=icpp

-------------------------

jeshli | 2024-01-06 20:59:49 UTC | #32

**Introducing an Opt-In Mechanism for Query Charging**

The discussion in this thread has revolved around whether to enable query charging for all canisters or for none. In contrast, I propose an opt-in mechanism. By integrating a system where canisters can opt-in to pay for queries, we can tailor the IC's capabilities to diverse needs and use cases, especially in the realm of advanced AI applications. Introducing options like `ic_cdk::query(cycles_fee=true)` or `ic_cdk::query(cycles_fee=true, composite_query=true)`, could responsibly increase the instruction limits per query. Such a modification would maintain robust security measures against DDOS attacks while enabling significant AI functionalities on the IC.

I am developing an open-source repository for AI inference on the IC facilitating the deployment of PyTorch/Tensorflow AI models. The instruction limit for single queries is a significant obstacle for running quality AI models. Experiments with splitting the AI model across multiple canisters and leveraging composite queries have encountered limitations due to the current instruction cap. This proposed modification to the query structure promises to overcome these barriers, enabling larger model sizes and faster response times, crucial for efficient AI inference and machine learning model training.

A preliminary empirical analysis I conducted focuses on the instruction limits for queries and updates in a local deployment setting, specifically using 32-bit floating point precision and 768 hidden dimensions. The current structure allows up to 20 billion instructions for an update but restricts queries to only 5 billion instructions. Below is a comparative analysis under these limits when using 32 bit floating point precision:

| Master Type  | Worker Type | Configuration     | Tokens | Time    | Max Tokens | Max Layers |
|--------------|-------------|-------------------|--------|---------|------------|------------|
| Composite    | Query       | Embed + 1 Hidden  | 12     | 0.156s  | 12         | f(tokens)  |
| Composite    | Query       | Embed + 2 Hidden  | 4      | 0.126s  | 4          | f(tokens)  |
|--------------|-------------|-------------------|--------|---------|------------|------------|
| Update       | Query       | Embed + 1 Hidden  | 4      | 2.069s  | 12         | 12+        |
| Update       | Query       | Embed + 1 Hidden  | 12     | 2.062s  | 12         | 12+        |
| Update       | Query       | Embed + 2 Hidden  | 4      | 2.056s  | 12         | 12+        |
| Update       | Query       | Embed + 2 Hidden  | 12     | 3.325s  | 12         | 12+        |
| Update       | Query       | Embed + 4 Hidden  | 12     | 4.788s  | 12         | 12+        |
| Update       | Query       | Embed + 12 Hidden | 12     | 11.18s  | 12         | 12+        |
|--------------|-------------|-------------------|--------|---------|------------|------------|
| Update       | Update      | Embed + 1 Hidden  | 4      | 2.049s  | 52         | 12+        |
| Update       | Update      | Embed + 1 Hidden  | 12     | 3.292s  | 52         | 12+        |
| Update       | Update      | Embed + 1 Hidden  | 52     | 8.633s  | 52         | 12+        |
| Update       | Update      | Embed + 2 Hidden  | 4      | 2.091s  | 52         | 12+        |
| Update       | Update      | Embed + 2 Hidden  | 12     | 6.588s  | 52         | 12+        |
| Update       | Update      | Embed + 2 Hidden  | 52     | 17.93s  | 52         | 12+        |
| Update       | Update      | Embed + 4 Hidden  | 4      | 3.289s  | 52         | 12+        |
| Update       | Update      | Embed + 4 Hidden  | 12     | 11.12s  | 52         | 12+        |
| Update       | Update      | Embed + 4 Hidden  | 52     | 27.28s  | 52         | 12+        |

In practice, we observe a range of context windows in AI models, from BERT's minimum of 512 tokens to the state-of-the-art Mistral's 8000+ tokens. My experiments have been on the smaller side of this range, constrained by the IC's "context window" of 52 tokens at the 20 billion transaction (tx) limit and 12 tokens at the 5 billion tx limit. Extrapolating, we could estimate that 200 billion tx would allow for a context window of approximately 520 tokens, and 500 billion tx could enable around 1200 tokens. Aiming for a context window of at least 128 tokens, which I estimate to be around 50 billion tx, would represent a significant advancement within the current limitations. Alongside these efforts, I plan to explore increasing the context window capacity by reducing the floating point precision to the lowest feasible level. This initiative, while promising, represents a significant and complex undertaking that may require considerable time to implement effectively.

While integrating GPUs is a promising direction for enhancing computational efficiency, particularly in matrix computations, it's becoming clear that increasing the query transaction limit could have a more immediate and pronounced impact on performance. This is not to diminish the importance of GPUs but to highlight that they, too, would benefit from such an adjustment in transaction limits, as they do not significantly reduce the number of operations required.

-------------------------

icarus | 2024-01-07 01:37:23 UTC | #33

@jeshli  I fully agree with the proposal for optional query charging. It maintains the status quo wrt all "small" instruction limit queries being free (for now) while allowing canister code to ask for higher query instruction limits in return for a fee in cycles. 

A question about your suggested cdk call options:
[quote="jeshli, post:32, topic:19247"]
Introducing options like `ic_cdk::query(cycles_fee=true)`
[/quote]
Would it be better to ask for a specific increased limit (in billions of wasm instructions) so that a cycle fee can be precalculated and also prevent a single "runaway" query continuing until some practical upper limit is reached?
This question might have direct impact on how this proposal would be implemented in practice. 
@stefan-kaestle could you comment on this?

-------------------------

icarus | 2024-01-07 02:06:49 UTC | #34

[quote="jeshli, post:32, topic:19247"]
I am developing an open-source repository for AI inference on the IC facilitating the deployment of PyTorch/Tensorflow AI models. The instruction limit for single queries is a significant obstacle for running quality AI models. Experiments with splitting the AI model across multiple canisters and leveraging composite queries have encountered limitations due to the current instruction cap. This proposed modification to the query structure promises to overcome these barriers, enabling larger model sizes and faster response times, crucial for efficient AI inference and machine learning model training.
[/quote]

On-chain AI and ML inference tasks are *the* prime use case for greatly increased query wasm instruction limits and therefore the need to implement a flexible canister query charging mechanism.

@jeshli thank you for providing a detailed analysis and discussion about different AI model inference requirements and mapping these back into estimates of single query instruction limit requirements to target.

As we discussed in the recent DeAI Working Group meeting (attn @icpp @patnorris @lastmjs ), providing the Dfinity IC engineering team with concrete use cases and requirements analysis will help to focus and prioritise for the whole IC development community.

@stefan-kaestle could your Dfinity engineering team provide a detailed response to this proposal and analysis by @jeshli and confirm technical feasibility and any blockers. The DeAI WG is keen to resolve this first-order problem of queries being able to execute realistic AI model inference tasks asap. There are multiple individual developers and teams working with on-chain AI inference who are spending precious development time on work-arounds instead of directing those efforts into product development.

-------------------------

icarus | 2024-01-07 02:37:22 UTC | #35

[quote="jeshli, post:32, topic:19247"]
While integrating GPUs is a promising direction for enhancing computational efficiency, particularly in matrix computations, it’s becoming clear that increasing the query transaction limit could have a more immediate and pronounced impact on performance. This is not to diminish the importance of GPUs but to highlight that they, too, would benefit from such an adjustment in transaction limits, as they do not significantly reduce the number of operations required.
[/quote]

I also agree with the above statement regarding adding both on-chain & off-chain GPU compute facility to the IC.

The greatest need for the data protection guarantees offered by the IC is for AI inference queries which will process vast amounts of private and protected information about individuals and businesses. Inference queries clearly belong on-chain using CPU and GPU computation so charging for queries with larger instruction limits and choosing single replica node query computation (default) or verified query computation (by choice) is important.

For AI model training, which typically has much higher memory, throughput and data storage requirements, the need for off-chain GPU compute will be necessary in the near term. We can discuss this in more detail over in the DeAI WG thread to keep this thread focused on query charging. However one related question for the Dfinity engineering team is who should we be asking about the goals and implementation progress for on-chain GPU compute coming in the Gen 3 replica node specification currently under development and testing at Dfinity?
The DeAI WG discussed in our last meeting the need to better understand how Gen 3 replica node GPU compute might be utilised by canister developers in the future and how the choice of physical GPUs specified for Gen 3 will align with their product development requirements wrt AI inference and model training/fine-tuning.
@stefan-kaestle could you point us in the right direction?

As this is somewhat off-topic we can take the GPU discussion over here [Technical Working Group DeAI](https://forum.dfinity.org/t/technical-working-group-deai/24621)

-------------------------

stefan-kaestle | 2024-01-08 10:55:18 UTC | #36

Hello everybody and thanks for the detailed use case.

What you are suggesting sounds definitely like a good solution to how to introduce query charging. We have internally discussed this briefly already.

As a first step, we have to finish up the work on "query statistics", which is nearly done. Query stats are a pre-requirement for query charging, as it deterministically aggregates per-node query statistics. Those statistics are then the foundation for charging for query calls.

We will get back to you very soon with detailed thoughts.

-------------------------

icpp | 2024-01-20 13:04:17 UTC | #37

Query charging would open up a world of opportunities for the type of AI apps I am working on. 

There is a whole field of research in conversational AI to create smaller models, trained on targeted datasets. Those type of models are ideally suited to run on the IC, and be used as embedded AI agents for every dApp imaginable.

The instruction limit is a blocker, while GPU is a great-to-have, but not a blocker.

-------------------------

TonyArtworld | 2024-02-10 08:18:43 UTC | #38

Hello @icpp , do you calculate compute costs in ICP or USD?

-------------------------

icpp | 2024-02-10 12:57:39 UTC | #39

Hi @TonyArtworld ,

I did not yet check how many cycles my LLM is burning per token prediction. Once I do that I can calculate the cost/token in USD, because the price of cycles is tied to XDR.

See: [Paying for Resources : Units and fiat value](https://internetcomputer.org/docs/current/developer-docs/gas-cost#units-and-fiat-value)

-------------------------

