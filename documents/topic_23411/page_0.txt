Sormarler | 2023-09-30 02:39:44 UTC | #1

I recently came across an analysis of node provider compensation versus network usage that raised interesting points about potential inefficiencies in the current system. According to the analysis, nodes are operating far below their storage capacities while node provider rewards appear high compared to cycles burned.

For example, it states that subnets currently have a ~300GB limit despite nodes supporting up to 30TB of storage. Additionally, August node provider rewards were approximately 583k ICP while only ~5k ICP were burned via cycles.

I'm sure there are good reasons for the current subnet storage limits and incentive structures. As the network matures, how are these parameters and balances evaluated and optimized? Are there plans to incrementally raise subnet storage limits as usage increases?

My goal is to better understand the rationale behind these protocol-level design decisions. I think we all want to see the Internet Computer succeed, which means constantly improving efficiency and utility alongside decentralization and security. I'd appreciate any clarification the community can provide on how we should interpret the data presented and the roadmap for continual optimization. Please let me know if I'm missing any important context.

-------------------------

Severin | 2023-10-02 10:06:33 UTC | #2

Recently there were plenty discussions about node provider rewards. Please use the search function if you're interested in those.

[quote="Sormarler, post:1, topic:23411"]
Are there plans to incrementally raise subnet storage limits as usage increases?
[/quote]

The limits get raised when it doesn't affect availability too much. The current limit is 450 or 700GB, depending on [this](https://forum.dfinity.org/t/increasing-subnet-storage-capacity-and-introducing-resource-reservation-mechanism/23447). To give you some hints why it's not easy to just raise the limit:
* The entire subnet state is hashed in some way. It is possible to cache a lot of hashing, but theoretically a lot of files can change between checkpoints
* State sync needs to be able to add/remove/update nodes to the current state relatively quickly in case a node has fallen behind or is added to the subnet
* Nearly everything about the state needs to be quickly accessible. If you're trying to run 10 queries on different canisters that use half of available stable storage you have to load/unload 10*50GB of state within <1 second

-------------------------

