skilesare | 2023-01-06 18:55:38 UTC | #1

ICDevs.org is very excited to announce a new initiative brought to you by the DFINITY Foundation and ICDevs.org. The QuickStart Dapp series will create a competition for developers to produce a number of sample dapps.  Each Bounty will be open for at least two weeks and we will award prizes to submissions as they come in and are evaluated.  There may be multiple awards based on the size/number of submissions.

The goal of this series is to produce a sizable set of sample content for the upcoming [SuperNova hackathon](https://dfinity.org/supernova/).

# QuickStart Dapp - Scaling With Canisters - #20

## Current Status: Discussion

* Open for submission - (03/28/2022)

* Closed

[Official Link](https://icdevs.org/bounties/2022/03/25/QuickStart-Dapp-Scaling-With-Canisters.html)

## Bounty Details

* Bounty Amount: 200 ICP First Prize, 100 ICP Second Prize, 50 ICP Third Prize

* Project Type: Single Contributor/Team

* Opened: 03/28/2022

* Time Commitment: Weeks

* Project Type: Sample App

* Experience Type: Intermediate - Motoko; Intermediate - Rust; Intermediate - Web

## Description

This bounty gives the opportunity to

* learn motoko

* learn rust

* learn how scaling works

* learn how to use canisters to create canisters

* learn about indexing

* learn how clients access the Internet Computer

The goal of this bounty is to produce a sample application on the Internet Computer.

Goal: Demonstrate scalability by using inter-canister calls

Create a practical dapp that has a common endpoint (primary canister) that can scale its application by creating secondary canisters and distributing requests across those canisters.

Reach Goal 1: The primary canister provides indexing information such that a client can distribute parallel calls across secondary canisters directly.

Reach Goal 2: Provide a security interface such that secondary canisters can hold private data from many users but only deliver requests to authorized requesters. Attempt to use as few inter-canister calls as possible.

Your application can be written in either motoko or rust. Further, a motoko and rust version can be submitted as seperate entries by the same person/team.

The code must be opensourced using the [MIT License](https://opensource.org/licenses/MIT).

## To submit for this bounty you should:

Create a github repo with your sample application and post the link to either the (dev forum post)[] or the (ICDevs.org dscvr portal)[https://h5aet-waaaa-aaaab-qaamq-cai.raw.ic0.app/p/icdevs].

We will start selecting prize winners by April 12th, 2022. Submission will stay open until we believe we have a sufficient number of sample applications. Multiple prizes may be awarded for submissions that reach a sufficient level of completeness.

## Bounty Completion

Once your app is complete and submitted, it will be judged on the following criteria:

* How relevant is this sample dapp for the community?

* How well is the sample dapp's functionality presented?

* Does this sample dapp help me to build enough? Can I use the sample dapp for a real project?

* How well was the sample dapp written?

* How many goals were reached?

Bonus considerations:

* Are there tests?

* Is the documentation provided (readme file on github) sufficient?

* A user interface of some kind is highly encouraged so that users of your sample application can get a visual view of how your application works.

## Funding

The bounty was generously funded by the DFINITY Foundation. Additional donations that fund the administration of these bounties can be sent to ICDevs.org. All donations will be tax deductible for US Citizens and Corporations. If you send a donation and need a donation receipt, please email the hash of your donation transaction, physical address, and name to donations@icdevs.org. More information about how you can contribute can be found at our [donations page](https://icdevs.org/donations.html).

[Other ICDevs.org Bounties](https://icdevs.org/bounties.html)

-------------------------

jzxchiang | 2022-03-29 01:05:49 UTC | #2

This is awesome and about time.

-------------------------

jzxchiang | 2022-03-29 01:06:58 UTC | #3

This might be helpful to whomever does this: https://github.com/open-ic/open-storage

-------------------------

GLdev | 2022-03-29 04:14:25 UTC | #4

Hmm, that's an awesome repo, but it seems to be missing a license?

Perhaps @hpeebles can help with that?

Also, I salute the first rust bounty from ICDevs! May there be many more in the future :slight_smile:

-------------------------

witter | 2022-03-29 09:45:23 UTC | #5

Feel free to check it https://dft.delandlabs.com/AutoScalingStorage

-------------------------

alexa.smith | 2022-03-30 22:34:49 UTC | #7

The Developer Grant Program is currently open to any and all participants who meet the eligibility requirements (which can be found in the Submittable application) and pass KYC.

-------------------------

simdi.jinkins | 2022-04-02 13:13:34 UTC | #8

Sorry to clarify, are you asking for a solution similar to the currently defunct bigmap ?

-------------------------

skilesare | 2022-04-02 15:17:12 UTC | #9

That would certainly qualify! We are targeting apps as opposed to utilities, but an app that used a generalized solution would certainly be welcome!

-------------------------

hoosan | 2022-04-11 08:19:07 UTC | #10

Hello,
I have created an application for the Bounty and want to apply.
Here is the link to Github: https://github.com/hoosan/auto-scaling-notes
I would be happy to receive your feedback.

-------------------------

skilesare | 2022-04-11 15:15:25 UTC | #11

Awesome. I will review it!  Do you have a demo deployed anywhere?

-------------------------

hoosan | 2022-04-11 22:13:39 UTC | #12

Yes, the app is deployed here: https://yflxa-iaaaa-aaaai-acfja-cai.ic0.app

-------------------------

simdi.jinkins | 2022-04-12 10:06:55 UTC | #13

Hi, when's the deadline ? Still working on my dht style solution.

-------------------------

skilesare | 2022-04-12 12:55:44 UTC | #14

We will likely keep this open for a while, but the sooner you get us a solution the better. Will likely shut things down after we have six or so submissions.

-------------------------

GLdev | 2022-04-12 21:09:55 UTC | #15

Hey @skilesare , great timing with this bounty. I had "research canister scaling" as a grant application todo, so I took a stab at it. Here's a brief description of the project. There's also a screen cap with the front-end, demoing the main concepts of the app.

Architecture: I went for the full scaling solution, where clients upload content to many buckets. The buckets create indexes based on the content they received and periodically (5s for the demo - but configurable) send the index to an Indexing canister. The Index canister instructs the front-end to upload new content to particular canisters based on the indexing strategy (more below). The Index canister also serves an index of canister IDs to the front-end, based on the indexes received from the Buckets. Thus the front-end first requests an "index" for #dogs, and then queries all the canisters where #dogs content was recorded, and displays the list.

To demo "user access" I chose a trivial top-to-bottom approach. "Moderators" are added to the Index canister (based on a principal) and the Index sends the "moderator list" to every Bucket. By default the Buckets only serve content created by the requester, or by Anonymous. If a moderator is added, however, they can view every piece of content uploaded to that Bucket.

I've used "entries" here, to denote pieces of content. This could be anything that we can track and quantize on a bucket. It could be megabytes for file storage, live sessions for proxy canisters, users served for game realms, etc. The 20 entry limit was chosen just so we can see the spawning in a live environment.

Implementation: The study is written in rust, with a react mock-up for front-end.

The canisters rely on heartbeat() functionality, as I wanted to also test this at scale. There are a ton of ways to optimize the flows, and the architecture could probably support having heartbeat() functionality just on the Index canister. Or, better yet, a dedicated heartbeat-enabled canister for the entire project.

Code organization: I tried to be as non-opinionated as possible. There are a lot of great production-ready projects out there where one could chose to get inspiration on file organization. I chose to keep it as simple as possible, so that people reading the code focus on the IC stuff and not on implementation details. A lot of things could obviously be optimized and better organized.

Both the Index and Bucket canisters have 4 main source-files. **lib.rs** deals with canister settings, and IC-related calls (queries and updates); 
**businesslogic.rs** deals with ... business logic. Here lies the main impl for most of the functionality; 
**env.rs** is an adaptation from @hpeebles' starting-project and deals with helpers for cdk API; 
**lifetime.rs** deals with pre and post upgrades and the heartbeat function.

The front-end is simply thrown together to demo the canister workflows, nothing to write home about.


Key things to note when playing with the demo (some of the things can be hopefully seen in the video below):

1. There are two indexing strategies implemented - FillFirst and BalancedLoad. The default one is Balanced. The front-end first requests a list of can_id where to upload content, and then calls the first canister in the list. We can imagine the Index canister using a multitude of indexing strategies, based on business needs. (an interesting one would be grouping content to optimize for querying as few canisters as possible on content display)

2. The Index canister maintains a number of metrics. The main thing to notice is the relation between Free Slots, Desired Free Slots and Planned Slots. Free slots are computed every 5 seconds. When Free Slots becomes lower than the Desired number, a new bucket is planned for and added to the spawn queue. We add the planned slots, so that we don't over-add too many canisters if the spawning process takes ~4-5 seconds and the heartbeat() gets called multiple times.

3. When sending multiple tags in a short time, notice that even though the Indexing Strategy wants to add content to the lowest canister, the re-indexing happens once every 5 seconds. So multiple entries will be sent to the same canister. (this turns out not to be a problem in larger deployments).

4. Adding a moderator to the Index canister will get populated to all the Buckets on the next heartbeat update, as this would probably need to be as close to synchronous as possible, since a real-life implementation would also require deleting moderators, and this approach would prevent weird edge-cases where moderators could still edit content on slow-to-update Buckets.


Play around with the demo, and let me know if there are any questions.

**Note:** please do not use this code as is in production!!! This was approached as a *scalability study,* and it is not optimized, and not tested well enough to be production ready. Feel free to use the code as you want, but please make sure it's tested and stable before using this with real stakes!


Repo:
https://github.com/GLicDEV/quickstart_scaling/

Video Demo:
https://youtu.be/2yoQ1O4rzN8

-------------------------

icme | 2022-04-13 02:22:53 UTC | #16

Great job! Thanks for the research, explanation, great video, and well organized project and code. If it was up to me I'd give you the grand prize :rofl:  (pokes @skilesare)

This gives me a bunch of new ideas I'd like to test out :)

A few follow up questions: 

**\1**. I see you're storing a `HashMap<hashtag, List<canisterId>>` in the index canister, and that the frontend is making two rounds of calls - one to query index canister to retrieve the list of storage canisters that contain a particular hashtag, and then one to query all of the storage canisters containing that specific hashtag. Did you explore any ideas of how you might scale out the index canister if the HashMap fills up?

I could see potentially having that "HeartBeat" canister you mentioned earlier turn into a management canister that holds your slot metrics and can spin up new index canisters as a second level of indirection, but now you're scaling by putting additional levels of indirection in front of the user. If you take this route you now have 3 rounds of front-end calls - one to the management canister holding the index canisters, one to all the index canisters asking for canisters with your hashtag, and then finally to the storage canisters. For now, this multiple query approach makes a lot of sense since inter-canister queries are blocked and inter-canister updates are very slow.

Wonder if you were able to ponder on this point of scaling out the index canister itself, potentially without adding additional levels of indirection.

**\2.** The tradeoff made to achieve scalable key value storage sacrifices sortability and some query depth (say getting the latest entries - sorted by timestamp, or getting all cat hashtags geotagged in a particular region). This also comes into play when I want to update a specific post that I've given the #cat hashtag, or someone else tries to like my post - with no unique identifier, which cat post gets updated?

Did you give any thought to how one might auto scale while still providing sortability and additional query depth?

-------------------------

GLdev | 2022-04-13 12:24:22 UTC | #17

Thank you for the kind words :blush: 

For the first question, that's literally on my whiteboard right now :slight_smile: I have two diagrams, one with an index of indexes and one with simply two index canisters. There are a couple of benefits of simply having two Index canisters, maybe sorted by "frequently used stuff". I believe that 8gb ought to be enough (famous last words), and worst case scenario sometimes the clients will query both index canisters (instead of always making two queries). I guess we'll have to grow and see...

Your second question is a bit more complicated, and I don't have a good answer at this point. There are a lot of factors going in how you decide to index your data, and obviously there are some tradeoffs, one way or the other. My hope for my project is that it's not a critically real-time system, thus I have the benefit of being able to schedule tasks and as long as they "eventually" complete, the system should still work. In other words, I might be able to have many indexing tasks work on each Bucket, and the main Index would just point towards them. It's a kind of both distributed storage and distributed computing. The tradeoff is of course the front-end needs to do more queries. Looking at existing projects, that seems to be a good tradeoff to make, as the IC seems uniquely positioned to serve this use case with ease. When in doubt, we'll "simply" add another layer of caching, trading space for compute time.

-------------------------

Iceypee | 2022-04-20 16:07:27 UTC | #19

Hey, as promised I updated my submission with docs, some diagrams, and cleaner code and an instructional tutorial. I will remove the old reply. Thanks! I hope its not too late

https://github.com/professionalGithub99/ScalingBounty

-------------------------

Iceypee | 2022-04-20 16:07:06 UTC | #20

The summary is from Github
The purpose of this project is to create canisters that have shared ownership. However, this is different than allowing multiple canisters to have control of the canister in the default sense. The problem is some default canister functions give full access to anyone with control (ie. there is only one level of control). A quick analogy is in a joint bank account from bank, you can have a married couple with each of their name on it but each party is able to completely withdraw funds. In a divorce, things can become messy if one party decides to withdraw everything quickly since both parties have full control. Instead, it would be nice to create a joint bank account where maybe people can vote on money spent, split evenly the amounts, divy up the percentage owned etc. In essence if the default settings for multiple users with control over a canister is like a legacy joint bank account,this program is meant to create extra sharing functionality for canisters!

I achieve this by creating a priary canister that creates canisters. This canister retains default control of the new canisters and any customized ownership is kept track in a list in the secondary canisters. Users interface with the secondary canisters through method calls from the the primary one. Essentially, the primary canister has an assoclist (dictionary) that has the users principal ids as a key and a buffer of secondary ids per principal id. This achieves project goal 1.

Primary canister provides indexing information such that a client can distribute prallel calls across secondary canisters directly.

principalids/users can join and unjoin secondary canisters as they please. (However, custom membership, limiting number of principal ids/users, etc. can be added). For now it is just join and unjoin. However, actual calling of canisters isnt from the the principalids/users themselves but from the primary canister. If we remove all control from the primary canister after enough development, the system can be quite trustless as default control would be essentially "black holed". This achieves project goal 2.

Provide a security interface such that secondary canisters can hold private data from many users but only deliver requests to authorized requesters. Attempt to use as few inter-canister calls as possible.

TO SEE CODES STRUCTURE, SEE DIAGRAMS AT BOTTM OF THIS PAGE. Essentially, main.mo is the primary canister, and it creates many instances of NodeCanisters (NodeCanisters.mo). These live in /src/Main directory.

-------------------------

GLdev | 2022-04-26 10:30:58 UTC | #21

@skilesare , any updates on this and the other bounties for the hackathon?

-------------------------

skilesare | 2022-04-26 10:34:36 UTC | #22

Office hours tomorrow.Hopefully an update today.

-------------------------

C-B-Elite | 2022-04-26 16:06:27 UTC | #23

@skilesare 
### Technical features of ICSP:

1. Infinite Capacity ICSP Canister: read and write to the same Canister without having to worry about storage space.
  * Explanation:
    * infinite capacity refers to the infinite creation of the Canister contract (in the case of a sufficient Cycle) , which supports the automatic creation of the Canister when the storage Canister is full and does not block the creation of data writes, smooth and smooth switching of storage destination.
2. Support CRUD(only support read and write at current version)
  * Explanation:
    * in the business, supports the data to add, delete and check four operations, and in the related operations on the memory of the appropriate optimization, to support the reuse of fragmented memory(Next Version).
3. One Step Store Data and Two Steps Get Data
  * Explanation:
    * One-step storage: support back-end direct: ignore await store the key and value into ICSP Canister. When store (key, value) , do not have to wait for the return value, which creates the convenience of storage.
    * Two-step get : first obtain from the ISP which storage unit is stored in, and then obtain metadata from that storage unit(bucket).
4. Cycle automatic monitoring
  * Explanation:
    * ICSP Heartbeat actively triggers the monitoring of ICSP and storage monomer Cycle balances and automatically Top Up, so the user only needs to monitor the ISP's ICP balances.

Welcome to check out !
https://github.com/PrimLabs/ICSP

-------------------------

skilesare | 2022-04-26 17:17:27 UTC | #24

[quote="C-B-Elite, post:23, topic:11756"]
https://github.com/PrimLabs/ICSP
[/quote]

Awesome and pure key/value store!  I'd love to see you replace blob with [CandyLibrary](https://github.com/aramakme/candy_library) so you can store all kinds of data!

-------------------------

borovan | 2022-04-26 19:18:56 UTC | #25

No, please don't replace anything with CandyLibrary.  The native types exist for a reason.

There's huge overhead with variant types, as we've discovered today in our project.  I couldn't think of anything worse than having a "variant type of all types"

-------------------------

skilesare | 2022-04-26 19:35:18 UTC | #26

What did you figure out?  I thought it was like two bytes as long as you kept it under 256 entries?

And how else are you doing hierarchical data?(If you are)

-------------------------

borovan | 2022-04-26 19:41:43 UTC | #27

I can't tell you exactly because it's a black box.  We replaced variants with text fields and reduced our wasm function count by 600, from 5600 down to 5000.  

It wouldn't matter if there wasn't a 6000 function cap on the wasm binary.

wasm-objdump and wasm-opt have helped us a lot

-------------------------

skilesare | 2022-04-26 19:54:17 UTC | #28

Good to know.  Thanks.  We're doing a bunch of json and whatnot and nested documents that seem to require variants of some sort.

-------------------------

C-B-Elite | 2022-04-27 02:39:20 UTC | #29

Okey, I will try it !

-------------------------

simdi.jinkins | 2022-05-06 16:14:42 UTC | #30

It's with great pleasure I present the fruits of weeks of labor: **Scaled Storage**

**Introduction**

Before starting this bounty, I knew I wanted to build a general solution that could be easily added into any rust project.

While working on the bounty, the following goals drove most decisions I made:

1. Simplicity
2. Ease of use
3. Storage efficiency
4. It should work as an imported library.

Scaled Storage is a generic distributed hash table tailored for the internet computer. It can scale to possibly infinite amount of canisters, with a worst case scenario of one inter-canister calls and usually a best case of zero (I'll got into this later). I've currently tested it to 10 canisters. The client side never needs prior knowledge of all canisters holding data, but instead just the canister id of any one of the canisters.


**Features**

1. Imported as a Rust library
2. Scales up or down depending on  developer defined behaviour.
3. “[Quine](https://en.wikipedia.org/wiki/Quine_(computing))” style canister replication. All canisters are functionally alike and use the same code.
4.  House keeping operations (migrations, scaling up and down) are abstracted away.
5. There isn’t a “primary”, “index” or “secondary” canister, any request can be taken from any canister.
6. Tries to reduce inter-canister calls.


**Developer UX**

1. Developer imports the [scaled_storage](https://crates.io/crates/scaled_storage) library
2. Copy-pastes a few predefined scaled_storage operations.
3. Uploads the canister's WASM using [ss_uploader](https://crates.io/crates/ss_uploader).


**How It Works**
Scaled Storage uses a [consistent hashing algorithm](https://en.wikipedia.org/wiki/Consistent_hashing) to map keys to their appropriate canisters.

 When a key is accessed through the exposed functions, the result is either the value or the canister where it is located, In the latter case the request is simply forwarded to that canister. Here's a code excerpt:

 
```
match canister.with_data_mut(key.clone(), |data| data.clone()) {
   NodeResult::NodeId(canister_id) => {
    //forward request to canister_id or return to client side
    }
   NodeResult::Result(result) => {
    //return result to client side
   }
}
```
Scaled Storage provides the following aptly named functions:
1. `with_data_mut`
2. `with_upsert_data_mut`

**Upgrades & Migrations**
When nodes are added or deleted, the changes are broadcast to all other nodes.

```
pub enum CanisterManagerEvent {
    NodeCreated(Principal),
    NodeDeleted(Principal),
    Migrate(MigrateArgs),
}
```

 These nodes then update their node list and hash function. The affected values that need to be migrated are converted to a generic byte array and sent to the relevant node.

```
#[derive(CandidType, Deserialize, Debug, Clone)]
pub struct MigrateArgs {
    #[serde(with = "serde_bytes")]
    data: Vec<u8>,
}

///candid file
type migrate_args = record {
    data: blob;
};

```

The consistent hashing algorithm in question is [Anchor Hash](https://docs.rs/anchorhash/latest/anchorhash/index.html) which has  "high key lookup rates,and a low memory footprint" according to the authors of its [paper](https://arxiv.org/abs/1812.09674).

This particular algorithm guarantees that given N number of nodes and K number of keys, the hash function distributes K/N number of keys to each node, achieving uniformity. It also reduces the amount of inter-canister calls necessary by:
1. Only needing to migrate values to a newly created canister (instead of having to migrate to other nodes).
2. Only needing to migrate values from a deleted canister without disrupting the location of other keys


**Architecture**
![image|585x500](upload://dFQjx0yZpiYxRH5PE9ov1sk1tJZ.png)

Scaled Storage can be described as a linked-list of nodes with each node having the following:

1. The previous node id
2. The next node id
3. A consistent-hashing function mapping keys to a canister 
4. A LIFO list of all nodes.
4. The underlying data


I chose this linked list structure over an “index node” → “secondary node” structure for the following reasons:

1. Canisters/Nodes have  exactly the same code and behaviour.
2. Minimal Responsibilities - Each node is only responsible for its subsequent node and serves as its sole controller (with the developer being the controller of the first node). This reduces surface area for any accidents and gives each node a sole scape-goat.
3. It reduces memory fragmentation - new canisters are only created after prior ones are full. Canisters cannot be removed in an arbitrary order, but must first have all subsequent nodes removed first.
4. Reduces inter-canister calls since there’s no need to check the memory usage of all canisters, when the last canister wants to upgrade, it can simply assume all other canisters are full.

**Issues and Solutions**
1. **Large binary from in-lining wasm via include_bytes!**:
 Wasm code for creating new canisters is added in heap memory at runtime via an init_wasm operation. I've created  [wasm_uploader](https://github.com/scroobius-pip/scaled_storage/tree/master/src/wasm_uploader)  for this.
2. **Payload size too large for init_wasm operation:** 
The wasm binary is chunked and sent one at a time.


```
"init_wasm":(wasm_init_args)->(bool);

type wasm_init_args = record {
    position: nat8;
    wasm_chunk: blob;
};
```


**Performance**
1. Update operations take at most two calls; one made by the end user and a second potential inter-canister call if the hash function points to another canister.
2. Query  operations take at most three calls; an additional call made by the end user.
3. Since operations can be taken from any canister, operation results could contain the id of the canister and stored/cached on the front-end. Subsequent operations could then be directly requested using the stored canister id. 

**Tests and examples**

1. [Scaled Storage Simple Scaling Test](https://github.com/scroobius-pip/scaled_storage/tree/master/src/scaled_storage_example_1)
  This demonstrates the scaling capabilities of scaled storage. The default test demonstrates 10 canisters storing simple string types.
2. [IC Snippets](https://github.com/scroobius-pip/ic_snippets)
  This is a more complex example, demonstrating storing more advanced data structures, and authentication to prevent unauthorised access:

```
         ...
        .with_upsert_data_mut(snippet_key.page_id(), |page| {
            match page.get_snippet(&snippet_key) {
                Some(snippet) => {
                    let owner = snippet.owner;
                    if owner != ic::caller() {
                        return Err("Auth Error");
                    }
               `  ...
                }
           ....
            }
        });
```

**Future updates and issues:**

1. **Consistent hashing algorithm change**


Although Anchor hash prevents unnecessary data migrations, it distributes data equally amongst nodes, typically this is a desirable effect, but for Scaled Storage this means "filled" nodes still have data inserted in them (although the possibility of that reduces as the number of nodes increases). 
A better hash function would have the following properties:


```
a. Given N nodes, only distributes keys to node N.
b. When number of nodes are increased from N to N+1, only keys in N have to be migrated to N+1.
c. When number of node are reduced from N to N-1, only keys in N are migrated to N-1
```
This would limit the number of canisters to be migrated to only one; either the prior node (downgrades), or the next node (upgrades).


2. A rewrite implemented as a finite state machine: easier to read, more bug free code since state transitions can be validated at compile-time, easier to write rollback operations when state changes error.
3. Macros to reduce the amount of boilerplate to write.
4. Downscaling hasn’t been implemented. 
5. More operations on the underlying data apart from inserting and getting data. For example map reduce operations that require data from every node.
6. Authenticate house-keeping requests between nodes. Currently it is trivial for a bad actor to pose as a newly created node, receiving migrated data.
7. **Query calls cannot be forwarded currently**, the first demo's query forwarding only works locally, pending inter-canister query calls. For other demos, the canister id  is returned to the client.
8. Cycles sent to one canister should be automatically distributed amongst all canisters.
8. Scaled storage uses heartbeat, it may be possible to avoid this and instead "hitchhike" on other requests, reducing cycle consumption.

**Summary**
There's still significant work to be done, but the foundation for a generic easy to use scaling solution has been created. Thanks for reading.
1. Scaled Storage repo: [Github](https://github.com/scroobius-pip/scaled_storage)
2. Scaled Storage library crate: [Crates](https://crates.io/crates/scaled_storage)
3. Scaled Storage uploader crate: [Crates](https://crates.io/crates/ss_uploader)
4. Canister for the simple scaling test: [qjikg-riaaa-aaaag-aaf7a-cai](https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.ic0.app/?id=qjikg-riaaa-aaaag-aaf7a-cai&did=dHlwZSBOb2RlRXJyb3IgPSB2YXJpYW50IHsKICAgIE1pZ3JhdGlvbjogdGV4dDsKICAgIFNjYWxlVXA6IHRleHQ7CiAgICBJbml0aWFsaXplOiB0ZXh0OwogICAgQnJvYWRjYXN0OiB0ZXh0Owp9OwoKdHlwZSBub2RlX2luZm9fc3RhdHVzID0gdmFyaWFudCB7CiAgICBJbml0aWFsaXplZDsKICAgIFJlYWR5OwogICAgRXJyb3I6Tm9kZUVycm9yOwogICAgU2h1dERvd247CiAgICBNaWdyYXRpbmc7CiAgICBTY2FsZVVwOwogICAgU2NhbGVEb3duOwp9OwoKCnR5cGUgbm9kZV9pbmZvID0gcmVjb3JkIHsKICAgIGFsbF9ub2RlczogdmVjIHRleHQ7CiAgICBwcmV2X25vZGVfaWQ6IG9wdCBwcmluY2lwYWw7CiAgICBuZXh0X25vZGVfaWQ6IG9wdCBwcmluY2lwYWw7CiAgICBzdGF0dXM6IG5vZGVfaW5mb19zdGF0dXM7CiAgICBjeWNsZXNfYmFsYW5jZTogbmF0NjQ7Cn07Cgp0eXBlIGluc3RhbGxfYXJncyA9IHJlY29yZCB7CiAgICBhbGxfbm9kZXM6IHZlYyB0ZXh0Owp9OwoKdHlwZSBpbml0X2NhbmlzdGVyX21hbmFnZXJfcGFyYW0gPSByZWNvcmQgewogICAgYXJnczogb3B0IGluc3RhbGxfYXJnczsKfTsKCnR5cGUgbWlncmF0ZV9hcmdzID0gcmVjb3JkIHsKICAgIGRhdGE6IGJsb2I7Cn07Cgp0eXBlIHdhc21faW5pdF9hcmdzID0gcmVjb3JkIHsKICAgIHBvc2l0aW9uOiBuYXQ4OwogICAgd2FzbV9jaHVuazogYmxvYjsKfTsKCnR5cGUgY2FuaXN0ZXJfbWFuYWdlcl9ldmVudCA9IHZhcmlhbnQgewogTm9kZUNyZWF0ZWQ6IHRleHQ7IAogTm9kZURlbGV0ZWQ6IHRleHQ7CiBNaWdyYXRlOiBtaWdyYXRlX2FyZ3M7Cn07Cgp0eXBlIG5vZGVfcmVzdWx0ID0gcmVjb3JkIHsKICAgIGRhdGE6IHRleHQ7CiAgICBmcm9tOiBwcmluY2lwYWw7Cn0KCnNlcnZpY2UgOiB7CiAgICAiZ3JlZXQiOiAodGV4dCkgLT4gKHRleHQpIHF1ZXJ5OwogICAgICJub2RlX2luZm8iOiAoKSAtPiAobm9kZV9pbmZvKSBxdWVyeTsKICAgICAiaW5pdF9jYW5pc3Rlcl9tYW5hZ2VyIjooaW5pdF9jYW5pc3Rlcl9tYW5hZ2VyX3BhcmFtKS0%2BICgpOwogICAgICJoYW5kbGVfZXZlbnQiOihjYW5pc3Rlcl9tYW5hZ2VyX2V2ZW50KS0%2BKCk7CiAgICAgImluaXRfd2FzbSI6KHdhc21faW5pdF9hcmdzKS0%2BKGJvb2wpOwogICAgICJnZXRfZGF0YSI6KHRleHQpLT4obm9kZV9yZXN1bHQpIHF1ZXJ5OwogICAgICJ1cGRhdGVfZGF0YSI6KHRleHQsdGV4dCktPihub2RlX3Jlc3VsdCk7Cn0KCg%3D%3D)

-------------------------

icme | 2022-05-07 06:11:19 UTC | #31

Curious, was this wasm function count decrease with respect to variants observed strictly when writing your code directly in Motoko, or when writing your code in Go and then using that tool to compile it to wasm?

-------------------------

icme | 2022-05-07 07:06:49 UTC | #32

First off, I want to say that I absolutely love this scaling idea and what you've done with data partitioning, especially in the context of canister reliability. **Very** cool work, and I can tell you put a lot of hard work into designing and building this!

If one can distribute their data enough that one canister has the same management capacity as all canisters, then it becomes much harder to DDOS an entire application (you may just take one or more canisters offline).

Although I love the creativity of this idea, I see several potential issues with the scalability of the design in terms of the involvement of inter-canister update and/or query calls required for all the nodes to remain in sync. The fact that multiple canisters would constantly be rebalancing with each other (using inter-canister update calls) quite often means that when under significant load the entire application would most likely slow down and get "behind" on rebalancing. 

In this situation imagine Canister A trying to update and re-balance with Canister C, Canister C simultaneously trying to re-balance with Canister B, and Canister B is trying to re-balance with Canister A. We end up with inter-canister update call 2+ second "locks" that lock up the rest of re-balancing efforts.

In a large enough application with enough of these requests coming in, I could see this eventually leading to application failure. 

A few questions.

1. When the number of nodes is increased from N to N+1, how long does the complete re-paritioning take? How does this change as we go from N=3 to N=50?

2. If you have N=10 canister nodes and you hit each canister with the exact same ~500 requests in a second, how long does it take to re-balance? What about ~1000 requests/sec?

3. Does Anchor Hashing allow me any control over knowing the exact canister node/partition that my data was sent to? For example, can I easily ensure that all data for a specific user is in a specific canister, or do I need to spin up a new scaled storage actor for each user in order to do this?

4. How is the client expected to know of and randomly pick which canister to interact with at any one time given that the management canister generates random canister ids?

-------------------------

borovan | 2022-05-07 08:22:44 UTC | #33

We were at about 5800 wasm functions (not using any optimisers at that point).  We had about 17 entities that used variants, ie..

#common; #uncommon; #rare

I changed the go generator to skip variants and just reference them by ID (like a normal database relation), then recompiled and we were at 5200.

Going through the wasm-objdump logs and grepping for #common returned way more lines than I would have expected.  It was at about 40 per variant, which makes sense if we dropped about 600 funcs.

-------------------------

simdi.jinkins | 2022-05-07 09:36:44 UTC | #34

Hi, thanks a lot for the praise. 
About re-balancing, only the last canister has sole responsibility concerning upgrades or downgrades. So it's impossible for an upgrade and downgrade to occur simultaneously. Also migrations (or re-balancing if I understood you correctly) only occur two ways: **To** the last node and **from** the last node, no other canister is affected in the re-balancing operation.

Answers
1. This needs benchmarks, i can't really say as it depends on the size of the values stored: a simple string contains far less bytes than a struct or a hashmap. Also there can be optimisation made to make sure the data migrated is chunked at little as possible; reducing the time it takes to complete a migration.
2. Rebalancing should be very rare; it happens only when a canister is filled up or is empty. Also since it's developer defined behaviour, you can only benchmark this with an actual concrete implementation. 
For example it's possible to define that the canister should be considered filled up when it has only 2GB memory remaining and empty when it has 1.9GB; this is terrible logic and nobody in their right mind would do that, but alas with great power comes....Anyways you can imagine if there are a lot of creation and deletion of keys, canisters would be created and deleted wildly but again it is the responsibility of the developer using the library to use good upgrade/downgrade logic.
3. Yes, you simply call a function with the data's key and you get the exact canister id it is located (happens in nanoseconds).
4. It's not up to the client to know anything. Ideally the system looks like one canister to the client.
To explain further, a client just needs to know one canister id, and update operations work seamlessly since the canister that the client knows, would forward the request automatically to appropriate canister.
For query requests though, due to to [query calls not being supported on the IC yet](https://forum.dfinity.org/t/inter-canister-query-calls-community-consideration/6754), requests can't be forwarded and instead the appropriate canister id is returned to the client for another query request.

A good practice would be to return the canister id of both query and update requests (Check out the second demo on how I did it). This canister id can be cached in localstorage and called later:
1. If the cached canister id points to a now deleted canister, the client can simply call the first canister  (Since it is guaranteed to always exist).
2. Even if the data has since been migrated to another canister, its new canister id is returned.

-------------------------

skilesare | 2022-05-07 12:00:18 UTC | #35

This looks fantastic. Amazing job!

-------------------------

simdi.jinkins | 2022-05-07 12:04:14 UTC | #36

**Update for the description of the second example (ic_snippets).**
The goal was to be able to serve paginated data which is somewhat difficult given we're storing data on what is basically a key value store. Here's how snippets are stored:

![image|569x500](upload://kcObLSi0z4WWNA17rcw0W7rSwgf.png)

Each page has a max number of snippets before a new page is created. So now the client can request for a page and request for prior or subsequent pages on demand

-------------------------

icme | 2022-05-07 23:07:36 UTC | #37

@simdi.jinkins  Thanks for correcting my misunderstanding of how and when re-balancing happens (when node "full" threshold is hit) and taking the time to answer my questions.

I hope you don't mind me asking few follow-ups :sweat_smile: 

(Follow up Q1)

[quote="icme, post:32, topic:11756"]
3. Does Anchor Hashing allow me any control over knowing the exact canister node/partition that my data was sent to? For example, can I easily ensure that all data for a specific user is in a specific canister, or do I need to spin up a new scaled storage actor for each user in order to do this?
[/quote]

[quote="simdi.jinkins, post:34, topic:11756"]
3. Yes, you simply call a function with the data’s key and you get the exact canister id it is located (happens in nanoseconds).
[/quote]

[quote="icme, post:32, topic:11756"]
4. How is the client expected to know of and randomly pick which canister to interact with at any one time given that the management canister generates random canister ids?
[/quote]

[quote="simdi.jinkins, post:34, topic:11756"]
4. It’s not up to the client to know anything. Ideally the system looks like one canister to the client.
To explain further, a client just needs to know one canister id, and update operations work seamlessly since the canister that the client knows, would forward the request automatically to appropriate canister.
[/quote]

I'm still not quite clear on the details of how when a new canister is spun up (we increase # of nodes from N -> N + 1 -> N + 2), how the client becomes aware not just of how many canister nodes exist and the canister node partition to send the request to, but the specific canisterId corresponding to that node.

When the management canister creates a new canister parition node, it generates a completely random canisterId along with that node, one where the application developer cannot fix, control, or predict what the canisterId will be (I wish we could do this!). 

How would a client then be able to guess this random canisterId when making requests if it has no control over the canisterId generation? There must be some "fixed" initial canisterId representing Canister A in the client application that the client can first use to make contact with the data storage solution, correct?

[quote="simdi.jinkins, post:30, topic:11756"]
Since operations can be taken from any canister, operation results could contain the id of the canister and stored/cached on the front-end. Subsequent operations could then be directly requested using the stored canister id.
[/quote]

I see this as your solution to the issue I mentioned above. The client is told a single canister that they will interact with, and they use this initial canister to then get all information about the current array of canister ids and the current hash ring that they can then use AnchorHash algorithm with on the client side. If a re-balancing happens during the middle of a client session, the client needs to re-fetch the new array canister ids and the new hash ring. 

However, this approach means that all clients are still originally directed to the same initial canister for fetching this cluster data, and that each application client will have a hardcoded canisterId that they will use, since they can't predict the canisterIds of dynamically created canister nodes.

Is this a correct assumption?

<br/>
<br/>


(Follow-up Q2)

[quote="icme, post:32, topic:11756"]
1. When the number of nodes is increased from N to N+1, how long does the complete re-paritioning take? How does this change as we go from N=3 to N=50?
[/quote]

[quote="simdi.jinkins, post:34, topic:11756"]
This needs benchmarks, i can’t really say as it depends on the size of the values stored: a simple string contains far less bytes than a struct or a hashmap. Also there can be optimisation made to make sure the data migrated is chunked at little as possible; reducing the time it takes to complete a migration.
[/quote]

About a month ago, I had these [exact same questions about data transfer speeds](https://forum.dfinity.org/t/how-long-does-it-take-to-transfer-a-gb-of-data-on-the-ic/11452) (inter-canister, ingress, egress, etc.), and the question got pretty much ghosted and went unanswered :smile:

Whether no one has been brave enough to test this out or just hasn't made their findings public, there are [multiple anecdotal comments from developers who have mentioned that downloading 1-2 MB of data from the IC can take a few seconds in this thread](https://forum.dfinity.org/t/simplest-example-of-http-streaming-chunks-in-motoko/9116/2).

By the way, consistent hashing is pretty cool - I didn't know much about it before reading your solution -> [this article](https://ably.com/blog/implementing-efficient-consistent-hashing) has a pretty neat breakdown with visualizations on how one might naively implement consistent hashing and how it works in general.

Looking into consistent hashing, it looks like for each new canister node being spun up the rebalancing cost is O(K/N), where K = the number of keys and N = the number of nodes.

This would mean that as you scale up, for each new node added the performance cost of data transfer redistribution remains constant.

<br/>
<br/>

One additional question (new)

5. Thinking about consistent hashing and the way that it further subdivides the hash ring as it scales, how does consistent hashing handle unbalanced data coming in, or more specifically how does the AnchorHash algorithm handle it? Let's say I'm seeding my scalable storage database and all of my keys are inserted in monotonically increasing order. How would that affect this architecture?

-------------------------

simdi.jinkins | 2022-05-08 12:03:06 UTC | #38

> I see this as your solution to the issue I mentioned above. The client is told a single canister that they will interact with, and they use this initial canister to then get all information about the current array of canister ids and the current hash ring that they can then use AnchorHash algorithm with on the client side. 

Not at all, the client doesn't have to know anything about the number of canisters or anything apart from the initial canister id. The hash ring is only implemented on the canisters.

> If a re-balancing happens during the middle of a client session, the client needs to re-fetch the new array canister ids and the new hash ring.

Hmm there are two cases for a re-balancing occurring:
1. **Re-balance occurs before** the client sends a request: It doesn't matter, the request would be forwarded to the appropriate canister (for update calls) or the appropriate canister's id is returned (for query calls). The whole re-balancing operation is completely transparent to the client.
2. **Re-balancing is occurring** while the client is sending a request: It shouldn't matter,  but I need to figure out a way to test this. For example it's possible a client might send a request during a downgrade operation to a soon to be removed node during the very brief period between the migration being completed and the hash ring being updated. But this can be made a non-issue by simply refusing requests and asking the client to wait during a node migration/rebalancing.

***Thanks for bringing this up.***

> However, this approach means that all clients are still originally directed to the same initial canister for fetching this cluster data, and that each application client will have a hardcoded canisterId that they will use, since they can’t predict the canister Ids of dynamically created canister nodes.

I think you may be forgetting that, all the nodes get their hash-ring updated after any re-balancing. So any request from the client to any node still gets sent to the appropriate canister id.

> 5. Thinking about consistent hashing and the way that it further subdivides the hash ring as it scales, how does consistent hashing handle unbalanced data coming in, or more specifically how does the AnchorHash algorithm handle it? Let’s say I’m seeding my scalable storage database and all of my keys are inserted in monotonically increasing order. How would that affect this architecture?

Hmm, I'm not sure if I interpreted your question correctly, but if you're asking how the keys are distributed then the answer is evenly, for one node all the keys map to that node, for two 50% map to a single node, for three 33%..

***Did I get that correct ?***

-------------------------

skilesare | 2022-05-08 12:27:47 UTC | #39

[quote="simdi.jinkins, post:38, topic:11756"]
Hmm, I’m not sure if I interpreted your question correctly, but if you’re asking how the keys are distributed then the answer is evenly, for one node all the keys map to that node, for two 50% map to a single node, for three 33%…
[/quote]

I think the question is that if it 50/50 on two and all keys are increasing ints(say a time stamp), do they all end up going to the same canister?

-------------------------

simdi.jinkins | 2022-05-08 12:49:21 UTC | #40

Most likely not, key distribution is unpredictable. If you want to group keys together in the same canister you would need to use a higher level data structure that stores multiple keys instead of using the keys themselves.

-------------------------

skilesare | 2022-05-08 13:37:12 UTC | #41

Does the library support certification? It would be interesting to see how one would go about splitting and distributing merkle trees.

-------------------------

simdi.jinkins | 2022-05-08 18:11:27 UTC | #42

 i've got a lot to learn about how certified data works but there is some [literature](http://www.cse.iitm.ac.in/~pkarthik/approxbc.pdf) on hash table alternatives to meckle trees

-------------------------

skilesare | 2022-05-16 19:31:16 UTC | #43

This Bounty is now Closed! Results soon!

-------------------------

Mercury | 2023-07-27 15:55:29 UTC | #44

Hi, is anyone aware of which one(s) of these five winning projects made it into production as is?

-------------------------

