lastmjs | 2024-02-20 03:23:28 UTC | #1

Hello devs and DFINITY engineers!

This forum thread is meant to be a place where devs can post every time they run into the instruction limit. The purpose is to show to DFINITY how important raising that limit is while also providing an opportunity for feedback on improving code/practices to perhaps overcome the limit in extra-protocol ways.

I invite all devs whenever they hit this limit to please post a small amount of details describing what caused you to hit the limit.

You'll know you've reached the limit when you see something like this: `Replica Error: reject code CanisterError, reject message IC0522: Canister bkyz2-fmaaa-aaaaa-qaaaq-cai exceeded the instruction limit for single message execution., error code Some("IC0522")`

-------------------------

lastmjs | 2024-02-20 03:26:18 UTC | #2

I just hit the instruction limit!

I have a simple Express app setup in Azle that uses the static middleware to serve static files. This has generally been working very well for simple frontends.

I have loaded a ~5.3 MiB into the Azle filesystem into the static directory, and I tried to load it with an `audio` HTML element...well, the first thing that happened when the browser tried to the load the audio file with a `GET` request (`GET` requests are treated as queries by the Azle Server Canister) was to return `Replica Error: reject code CanisterError, reject message IC0522: Canister bkyz2-fmaaa-aaaaa-qaaaq-cai exceeded the instruction limit for single message execution., error code Some("IC0522")`.

Major bummer...

-------------------------

lastmjs | 2024-02-20 03:43:39 UTC | #3

Right now it looks like I can work on the range requests, I thought they would be automatic but I might need to do some configuration to ensure that the entire file isn't returned in one request...it would still be nice if this entire file could be processed in one request ideally though.

-------------------------

ulan | 2024-02-20 08:18:48 UTC | #4

Hey Jordan. Thanks for starting this thread. I am looking forward to see more real world cases.

As we were discussing offline. In some cases, hitting the instruction limit is not a root cause, but rather a symptom of the underlying performance problem. 

Let's take your example of serving a 5.4MB audio file. That is **5.4 million** bytes. Let's be very generous and say that the program needs **100 instructions for each byte** (I would actually expect 10 instructions per byte). This gives us **540 million instructions**.

However, the program has hit the limit of **5 billion instructions**. That is 10x higher than an already super generous estimate.

I would say let's investigate what causes the program to use 5B instructions. There seems to be some performance bottleneck. The first step to start would be to see how many instructions does the program need to complete. You can check that by calling the same function in an update method, since updates have 20B instruction limit. I would also suggest to use a performance counter. If you share the code, I can also take a look when I get free time.

In this particular case, blindly raising the limit would not do any good for the developer and the users because executing 5B instructions takes 2 second. So the server that the developer is writing would have throughput of 0.5 requests per second per thread.

-------------------------

ielashi | 2024-02-20 09:05:38 UTC | #5

And, shameless plug, we've developed [canbench](https://github.com/dfinity/canbench/) to help analyze performance bottlenecks such as this. It currently supports Rust, but supporting other languages is quite simple (on the order of days of work, I'd guess).

-------------------------

icarus | 2024-02-20 10:11:20 UTC | #6

I reckon some of the IC devs working on LLM inference in a canister would have clear examples of hitting the wasm instruction limit that they could reproduce and share details about. 
I a thinking of you @icpp @jeshli @hokosugi and several others who have shared their work in the DeAI Working Group meetings

-------------------------

timo | 2024-02-20 12:19:03 UTC | #7

How does Azle work? You have a VM running in a canister?

-------------------------

lastmjs | 2024-02-20 13:54:00 UTC | #8

JavaScript interpreter (QuickJS) running in a canister at the core.

-------------------------

skilesare | 2024-02-20 14:30:23 UTC | #9

Being able to just run code from anywhere on the IC as a distributed app is a panacea for those of us working with the IC. I've made the fundamental assumption that it is inherently not possible given the compute pattern the IC uses. 

Software written outside the IC makes fundamentally different architectural assumptions. You'll never be able to run 'the best' database out there on the IC because it makes assumptions about linear compute availability and writes its indexers and supporting protocols in a way that requires us to fundamentally rewrite most of the pieces to fit the IC.  

Given that most software(outside of known quantity compute tasks) needs to be rearchitected to fit into the IC paradigm I've been focusing my time on fundamental architectural pieces that make it easier to write general-purpose programs in the round-based architecture that the IC is going to demand(as compute demand will always outstrip consensus restricted by the speed of light and available bandwidth).  Building a "New Internet" likely requires stripping down to the wall studs and not bringing the old bad habits with us.

I'd define my base assumption as: *There are very few future universes where IC survives another 40 years, but in almost all of them the software running on it is written in a language and set of frameworks '***shaped like the Internet Computer***' that were purposely architected for the platform.*

Given this assumption, there is far more value in steering the thread of time through this small eye of a needle by focusing on the base infrastructure that makes that possible rather than trying to shove the elephant of everything in cargo, npm, pip, and github through it.

(Don't despair though as we humans have been in the business of needle threading since we've been around...I think we can do it.)

Or maybe more applicable to this thread so far, instead of trying to run express we should be trying to build a web server that ***cannot*** violate the cycle limit. This seems easier to do from the ground up rather than refactoring the pieces of express that don't fit the pattern.(although we should borrow the good ideas liberally...and AI may 100x reduce the time necessary to do this over the next couple of years.).

I'm absolutely ecstatic that teams like Demergent are pushing the envelope and making different assumptions because I'm probably wrong and I want to keep working on the IC even if it surpasses my best guesses. ðŸ˜‚

-------------------------

lastmjs | 2024-02-20 15:03:37 UTC | #10

Thanks for the comment and that little part at the end haha!

I fundamentally disagree with this point of view as a guiding principle, and I want to see the IC upgrade itself to handle the world's software mostly as it already exists, otherwise it will fail in its loftiest visions, which are to run the world's software.

The amount of pain and development time and learning curve slope required to rewrite everything for the IC is IMO a non-starter or just a very bad problem to have.

I will continue to push for the IC to do what it was promised to do essentially from the beginning, and that is to be an unbounded virtual machine, that's the vision I was sold and that I signed up for.

-------------------------

Zane | 2024-02-20 22:00:58 UTC | #11

100% agree. 
The significant blocking barrier of having to rewrite everything with a new paradigm, along with the time and monetary investment it'd require, is on its own a major impediment for widespread adoption. 
But even if it weren't the case, imho there is no way the IC can realistically even come close to running the world's software with its current limitations.
The scaling solutions offered still aren't enough to provide for all use cases and even in the scenarios where they might be, the rate at which the network would need to scale in terms of nodes is unsustainable.

-------------------------

Samer | 2024-02-20 16:38:52 UTC | #12

[quote="lastmjs, post:10, topic:27700"]
to handle the worldâ€™s software mostly as it already exists,
[/quote]

Hey Jordan :slight_smile: perhaps a little off topic

What do you think about the notion that the current software stack has become increasingly complex due to several decades of incremental improvements and integration of many components and platforms and the idea to have another "swing at the ball" with a system like the IC?

On topic:
Everytime I reached the limit, I resolved it by making the code more efficient

-------------------------

skilesare | 2024-02-20 16:39:42 UTC | #13

[quote="lastmjs, post:10, topic:27700"]
unbounded virtual machine
[/quote]

Hmm....well I think it is that, but the opcodes don't necessarily line up with what runs on an AWS server or your laptop. The biggest issue is solving consensus. If you want consensus on your updates you're bound by the speed of light.  If you don't want consensus then you can just run a standard app and db servers behind a load balancer.  Unless I'm mistaken about time-slicing, you're currently blocking all other threads if just run a linear process. Even if you do 32x time-slicing you block updates on that canister for the duration.  I don't know if parallel time-slicing is even on the table.  Of course, this all begs the question about whether consensus is necessary with zk stuff, and maybe(hopefully) there are some answers in there.

> The amount of pain and development time and learning curve slope required to rewrite everything for the IC is IMO a non-starter or just a very bad problem to have.

A year ago I was a lot more pessimistic about this proposition than I am today.  There is likely an AI solution here.  "Break this module compute into committable chunks that take up no more than 1/10 of a processing round given what you know about the Internet Computer"

* **magic happens**  
* **fix the AIs stupid bug** 
* **profit**

-------------------------

lastmjs | 2024-02-22 07:05:41 UTC | #14

Okay in this case I have mostly overcome the issue. I've since implemented range requests and done some optimization of how Azle handles responses, and now I can handle ranges up to 3 MiB (which is strange because I thought the message limit was 2 MiB, maybe the response limit is a bit higher?), all from within a query call.

-------------------------

dsarlis | 2024-02-23 08:32:43 UTC | #15

> which is strange because I thought the message limit was 2 MiB, maybe the response limit is a bit higher?

Response limit is only higher, at 3MiB as you observed, for query calls (i.e. non-replicated execution).

-------------------------

lastmjs | 2024-02-23 13:06:03 UTC | #16

Okay, it would be great for this page to reflect that then? It says response limit is just 2 MB, so it should differentiate between replicated and non-replicated: https://internetcomputer.org/docs/current/developer-docs/smart-contracts/maintain/resource-limits

So replicated queries would have the 2 MB limit?

-------------------------

dsarlis | 2024-02-23 13:45:09 UTC | #17

I can make sure the page reflects that, that's a good point.

> So replicated queries would have the 2 MB limit?

Yes, correct.

-------------------------

lshoo | 2024-03-18 08:01:08 UTC | #18

I encountered the same issue in the `rgbonic` project code, which is the IC5022 error when importing  RGB20 asset to  Stock. 
How can I increase the instruction limit to address this? Since Stock is an initialization operation and cannot be split into instructions, does anyone have any suggestions? Thank you.
[the code](https://github.com/lshoo/rgbonic/blob/main/actors/rgb/src/lib.rs#L28

[got-ic5022-error-when-run-rgb-on-ic](https://forum.dfinity.org/t/got-ic5022-error-when-run-rgb-on-ic/28556) is here

-------------------------

lastmjs | 2024-03-21 03:26:44 UTC | #19

I just hit the instruction limit!

A ~600 KiB file in an Express get request (query method under-the-hood) using the static middleware, in Azle, is causing the instruction limit to be reached for some reason. I haven't tracked down why yet.

-------------------------

lastmjs | 2024-03-21 03:32:06 UTC | #20

It seems like this is actually just a relic of something we had already resolved referenced earlier in this thread, we just haven't released the new version of Azle yet that resolves this, thus it was hit by someone in the wild. We should be good but I will update if it's still a problem.

-------------------------

lastmjs | 2024-06-11 17:36:24 UTC | #21

I just hit the instruction limit with dfx 0.20.1 and azle 0.20.1 (whoa, cool version coincidence).

This is a great example of the major blocker the instruction limit can be, as I am trying to use the `motoko` npm package, calling into its API. To overcome the limit as an end-user developer might be practically impossible, as it would require improvements to ICP, Azle, or the `motoko` npm package.

Here's the code:

```typescript
import { Server } from "azle";
import express from "express";

export default Server(() => {
  const mo = require("motoko");

  const app = express();

  app.post("/compile", (req, res) => {
    mo.write(
      "/main.mo",
      `
          actor {
              public query func helloWorld(): async Text {
                  "Hello World!"
              };
          }
      `
    );

    const result = mo.wasm("/main.mo", "ic");

    res.send(result.wasm.length);
  });

  return app.listen();
});
```

Calling the `/compile` endpoint gives in part: `Replica Error: reject code CanisterError, reject message Error from Canister bkyz2-fmaaa-aaaaa-qaaaq-cai: Canister exceeded the instruction limit for single message execution`.

When executing this during post_upgrade the instruction limit was not reached.

-------------------------

seiferxiii | 2024-07-18 00:24:47 UTC | #22

Hello there, I am experiencing the same issue. I have reached the instruction limit, even though the code is not very complex.

![code_20240717_142910_via_10015_io|307x500](upload://5QzZs9zv6V2i02f5Mibjzr9bYQJ.jpeg)

Code explanation:
1. This is an API that validates the `initData` from the Telegram web app to check if the hash is legitimate.
2. If the hash is legitimate, it will be used as the "password" and hashed using bcrypt, then updated in the database.
3. Respond to the user.

Does this mean the process of few loops, two HMACs (secret and Telegram hash generation), querying the database, and then updating the data is too much?

-------------------------

mycelia | 2024-07-18 02:12:11 UTC | #23

There's clearly an issue if we're hitting computation limits. Are these tools even necessary? ICP should be built with tools that don't hit instruction limits.

-------------------------

ulan | 2024-07-19 08:09:52 UTC | #24

> Does this mean the process of few loops, two HMACs (secret and Telegram hash generation), querying the database, and then updating the data is too much?

If you would write the same code in Rust or Motoko, I think it would be ~~100x - 1000x~~ 10x - 100x times faster. (Edit: benchmarks show 10x - 100x).

The issue is that interpreting JavaScript currently has high performance overhead and AFAIK no one is looking into that.

-------------------------

lastmjs | 2024-07-18 14:14:57 UTC | #25

My suspicion is the crypto operations in this case, especially since we're using crypto-browserify under-the-hood.

Wasi-crypto would allow us to use a much more performant version of Node's crypto module.

-------------------------

lastmjs | 2024-07-18 14:42:46 UTC | #26

[quote="ulan, post:24, topic:27700"]
100x - 1000x times faster
[/quote]

Are you thinking this for general JS interpretation or just for crypto operations?

At least the QuickJS benchmarks put it about 35x less performant than JIT V8.

-------------------------

ulan | 2024-07-19 08:37:37 UTC | #27

> Are you thinking this for general JS interpretation or just for crypto operations?

I meant it for general JS interpretation, but crypto operations are probably at the top range.

Note that the numbers I wrote were based on my intuition from seeing a few examples. Maybe my impression was biased due to the slow JS candid implementation.

To get more concrete numbers, I did a small experiment: a program that computes `sum((i % 100)^2)` for all `0 <= i < 1M`. The expression was chosen to make it difficult for the compiler to optimize it away.

Here are the results:

- Rust: instructions: 13_761_694, sum: 3_283_500_000
- Motoko: instructions: 92_001_970, sum: 3_283_500_000
- Azle: instructions: 1_341_172_853, sum: 3_283_500_000

In this experiment Azle is 97x slower than Rust and 14x slower than Motoko.

I corrected `100x-1000x` to `10x-100x` in my post based on these results.

> At least the QuickJS benchmarks put it about 35x less performant than JIT V8.

Assuming that V8 is close to native, I wonder if there is a factor of ~3x that's missing here?

Attaching the source code of programs in case anyone wants to double check.

```
#[ic_cdk::query]
fn bench() -> String {
    let mut sum: i64 = 0;
    for i in 0..1_000_000 {
        sum += (i % 100) * (i % 100); 
    }
    format!("instructions: {}, sum: {}", ic_cdk::api::performance_counter(0), sum)
}
```

```
import IC "mo:base/ExperimentalInternetComputer";
import Nat64 "mo:base/Nat64";

actor {
  public query func bench() : async Text {
    var sum : Nat64 = 0;
    var i: Nat64 = 0;
    while (i < 1_000_000) {
       sum += (i % 100) * (i % 100);
       i += 1;
    };
    let instructions = IC.performanceCounter(0);
    return "instructions: " # Nat64.toText(instructions) # ", sum: " # Nat64.toText(sum);
  };
};
```

```
import { IDL, query, update, instructionCounter } from 'azle';

export default class {
    @query([], IDL.Text)
    bench(): string {
        let sum = 0;
        for (let i = 0; i < 1_000_000; ++i) {
            sum += (i % 100) * (i % 100);
        }
        let instructions = instructionCounter(0);
        return `instructions: ${instructions}, sum: ${sum}`;
    }
}
```

-------------------------

lastmjs | 2024-07-19 12:39:54 UTC | #28

Awesome thanks for going through the effort! For anyone who is interested Demergent Labs has some ideas for closing the performance gap over time, including using SpiderMonkey or V8 potentially in the future, looking into JIT in Wasm, and looking into JS compiled into Wasm directly.

Swapping QuickJS for SpiderMonkey is the most promising of those options, but it's unknown how much better the performance will be.

Apparently JIT in Wasm is either very difficult or impossible (hoping it's not impossible).

V8 is apparently so complicated it would be hard to compile it into Wasm/Wasi.

What's very interesting is that this project which compiles JS directly into Wasm is having surprisingly excellent performance results: https://github.com/CanadaHonk/porffor

-------------------------

rossberg | 2024-07-19 13:26:09 UTC | #29

Hi Ulan, interesting numbers. Did you try Motoko with `*%` and `+%=` operators, which avoid the overflow check?

-------------------------

kristofer | 2024-07-19 13:28:56 UTC | #30

Great to see those figures, thanks! 

In my proj, [Câ€“ATTS](https://catts.run/) I am running JS based "recipes" in the Rust based canister using [javy](https://crates.io/crates/javy) (QuickJS). But perhaps I should reconsider that choice. Slight tangent from the topic of this thread, but would it be difficult to run compiled WASM binaries in the canister? In my case, the recipes could as well be created using assemblyscript and compiled before use.

-------------------------

lastmjs | 2024-07-19 13:33:13 UTC | #31

Easy to run basic Wasm binaries: https://crates.io/crates/wasmi

We have Wasmi integrated into Azle, not with full general host imports yet, but it works with basic binaries.

I'm not sure on the performance trade-off yet for running Wasmi in Wasm, but I feel optimistic.

-------------------------

kristofer | 2024-07-19 14:01:57 UTC | #32

Ok, just tried Wasmi. Setup a new dfx project and copypasted from Wasmi docs example. Worked great, gzipped canister weighs in at 700k. Promising!

-------------------------

lastmjs | 2024-07-19 14:06:22 UTC | #33

I would love to see some performance numbers if you can!

-------------------------

ulan | 2024-07-19 16:09:11 UTC | #34

That gives a good speedup!

```
sum +%= (i % 100) *% (i % 100);
```
instructions: 57_001_970, sum: 3_283_500_000

-------------------------

kristofer | 2024-07-19 23:59:08 UTC | #35

@lastmjs @ulan 

Rust based wasm run by wasmi in the canister:

instructions: 205_812_831, sum: 3_283_500_000

- 15x slower than Rust
- 3,5 x slower than Motoko
- 6,5 x faster than JS

I believe there might be ways to optimise the generated wasm. Haven't looked into that and don't know if it applies to this kind of thing. But, the [wasmi usage guide](https://github.com/wasmi-labs/wasmi/blob/main/docs/usage.md) mentions it.

```rust
#[no_mangle]
pub extern "C" fn run() -> i64 {
    let mut sum: i64 = 0;
    for i in 0..1_000_000 {
        sum += (i % 100) * (i % 100);
    }
    sum
}
```

https://github.com/kristoferlund/ic-wasmi-benchmark

-------------------------

kristofer | 2024-07-20 21:32:01 UTC | #36

I started a new thread for a related topic. The wasmtime execution environment used to run the canisters should expose functionality accessible through `ic_cdk` to let us run wasm from wasm. So we don't have to run a wasm runtime inside a wasm runtime like in my benchmark test.

https://forum.dfinity.org/t/idea-ic-cdk-should-let-us-run-wasm-in-wasm/33442

-------------------------

