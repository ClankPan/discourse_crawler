dieter.sommer | 2024-05-16 12:26:20 UTC | #1

# Internet Computer Roadmap ‚Äî Year 4 And Beyond
DFINITY‚Äôs contributions to the Internet Computer's roadmap has just been revealed on the [internetcomputer.org](https://internetcomputer.org/roadmap) web site. The roadmap outlines the key work items that have been defined to progress ICP technology for the Year 4 of ICP and beyond, towards the ultimate goal of having a World Computer.

The roadmap currently reflects mainly DFINITY's views and items that the foundation plans to work on. However, a good number of features have actually been requested by the community and milestones encompass community contributions too. Community input regarding further features that should be on the roadmap but are not there yet is welcome to augment the roadmap and provide an even more complete picture of the full development activities, including activities driven by the community.

The roadmap is a living document and its current state reflects the current knowledge and thinking. It is expected that ongoing community involvement will help to further shape and maintain the roadmap as we all move forward building the Internet Computer. Changing priorities will be reflected in the roadmap by new features being added, future features being removed, or reprioritized. This new format replaces the [framework used since July 2022](https://forum.dfinity.org/t/update-on-the-ic-roadmap-july-2022-summary/14615).

## Scope And Structure
The roadmap is focussed on technology development and comprises work items that advance the state of the Internet Computer. This comprises items related to the Internet Computer *protocol* itself (e.g., core protocol, chain-key cryptography, AI), its *decentralization* (e.g., crypto economic aspects such as node provider remuneration), the *languages and toolchains* used for building on ICP (e.g., Motoko, Azle, and dfx), and *services*, i.e., important decentralized services that are providing value to the ecosystem (e.g., Oisy and Orbit). Regular dapps and their development are not reflected in this system-centric roadmap.

The roadmap is structured to comprise the following nine core areas, or themes, of work:
* Compute Platform
* Decentralized AI
* Chain Fusion
* Privacy
* Platform Decentralization
* Identity
* Digital Assets
* Governance & Tokenomics
* Developer Experience

### Milestones
In each of the abovementioned nine areas at least one *milestone* has been defined and further milestones will be defined together with the community over time as the implementation moves along. Each milestone has a strong focus towards reaching a specific goal that provides major added value to the community, be it the users, the developers, or other stakeholders. A milestone comprises a handful of features that are required to accomplish its goal. Features that are currently being worked on show an orange progress icon, completed features have a green checkmark icon.

The roadmap re-introduces the concept of codenames for milestones. You surely remember the use of metal-themed milestones used for development up to the launch of ICP, namely Copper, Bronze, Tungsten, Sodium, and Mercury. This time, milestone names are inspired by fusion technology, for example cyclotron and gyrotron: The bold ambitions for technological advances of fusion technology reflect the ambitions for the Internet Computer Protocol. Also, fusion hints at the importance of the Chain Fusion area of development of ICP and the major goal of fusing AI with blockchain technology, two crucial themes on the near-term development roadmap.

### Past And Future Features
Besides milestones and their features, *past features* that have been completed and deployed are shown as well. This illustrates what has been achieved over the years through the substantial R&D investment so far and to see the continuous improvement process the Internet Computer is subjected to. It also gives a good impression of the functionalities deployed on ICP.

Future features that are already on the roadmap but have not yet been assigned to a milestone are shown as part of the respective theme, but may not always have been subject to the same level of detailed technical scrutiny that features that are part of milestones have received. As work on ICP progresses and milestones get completed and the priorities for upcoming work become clearer, new milestones will be defined by the community and DFINITY, subsuming parts of the defined future features and adding new ones. Due to changing priorities or technical considerations, future features may occasionally be dropped from the roadmap.

## Some Highlights
Let us take a look at some upcoming highlights of the roadmap in the key areas of Decentralized AI and Chain Fusion.

### Decentralized AI
As AI is gaining massive traction and is playing an increasingly important role in consumers' lives, the *trust problem* of "traditional" AI needs to be addressed: In traditional AI, users have no visibility into how their data is used and how AI models produce responses and whether they work correctly, reliably, and consistently. Leveraging the properties of a decentralized blockchain protocol allows users to verify how models are trained and to be assured that precisely those models are used for the inference that generates results for users. ICP's canister smart contracts with their special properties, e.g., being able to store large amounts of data and perform substantial amounts of computation, are a perfect match for realizing decentralized fully-on-chain AI. The *Cyclotron* milestone enables ICP to efficiently run decentralized AI inference using CPUs, while the longer-term *Gyrotron* milestone marks the availability of decentralized on-chain AI inference and training of large models using GPUs on ICP.

### Chain Fusion
Chain Fusion, the concept of fusing ICP with other chains to allow for building multi-chain dapps with a single-chain user experience, is a crucial theme for the future that will bring a much tighter integration of ICP with major blockchain ecosystems. Multiple milestones are scheduled for this area: *Tritium* for EVM chain integration, *Deuterium* for supporting Bitcoin protocols such as ordinals and runes, and *Helium* for ICP's integration with the Solana network, another logical step for ICP's Chain Fusion strategy.

Besides the few highlights mentioned further above, there are many more highlights available in the roadmap, and we invite you to have a look yourself at the following link:
https://internetcomputer.org/roadmap

We also invite you to read our [Medium article](https://medium.com/@dfinity/the-internet-computer-roadmap-62487bdce289)  summarizing the new roadmap and its milestones.

## Call For Action
The ICP community is most crucial to the success of the Internet Computer and realizing the World Computer vision! Thus, this forum post is also a call for action to the community to help complement the roadmap with further key community work items that are in progress or planned to be done by the community and will lead to widely useful production-grade features for ICP. Also, requests for features to be built by DFINITY can be placed. For now, please provide your feedback and requests by leaving a comment in this forum topic or by creating a now forum topic. Feedback regarding developer experience can be given directly on the [DX feedback board](https://dx.internetcomputer.org). The goal is to reach a public roadmap accurately reflecting the core technical activities around the Internet Computer protocol, tooling and languages, and important services, considering activities from both the community and DFINITY.

We also appreciate any other feedback w.r.t. the roadmap, be it related to its content or presentation thereof. Thank you in advance for your efforts in helping to improve the Internet Computer roadmap.


We hope you will enjoy reading through the roadmap and look forward to your feedback!

-------------------------

Seers | 2024-05-16 12:53:44 UTC | #2

Awesome roadmap. I personally would love to see vetKeys to improve privacy for AI, as well as blob storage or a solution that reduces the costs of storing data that will be rarely accessed and doesn't need low latency/high replication, given higher priority.

-------------------------

Samer | 2024-05-16 13:29:39 UTC | #3

Holy moly, this roadmap is on fire!

Big kuddos to Dfinity. IC truly becoming the World Computer!

Very excited about storage features, privacy features, DeAI and soooo much more üöÄüöÄüöÄ

-------------------------

gatsby_esp | 2024-05-16 14:24:32 UTC | #4

No native USDC and USDT issuance? Seriously?

-------------------------

ajismyid | 2024-05-16 15:45:19 UTC | #5

I see [Inrupt](https://www.inrupt.com/) and Internet Computer both have almost the same objective: decentralizing the web and giving users more control over their data (Inrupt develops Solid Pods, ICP develops wApps). 

This seems like a redundancy in tech development. Is there a way Inrupt can collaborate with DFINITY to achieve their same goals and reduce overlap? Has DFINITY ever considered reaching out to [Sir Tim Berners-Lee](https://twitter.com/timberners_lee) to collaborate? @dominicwilliams

-------------------------

dieter.sommer | 2024-05-16 17:43:25 UTC | #6

[quote="gatsby_esp, post:4, topic:30852, full:true"]
No native USDC and USDT issuance? Seriously?
[/quote]

Have a look at the feature "Native USDC integration through CCTP" in the set of future features in Chain Fusion. :-)

-------------------------

dieter.sommer | 2024-05-16 18:06:23 UTC | #7

[quote="Seers, post:2, topic:30852, full:true"]
Awesome roadmap. I personally would love to see vetKeys to improve privacy for AI, as well as blob storage or a solution that reduces the costs of storing data that will be rarely accessed and doesn‚Äôt need low latency/high replication, given higher priority.
[/quote]

Thank you, @Seers, for your feedback on priorities, much appreciated! The reasoning behind the current priorities for the crypto-related features is as follows: The big features that were on the plate for coming next were t-Schnorr / EdDSA and related improvements to the implementation (performance, latency) and vetKeys. After longer discussions, the outcome was to prioritize t-Schnorr / t-EdDSA for the reason of the immediate applicability to Bitcoin ordinals and other Bitcoin protocols we currently cannot support as well as having the performance improvements benefitting all t-signing use cases. t-EdDSA is relevant for a Solana integration and integration also with other chains. The decision was not an easy one, but was eventually taken in favor of t-Schnorr / t-EdDSA due to the concrete use cases we have. vetKeys is next in the pipeline as of current planning and is expected by end of this year or early next year if things go well.

Regarding a different storage architecture for storing larger amounts of data that is accessed less frequently, i.e., an architecture using erasure coding, that's indeed an extremely enticing feature, but it'a also rather large. For this reason it's currently not in one of the upcoming milestones, but in the future features and it's not yet clear when it would be scheduled. Getting more information on concrete use cases from the community would be extremely helpful for the future planning. In the end, this is all a joint effort and it's crucial for DFINITY to know about the community use cases so this can be prioritized to provide most value to the community.

Thanks again and let's keep the discussion going!

-------------------------

dfisher | 2024-05-16 18:51:35 UTC | #8

Wow. Just wow. Thanks team. May 23 and June 18 I see big things are being shipped. Lets go!

-------------------------

Jabberwocky | 2024-05-16 18:57:34 UTC | #9

Where‚Äôs Utopia? Why is it not included in the roadmap?

-------------------------

erikblues | 2024-05-16 21:14:02 UTC | #10

Love it, great roadmap! you have no idea how excited this makes me for the future of ICP. Latency-aware routing will allow ICP to compete with any CDN out there. This is amazing, thank you!

![image|690x335](upload://4Yh8UT7kARliPKbp62Sebw5j22H.jpeg)

## Suggestions:

Plan for a way to incentivise node-creation near high-density areas, where most of the requests come from and where nodes are needed the most.

The challenges I see are:
- ICP need more nodes in locations where there are more users, and not only in places where rent or hardware is cheaper (we need nodes in New York, not only in Alaska).
- if the TOKAMAK update directs requests to nodes closer to the user, it might overburden nodes in high-density locations if there is not an incentive to create more nodes there.

IMO, the solution to that would be to reward nodes based on cycles instead of the current flat fee model.

I know this topic is unpopular, because currently we need to support nodes to grow the network. But I'm not saying we should stop doing that. For example: we could guarantee a minimum payout for as long as the cycle usage is still low.

All I'm saying is that we will need to implement a system that self-regulates node-creation in the areas they are needed the most. 

Advantages of rewarding nodes based on cycles would be:
- if the TOKAMAK update directs requests to nodes closer to users, then there would be higher demand in higher density areas.
- if nodes are rewarded based on the amount of cycles they served, then node-providers will flock to locations where most of the requests come from, automatically resulting in a more robust network.
- it would also be nice if cycle-prices could fluctuate according to demand, allowing for nodes in cheaper locations to host data and apps that aren't that latency dependent. In other words: some apps could pay less, and be hoted further away, while other apps could pay more, to be hosted near high-density areas.
- as the cycle-burn rate increases, demand for cycles rises, making the price of cycles shoot up, which incentivise node-creation, which makes the price go down again. 

Similar to how BTC's tokenomics result in miners flocking to where energy is cheaper, I think ICP needs an economy that incentivises node-creation in the areas they are needed.

To be clear:
is this a priority today? No, not at all.
But shouldn't it be part fo the 4-year plan? 

Am I overlooking anything? How does ICP incentive node-creation in areas where they are needed the most? What incentives will node providers have, to pay higher rent+energy in high density areas? Or even better: is this not even an issue? Is this solved elsewhere already?

Thanks for reading and again: Amazing roadmap, congrats to everyone from the Dfinity team, this is looking really really good.

-------------------------

gatsby_esp | 2024-05-16 21:49:52 UTC | #11

Thank you!
What would be the ETA for all these features? Are we talking about 2 years, 4 or 10?

-------------------------

samuelburri | 2024-05-17 07:19:49 UTC | #12

Hi @gatsby_esp 
We consciously decided to not provide ETAs for features, except when they are bundled with a milestone that has an ETA. Most milestones are quarters away, i.e. not years away. Features in the "future features" buckets are further out. Some of them are very ambitious, e.g. " Fully-homomorphic encryption (FHE)" in the Privacy theme, whereas others are closer to realization and just haven't been scoped and planned yet. We felt it's valuable to show where the journey is going even though there is not a detailed timeline.

-------------------------

Jan | 2024-05-17 09:11:33 UTC | #13

Utopia is all about making ICP fast, more scalable, etc so it's in Compute Platform.

-------------------------

dieter.sommer | 2024-05-17 09:37:05 UTC | #14

Hi @erikblues!

[quote="erikblues, post:10, topic:30852"]
Latency-aware routing will allow ICP to compete with any CDN out there.
[/quote]

Indeed! And there are also other features that complement this one for CDN functionality of ICP. Have a look at "CDN canisters on edge infrastructure" in the category "Platform Decentralization". This is about caching (static and certified) content in Boundary Nodes, which brings the data closer to where it is needed, much like a CND works.

[quote="erikblues, post:10, topic:30852"]
Plan for a way to incentivise node-creation near high-density areas, where most of the requests come from and where nodes are needed the most.
[/quote]

I think you are making a very valid point here, particularly for the (not immediate) future of ICP. Node allocations are currently determined by the decentralization of ICP only, but do not take the "distance" in terms of latency to users into account.

Proximity of Boundary Nodes with the large masses of users is an important aspect in the future and is something that may need to be considered as another dimension for node allocation in the future, particularly for Boundary Nodes. Currently, as you say, it's not an issue, but once traffic volumes increase, this will be increasingly important to be considered as well. In the near-term future, the new Boundary Node architecture will allow the community to run their own NNS-controlled BNs, which will allow deployments matching demand better than nowadays. This is a first step towards a topology that goes into the direction you suggest, but without the incentives.

So far, node cryptoeconomic incentives have favored node machines in regions so that decentralization is enhanced as much as possible. The cost structure of the region is already accounted for in the node rewards. As you suggest, it may be well worth looking into creating additional incentives, of whatever form, that favor deployments of (Boundary) Nodes closer to where the demand is. But of course, decentralization overall must remain as strong as now, so I'd say that closeness to the users would be a secondary dimension to be considered behind decentralization.

If one would incentivize only as you suggest, decentralization would suffer as all nodes would be allocated in a few areas, thus being less decentralized. What we need is retaining the strong decentralization of today, while still serving the high-demand areas well. Maybe this is something that can go into one of the future remuneration models for node providers once demand picks up further and this is becoming an issue.

Thanks again for your thoughts, we will keep them in mind and I hope that this will be discussed in more detail with the community in the future! This will definitely be an interesting discussion to have.

-------------------------

dieter.sommer | 2024-05-17 12:51:33 UTC | #15

[quote="ajismyid, post:5, topic:30852"]
I see [Inrupt ](https://www.inrupt.com/) and Internet Computer both have almost the same objective: decentralizing the web and giving users more control over their data (Inrupt develops Solid Pods, ICP develops wApps).
[/quote]

Indeed, both initiatives have similar high-level goals of decentralizing the Web and giving better privacy to users, however, it seems that the approach is quite different. While Inrupt, or the Solid protocol they build on, attempts to use a peer-to-peer architecture largely based on Web standards and without heavy cryptography, ICP uses a decentralized cryptographic protocol with novel, advanced, cryptography to have an architecture offering full replication, the security resulting from it, and privacy. Thus, very different approaches to reach similar high-level goals of making the Web free again. I don't want to imply that there should be no conversations between the projects, though, as both seek to achieve in essence the same thing.

-------------------------

CoolPineapple | 2024-05-17 13:49:23 UTC | #16

Amazing exciting and ambitious roadmap, that really goes a long way to making ICP a truely performant decentralised cloud. There is however one thing that is missing and that is: **Shared security**.

-------------------------

skilesare | 2024-05-17 14:04:27 UTC | #17

I've only just started looking at solid, but it looks like something that we could do at the application level while dfinity focuses on the protocol.  Feel free to reach out. We have proposed something similar called neutron in the past and it would be interesting to see where we there might be overlap.

-------------------------

skilesare | 2024-05-17 14:17:40 UTC | #18

[quote="dieter.sommer, post:7, topic:30852"]
After longer discussions, the outcome was to prioritize t-Schnorr / t-EdDSA for the reason of the immediate applicability to Bitcoin ordinals and other Bitcoin protocols we currently cannot support as well as having the performance improvements benefitting all t-signing use cases
[/quote]

Boo. You can't win them all. üòÄ

The rest looks really really good. Mad props.

My one critique(so it has a voice more so than I'm really upset about it):

I'd reemphasize that from my viewpoint, replica access to SEV is the single biggest unlock on this list on the order of billions of dollars in the near term. But I guess we want to keep propping up an archaic meme coin that has been surpassed in technology and burns the atmosphere to put jpegs on the least efficient technology possible. 

I'll go back in my hole now and focus on all the amazing stuff on this list.üéâüöÄüöÄüöÄ

-------------------------

erikblues | 2024-05-17 19:30:57 UTC | #19

[quote="dieter.sommer, post:14, topic:30852"]
Indeed! And there are also other features that complement this one for CDN functionality of ICP. Have a look at ‚ÄúCDN canisters on edge infrastructure‚Äù in the category ‚ÄúPlatform Decentralization‚Äù. This is about caching (static and certified) content in Boundary Nodes, which brings the data closer to where it is needed, much like a CND works.
[/quote]

I hadn't seen that, thank you!

[quote="dieter.sommer, post:14, topic:30852"]
So far, node cryptoeconomic incentives have favored node machines in regions so that decentralization is enhanced as much as possible.
[/quote]
That is a good point, I didn't know node rewards were already rewarding some regions over others. In other words, what I touched on is already being done, even though through a different approach. So yes, for now that works, and it can always be revisited in the future, once all the more pressing things are taken care of.

Again: really good job on the roadmap, this is really really good.

-------------------------

cchung | 2024-05-17 20:01:27 UTC | #20

Regarding Decentralized AI, we will have to ensure that the node machines have the hardware that can support this new initiative at scale so inference can be done easily by the majority of nodes. Gen-1 nodes have already been paid for and are good candidates for upgrades.

-------------------------

ilbert | 2024-05-18 14:00:47 UTC | #21

Hey @ajismyid, there's a new idea that @massimoalbarello published a few days ago which is related to Solid and hence Inrupt:
https://forum.dfinity.org/t/30823

You guys should move the discussion in that thread!

cc @skilesare

-------------------------

massimoalbarello | 2024-05-19 09:45:26 UTC | #22

I agree, will reach out in DM

-------------------------

w3tester | 2024-05-19 12:00:21 UTC | #23

Glad to see the t-EdDSA signature is on its way. Can you give us a more specific timeline? Like when can we use it in production environment? We can really use it now :laughing:

Plus, any timeline for improved t-ECDSA latency? Currently we are seeing more than 10 seconds delay when using it for signatures. Short signature time can definitely help us a lot.

-------------------------

dieter.sommer | 2024-05-20 12:05:44 UTC | #24

[quote="CoolPineapple, post:16, topic:30852, full:true"]
Amazing exciting and ambitious roadmap, that really goes a long way to making ICP a truely performant decentralised cloud. There is however one thing that is missing and that is: **Shared security**.
[/quote]

Hi @CoolPineapple!

Isn't the shared security paradigm inherent to ICP as it is to IaaS cloud providers? In my opinion it is: ICP provides you a secure platform while a dapp developers is responsible for building a secure dapp on top of this platform.

Do you share this view of ICP already now featuring shared security? Or what do you think is missing on the roadmap to get where you think ICP should be w.r.t. shared security? Many thanks for your thoughts!

-------------------------

dieter.sommer | 2024-05-20 12:10:45 UTC | #25

[quote="cchung, post:20, topic:30852, full:true"]
Regarding Decentralized AI, we will have to ensure that the node machines have the hardware that can support this new initiative at scale so inference can be done easily by the majority of nodes. Gen-1 nodes have already been paid for and are good candidates for upgrades.
[/quote]

Yes, indeed! There are roadmap items called "Public specification for GPU-enabled nodes" and "AI-specialized subnets with GPU-enabled nodes" in the Decentralized AI theme that are thought of taking care of this. Current thinking is that ICP will start with one AI subnet and then, based on demand, create more. How exactly those nodes look like needs to to be decided by the community in the end, but likely they will offer multiple high-end AI accelerator cards per node.

-------------------------

dieter.sommer | 2024-05-20 12:21:11 UTC | #26

[quote="w3tester, post:23, topic:30852, full:true"]
Glad to see the t-EdDSA signature is on its way. Can you give us a more specific timeline? Like when can we use it in production environment? We can really use it now :laughing:

Plus, any timeline for improved t-ECDSA latency? Currently we are seeing more than 10 seconds delay when using it for signatures. Short signature time can definitely help us a lot.
[/quote]

Hi @w3tester!

Threshold EdDSA is part of the Helium milestone which has no firm arrival date yet. However, it is planned to be worked on once Deuterium has been finalized by July 25. The work on Schnorr signatures in Deuterium is prerequisite work that prepares the implementation architecture for threshold EdDSA as well. Threshold Schnorr is structurally very similar to threshold EdDSA, but using a different algebraic structure to operate in, thus Schnorr already anticipates much of the implementation that threshold EdDSA requires. The performance improvements for threshold ECDSA being worked on currently also apply to threshold Schnorr and EdDSA. Considering that threshold EdDSA is not a huge item once Schnorr has been done, I'd wager that Q3 this year might be doable for threshold EdDSA.

Threshold ECDSA latency improvements is currently being worked on so should not be that far out to be in production.

-------------------------

CoolPineapple | 2024-05-30 17:30:35 UTC | #27

Hi @dieter.sommer 

What I mean by shared security is that the security of a canister executing on a subnet should not be at risk if the 1/3 of the nodes *on that subnet* are compromised (typically comprised of 13 nodes) but rather should be secured by the network as a whole (559 nodes) in some sense. 

Scaling while ensuring shared security is what [Ethereum](https://vitalik.eth.limo/general/2024/05/23/l2exec.html) is trying to achieve with rollups, [Polkadot](https://wiki.polkadot.network/docs/learn-parachains#shared-security) is trying to achieve with the relay chain/parachain model, and Near is trying to achieve by randomly and frequently reassigning validators to different shards. This can be distinguished from the Cosmos/ICP approach where each subnet is essentially independent and can be separately compromised.

Possible approaches:

1. [Node shuffling.](https://forum.dfinity.org/t/shuffling-node-memberships-of-subnets-an-exploratory-conversation/7478/9) although this doesn't increase the number of nodes that need to be compromised frequent shuffling makes it harder for an attacker to target a particular subnet via time consuming bribery or hacks since the nodes involved can change unpredictably. 
2. [Subnets as Optimistic rollups](https://forum.dfinity.org/t/anytrust-model-for-subnets/18050): Each subnet acts like an [Arbitrum (antitrust)](https://docs.arbitrum.io/how-arbitrum-works/inside-anytrust) rollup where in the happy case execution proceeds as it does now, but if one node within a subnet disputes a fraud proof is initiated. Since ICP lacks a canonical subnet that can act as an "L1" like computation court I suggest either reserving a high replication subnet for this purpose or simply picking a random subnet to handle the dispute since this would be unpredictable to the attacker.
3. Only [High security canisters](https://x.com/CoolPineapple18/status/1793615404151054503) marked as having a high security demand (for example a ledger canister that holds asset balances) rather than the whole state are re-validated by additional nodes or subnets as an additional assurance. For example:
   *  By additional randomly chosen subnet(s).  
    * Using Dominics [validation towers](https://x.com/dominic_w/status/1752416340600659975) where a random beacon and staking system give additional assurance that the state of a particular canister has advanced correctly

4. Some crazy ZK thing.

These are just ideas. The point is more that there should be some kind of program looking at shared security as an objective since this is one of the [main criticisms people](https://x.com/Justin_Bons/status/1748762764233826381) make of ICP, and while it is not necessary for decentralised social media it is necessary for ICP to be trusted for DeFi and asset applications.

-------------------------

dieter.sommer | 2024-05-27 12:39:22 UTC | #28

Hi @CoolPineapple, thanks for clarifying what you meant with *shared security*. At least node shuffling has been discussed AFAIK, but it also has downsides, e.g., all potential attackers get rotated into and again out of each subnet over time, which may even weaken the model (e..g, an attacker could extract the state of the subnet, which is easier now, and much harder once SEV-SNP is available).

Overall, I agree that this is something to have on the roadmap, but I think it's just not clear what is the best approach, if any, to achieve shared security in the ICP's model, thus there is no roadmap item (yet) for this. Not all approaches applicable for other chains will work in the ICP's model of subnets. We'll keep this one on our mind and discuss this in the Foundation. Once there is some clearly preferable approach to choose, there should be a roadmap item for it. We might also put a roadmap item on already now about exploring shared security, just to keep it in our official "backlog" of things to be done for ICP.

By the way, are you aware of concrete options having been discussed in the community (and links to the discussions, if applicable)?

Thank you again for your input and thoughts about this topic!

Update: The roadmap items about settling the hash of a canister's state to the Bitcoin or Ethereum network is somewhat related to this, but fulfills only parts of the goals. It is similar to an L2 settling on some L1, but in this case only a hash to guarantee integrity of the data.

-------------------------

lastmjs | 2024-05-27 16:56:30 UTC | #29

I agree. Wow how I wish we could have shared security on ICP. I've long hoped that node shuffling could be a promising solution to this problem. There is a lot of discussion publicly available in this forum on the subject.

And as for my simple definition of shared security, or how we know when the property of shared security has been implemented: individual canister security becomes a network effect of ICP.

By network effect, I mean that as the ICP network grows in subnets and/or nodes, the security of individual canisters improves as an emergent property.

More subnets/nodes = more security for all canisters

There is no current security network effect like this on ICP unfortunately. Adding more subnets makes no direct security difference to other subnets. And adding more nodes to an existing subnet makes no difference to other subnets.

Also subnet node operators membership is static. This makes me uncomfortable over long periods of time.

Also the fact that node operators know which subnets they're in and can find out which canisters they are hosting seems unideal.

-------------------------

lastmjs | 2024-05-28 05:57:26 UTC | #30

Can someone shed light on the privacy theme?

Most of the truly interesting and impactful features for privacy aren't even scoped into milestones. The only privacy milestone focuses on vetKeys, and I am starting to doubt how impactful vetKeys will be alone, considering that end-to-end encryption greatly limits computability over data, and that users can manage their own private keys relatively easily outside of browser applications. Many apps have end-to-end encryption now without using any kind of decentralized blockchain protocol.

The truly useful innovations will be a combination of (maybe vetKeys) secure enclaves, FHE, and MPC. But they are only listed as future features.

I understand how incredibly difficult FHE and MPC will be to implement efficiently enough to be useful.

But what is taking so long for SEV? Why is it not being prioritized more?

I remember an early version of this being ready before genesis, and there was some public discussion about whether or not to enable it at launch.

There is no technical privacy protection right now for canisters...SEV is possibly the most impactful low-hanging fruit here.

-------------------------

bjoern | 2024-05-28 07:58:43 UTC | #31

The main reason that for now only the vetKeys milestone has been scoped is that we had to start somewhere, and it made most sense to start with the milestones that are most likely finished first. VetKeys build on many existing protocol components that have been used in production for 3+ years, and the remaining work is fairly well planned at this point.

I agree both with your view that SEV will deliver important value and with your frustration that we have not been able to keep our intended timeline. The reason is simple: while SEV is already by itself a fairly complex piece of technology, operating it in a decentralized high-availability environment, where the administrative control over the nodes is done via a governance system and not direct control of the nodes, makes all steps significantly harder. Two examples:
- When upgrading replica software, there has to be a hand-over between the old and new version that preserves the integrity, using bilateral attestation between the two versions. (Right now, the nodes just install a new guest OS image and give it access to the same data partitions.)
- Subnet recovery becomes inherently difficult, since the current recovery process requires accessing the data. This is of course inherently tricky with SEV subnets, where canister data would be expected to be confidential. Now many recoveries in the past could have done without any access to the data; however, this means we need more elaborate procedures and tools for recovery as well.

The path we take for SEV is now as follows: We first start with a use case that will provide us high impact with lower operational complexity, namely HTTP gateways. Upon gaining operational experience, we will step by step move toward the more complex use cases. The most complex one is then arguably running a full replica with SEV support.

A more detailed timeline with milestones will follow on the roadmap, it just isn't in the appropriate shape yet.

-------------------------

lastmjs | 2024-05-28 13:14:30 UTC | #33

Thank you for the detailed explanation üëçüëçüëç

-------------------------

w3tester | 2024-05-29 02:20:32 UTC | #34

Regarding the privacy part, we'd be very much interested to see if ZK proofs can help. ZK enables a paradigm in which computation of user data will take place at client side, not server side. This automatically gives the strongest privacy assumption‚Äîyour data is most secure when you are the only one holding them. 

With zCloak building a ZK verification layer in ICP, we'd love to see real world use cases which require privacy-preserving computations. It would be great if people can post their requirements here and we will discuss how we can help to solve their issues with ZK.

-------------------------

dieter.sommer | 2024-05-29 06:25:58 UTC | #35

ZK proofs indeed play a strong role for privacy. I propose to create a dedicated forum topic for this and continue the discussion there to have it in one place and make it easier to follow. Looking forward to the thoughts on this! Please link the new discussion topic from here if you want to create one!

-------------------------

w3tester | 2024-05-29 06:50:34 UTC | #36

Great idea! I have created a separate post to discuss it.

https://forum.dfinity.org/t/discussion-zero-knowledge-proofs-for-privacy-apps-in-icp/31291

-------------------------

lastmjs | 2024-05-31 23:18:03 UTC | #37

I have a question about the Cyclotron roadmap in the AI theme.

"Allow smart contracts to run inference using AI models with millions of parameters fully on chain."

Notice it says millions and not billions. Why is this the explicit goal? Obviously models with millions of parameters are, I believe, much less capable than models with billions of parameters, like the most capable Llama models.

I would love insight into if the end goal will eventually be billions of parameters.

-------------------------

dieter.sommer | 2024-06-02 09:06:39 UTC | #38

Hi @lastmjs!

The Cyclotron milestone talks about "millions" of parameters, and not "billions" because it is still using CPU cores for the inference. Reasoning on large LLMs with billions of parameters on the CPU would just take too long per token to be practical.

Large LLMs with billions of parameters require GPU support, which is the key part of the Gyrotron milestone. Once a deterministic API for smart contracts to use GPUs as well as Wasm64 is available, large LLMs with billions of parameters should be practically feasible also on the Internet Computer.

-------------------------

lastmjs | 2024-06-03 14:31:47 UTC | #39

Do you have any insight into why a deterministic API for GPUs is difficult to achieve? Or is it difficult to achieve?

-------------------------

lastmjs | 2024-06-03 17:22:05 UTC | #40

In the `STELLARATOR` milestone, the tile `Improved consensus throughput and latency` says this: 

"Improved consensus throughput and latency by better, and less bursty, node bandwidth use. Achieved through not including full messages, but only their hashes and other metadata, in blocks."

Will this help to increase the message size limit? More discussion here: https://forum.dfinity.org/t/hashed-block-payloads/28365

-------------------------

