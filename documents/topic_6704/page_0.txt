ccyanxyz | 2021-08-25 15:33:51 UTC | #1

Hi, this is the developer from DFinance, we launched our testnet today, it all went smoothly until a while ago, users talking about loading too slow, I tried to call our backend canister with dfx, but I got 504 error, is this because of too much traffic? I thought IC has load balance?

![37551629904445_.pic|690x159](upload://rRLyyoD8GR4NmHevojRn3cfyaL.jpeg)

Canister id: https://ic.rocks/principal/lf23w-ciaaa-aaaah-qaeya-cai
We are in this subnet: https://ic.rocks/subnet/pjljw-kztyl-46ud4-ofrj6-nzkhm-3n4nt-wi3jt-ypmav-ijqkt-gjf66-uae

-------------------------

ccyanxyz | 2021-08-25 15:28:15 UTC | #2

![Screen Shot 2021-08-25 at 11.28.07 PM|690x187](upload://AdfKw9XVsZgbJtoPbYY1SOJFjEI.png)

-------------------------

cryptoschindler | 2021-08-25 16:41:43 UTC | #3

I believe this is a problem for your subnet, other projects reported the same issues, maybe you're all on the same subnet? (entrepot & icpunks)

https://dashboard.internetcomputer.org/subnet/pjljw-kztyl-46ud4-ofrj6-nzkhm-3n4nt-wi3jt-ypmav-ijqkt-gjf66-uae

-------------------------

ccyanxyz | 2021-08-25 16:44:31 UTC | #4

Yes I believe so, we are in the same subnet.

-------------------------

cryptoschindler | 2021-08-25 16:47:04 UTC | #5

gg, you broke the IC!

-------------------------

ccyanxyz | 2021-08-25 16:49:52 UTC | #6

Haha, stress test, find the problem early so we can grow stronger.
It seems that the current subnets architecture can not handle too much triffic.

-------------------------

cryptoschindler | 2021-08-25 16:51:54 UTC | #7

Luckily the IC can scale and add nodes to the subnets :slight_smile:!

-------------------------

stopak | 2021-08-25 16:52:04 UTC | #8

We've also launched a whitelist event which involved mutiple people to login to our website icpunks.com and had exact problem. It looks like that we did a stress test :D

-------------------------

ccyanxyz | 2021-08-25 16:53:04 UTC | #9

About 9k user participated in our testnet event, at the first several hours, it's working smoothly, then it stucked.
![Screen Shot 2021-08-26 at 12.52.33 AM|328x39](upload://yrRaq32xFiIcjr54woI0p5v7sRz.png)

![image|690x373](upload://64e76ukeBE2LhcFrYCyqVTq9hFH.png)

-------------------------

stopak | 2021-08-25 16:55:20 UTC | #10

So you are the culprit ;). I'm glad that it happend today and not on the claiming day :D.

-------------------------

free | 2021-08-25 17:03:43 UTC | #11

As far as we can tell this has to do with the fact that subnet `pjljw` handles by far the most queries, combined with an issue we discovered recently, where we only cache the compiled Wasm when executing updates, not queries. Meaning that before an update is executed on a canister, each query will compile the Wasm from scratch. Because of the load, some queries will be really slow, while other queries will time out before they even get a chance to execute.

The reason why this started suddenly is that we [upgraded the replica version on subnet `pjljw` a few hours ago](https://github.com/ic-association/nns-proposals/blob/main/proposals/subnet_management/20210825T1344Z.md) and and caches got purged. Why this behavior wasn't noticed until now (on previous replica upgrades) is unclear.

We're working on a fix, but it may take a while to deploy. In the meantime, the more canisters on `pjljw` handle at least one update, the less contention among queries, so this should become better over time.

-------------------------

stopak | 2021-08-25 17:09:37 UTC | #12

Thanks for the fast response. Is there anything that we can do right now, or just wait?

-------------------------

free | 2021-08-25 17:12:29 UTC | #13

Make sure to run an update query on each of your canisters. (o:

More seriously though, I don't think there's anything you can do. I'll try to figure out if I can find a way to run a replicated query (i.e. run a query via an ingress message) on all canisters on the subnet, to prime the cache. Actually, now that I think of it, anyone could do it.

-------------------------

alexander | 2021-08-25 17:33:39 UTC | #14

We have same problem accessing our application which runs in the same subnet.

-------------------------

nandit123 | 2021-08-26 00:19:34 UTC | #15

IC Drive was also affected, the app was loading super slow and our public file links were also affected. Seems it's fine now.

-------------------------

peterparker | 2021-08-26 04:46:34 UTC | #16

The issue seems to still happen, I face the `net::ERR_ABORTED 504` with my [asset canister](https://iey7l-kaaaa-aaaah-qadoa-cai.raw.ic0.app/) too right now.

![Capture d’écran 2021-08-26 à 06.46.14|566x500](upload://iIMQjmfFdu40Mn7WTevLZJLy6bV.jpeg)

-------------------------

ccyanxyz | 2021-08-26 04:47:56 UTC | #17

Yes, it happens again

-------------------------

Shuo | 2021-08-26 05:30:34 UTC | #18

We noticed the issue happened again. The root cause has been identified and the fix is on the way. Apologies for the inconvenience and thanks for your patience.

-------------------------

stopak | 2021-08-26 19:42:04 UTC | #19

Hi Shuo, do you know when it will be fixed? It's getting hard to test anything

-------------------------

diegop | 2021-08-27 03:20:33 UTC | #20

Thanks for asking, I’ve pinged the team to see who can give an update. Sorry I can’t help, I’m not familiar with the status.

-------------------------

Shuo | 2021-08-27 05:10:51 UTC | #21

Hi @stopak , we are preparing the release including the fix for an NNS proposal.

-------------------------

diegop | 2021-08-27 05:22:57 UTC | #22

Thanks @Shuo ! Much appreciated

-------------------------

stopak | 2021-08-27 07:32:47 UTC | #23

Hi @Shuo , thanks for the good news!

-------------------------

ulan | 2021-08-27 13:38:14 UTC | #24

Hi folks. We have rolled out the fix and are seeing improvements in the message throughput. Do you see still see any errors on your side?

-------------------------

ccyanxyz | 2021-08-27 16:25:04 UTC | #25

Looks like still unstable
![image|657x427](upload://1Gmy1eDuLdsPMXsbY7SCk253Qz3.png)

-------------------------

diegop | 2021-08-27 16:28:55 UTC | #26

Thanks for letting us know @ccyanxyz , I will ping team.

-------------------------

dfkiranj | 2021-08-27 17:32:34 UTC | #27

Thanks @ccyanxyz for sharing your observation. The rolled out fix might not reflect accurately in the finalization rate. It would be helpful if you could let us know if you are seeing request failures on your side, similar to 504s observed originally.

-------------------------

ccyanxyz | 2021-08-27 17:50:42 UTC | #28

Now the query calls seem to be smooth, no errors.

But we did also get some 403 errors before the rolled out fix, not quite clear why.

-------------------------

dfkiranj | 2021-08-27 19:10:31 UTC | #29

Thanks @ccyanxyz for confirming this. We will also keep an eye out.

-------------------------

stopak | 2021-08-27 21:25:47 UTC | #30

I can also confirm no more 504s. Thank you guys for great work!

-------------------------

