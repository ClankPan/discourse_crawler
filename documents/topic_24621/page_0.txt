patnorris | 2023-11-12 16:36:55 UTC | #1

Hi everyone, @icpp and I have been talking and thought it would be great to have a Technical Working Group around DeAI where we can e.g. exchange ideas, initiatives and updates on relevant IC features. So we're reaching out with this post to get your feedback on this idea; Who would be interested in joining such a group? What kind of content would you like to see and discuss there?
Happy to hear everyone's feedback! 
Have a great week, Pat

-------------------------

mnl | 2023-11-13 14:50:02 UTC | #3

I'd be interested in joining for sure

-------------------------

patnorris | 2023-11-14 17:38:10 UTC | #4

That'd be awesome, cheers! Let's see how much initial interest there is but we could also start with a smaller group of us at first :)

-------------------------

sadernalwis | 2023-11-16 07:42:43 UTC | #5

bout time.. count me in!

-------------------------

lastmjs | 2023-11-17 04:44:11 UTC | #6

I'd like to join too, very interested in discussing what Kybra needs to support AI use cases.

-------------------------

ildefons | 2023-11-18 13:28:34 UTC | #7

I want to join this tech group. I am an AI researcher currently working on the second milestone of a grant to develope a machine learning motoko library to train models on chain.

-------------------------

evanmcfarland | 2023-11-18 13:32:33 UTC | #8

I'm interested. 

Would be helpful to have a community push for getting information about the DeAI subnets Dom suggests are under development.

 My current focus is also on migrating to an on-chain vector databases, which needs community discussion.

-------------------------

patnorris | 2023-11-18 15:42:40 UTC | #9

Great, thank you for your responses! These sound like super interesting projects, I'm looking forward to learning more :)

-------------------------

patnorris | 2023-11-18 16:12:15 UTC | #10

For our first meeting, I'd like to propose this Thursday, Nov 23 at 5pm UTC.

Agenda draft:
* Round of introductions to get some context on each of our projects/work and goals in terms of AI on IC
* Capture an overview of the tech each of us is using and any lessons learnt, best practices and current challenges
* Start a "wish list" of features/ressources/... that would help the projects' development
* @mnl, if you can join and are open to sharing it, I think it'd be interesting to hear about DFINITY's current work around DeAI and the roadmap
* Decide on initial frame for the working group (e.g. cadence, meeting day/time, structure, typical agenda items, organization tools)
* Time to discuss specific topics of interest

What do you think and do you have any other items you'd like to see on the agenda?

I'd schedule us for an hour and if anyone needs to leave earlier, they can let me know and we'll make sure to capture their perspective and ideas before they need to head out.

How does this sound to you?

-------------------------

Gamris | 2023-11-20 02:11:38 UTC | #11

I'm interested too. Sign me up

-------------------------

NeutronStarPRO | 2023-11-20 02:44:47 UTC | #12

That's good idea
:smiling_face_with_three_hearts: :heart_eyes: :star_struck: :kissing_heart: :partying_face:

-------------------------

patnorris | 2023-11-20 12:47:11 UTC | #13

Hi everyone, I'll also post the link to the video meeting (Google Meet) for Thursday here into the group but if anyone wants to get a direct invite and have it on their calendar, please message me with your email address and I'm happy to invite you.

We're thinking to record the meeting (and future ones) and also publish some meeting notes to build up a knowledge base and allow anyone who can't join to quickly catch up. Would this be fine?

Looking forward to meeting you all!

-------------------------

patnorris | 2023-11-22 18:35:56 UTC | #14

Hi everyone, this is the link for our call tomorrow (Thu Nov 23 5pm UTC):  https://meet.google.com/ddh-jnpq-dji
Looking forward to meeting you all!

-------------------------

patnorris | 2023-11-23 16:02:42 UTC | #15

See you all in an hour :)

-------------------------

patnorris | 2023-11-23 18:03:02 UTC | #16

Sorry the end got cut; first, thank you all for joining, it was great to meet you and hear about your projects and ideas!

I will put out another post about how we could organize ourselves going forward. If you have any ideas for this (or anything else), please post them here or message @icpp or me.

I'm looking forward to continue the conversation :)

and for everyone in the US, happy Thanksgiving!

-------------------------

patnorris | 2023-11-24 21:35:15 UTC | #17

Hi everyone, as mentioned, it'd be great to have your input on how we'd like to organize ourselves going forward.

First, I'd like to invite feedback around our first meeting yesterday; which aspects should we keep, which ones should we reduce, which new ones should we add?

We had planned to also discuss some administrative things yesterday but ran out of time to do so (I hope you enjoyed the lively discussion though :) ).
@icpp and I had a few ideas around this that I'll put here as a starting point. It'd be great to have your views and ideas here as well.

* Weekly call on Thursdays at 5pm UTC for 30-60min (initially, and we can see how we'd like to adapt this)
* Record the meetings, automatically extract the script and a summary from it
* Manually write down any important decisions or action items
* Tools: OpenChat community with channels for topic-specific discussions and to keep the recordings/notes

Common agenda items:
* Summary of news (related to DeAI on IC and beyond)
* Project updates: what’s done, plans/milestones, blockers
* Action items
  * Living doc of "wish list" of features/resources/… that would help the projects’ development (IC and beyond)
  * Dashboard for IC DeAI projects (e.g. projects overview, stats)
  * Standards (e.g. APIs)
  * Documentation/education, e.g. how to get started (incl. programming languages, tech stack, resources, examples)

What are your thoughts around these? What else do you think is important? 

If you like, please also share your priorities and which goals you think the WG should have.

Thanks and have a great weekend!

-------------------------

patnorris | 2023-11-28 20:08:55 UTC | #18

Hi everyone, I'm looking forward to our second meeting this Thursday, 11/30, 5pm UTC. I will post the link to the video call here in the group. If you like to get a direct invite, please message me with your email address.

Agenda-wise, it'd be great to decide on a few administrative things (see previous post) and discuss which action items and goals/initiatives we'd like to tackle within the group. What else would you like to see on the agenda?

Looking forward to seeing everyone,
Pat

-------------------------

patnorris | 2023-11-30 12:53:32 UTC | #19

Hi everyone, this is the video call link to our meeting later at 5pm UTC: https://meet.google.com/ddh-jnpq-dji

Looking forward to talking to you all!

-------------------------

patnorris | 2023-12-01 14:09:07 UTC | #20

Thank you for joining today, it was great talking to you again!

These are some of the things we discussed:
* Best timeslot for the WG to meet?
* Singularity.Net how are they decentralizing their marketplace?
* Vector database in IC canister
* Could FAISS (vector database) library work on IC (with Kybra in Python or icpp-pro in C++)?
* Could Kybra integrate icpp-pro for C++ libraries?
* Run zkWASM on IC? Maybe good use case for GPU subnet?
* Shared projects
  * Pure Python framework for deep learning: https://github.com/tinygrad/tinygrad, https://www.youtube.com/watch?v=2QO3vzwHXhg
  * Ring transformer: https://arxiv.org/pdf/2310.01889.pdf
  * https://github.com/bigscience-workshop/petals
  * Tinyllama: https://github.com/jzhang38/TinyLlama
  * https://github.com/QwenLM/Qwen
  * Gorilla LLM (make API calls); run on IC? https://gorilla.cs.berkeley.edu/
* Goals:
  * Showcase capabilities and progress (also to outside of IC community), e.g. in gaming NPCs who talk thanks to models running in canister
  * Stable diffusion (small model: SSD-1B)
  * Streamline deployment of small models
  * Rust ML
* Action items
  * Link dashboard (with IC projects, stats etc) to Awesome IC, focus on discoverability and communication
  * Hacker news top with developments/projects on IC
  * Develop GPU subnet (at least a first version) as a community effort

-------------------------

Gamris | 2023-11-30 18:51:53 UTC | #21

Thanks for the discussion. Re-sharing some of the links to the points discussed earlier.

Dapps:
[ICGPT](https://icgpt.icpp.world/) - Canister LLM CPU-inferencing
[DeVinci App](https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io/) - Canister LLM WebGPU-inferencing

ML Programming:
[C++ CDK](https://docs.icpp.world/index.html) - C++ dependencies and cpp models
[Rust Burn](https://github.com/Tracel-AI/burn) - ML framework in Rust
[Tinygrad](https://github.com/tinygrad/tinygrad) - ML framework in pure Python

Distributing AI compute:
[RingAttention](https://arxiv.org/abs/2310.01889) - Distributing long sequences across multiple devices
[PetalsML](https://github.com/bigscience-workshop/petals) - BitTorrent style ML computation
[Stable Horde](https://stablehorde.net/) - Distributed cluster of text and image generation workers

Lightweight AI models:
[TinyLlama 1.1B](https://github.com/jzhang38/TinyLlama) - Small but coherent LLM
[Qwen 1.8B Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat) - Small but coherent LLM
[SSD-1B](https://huggingface.co/segmind/SSD-1B) - Distilled version of SDXL (can use [LCM version](https://huggingface.co/latent-consistency/lcm-ssd-1b) too)
[Gorilla LLM](https://gorilla.cs.berkeley.edu/) - LLM trained to perform API calls (agentic software)

LLM structured output:
[LMQL](https://lmql.ai/), [Guidance](https://github.com/guidance-ai/guidance), [Guardrails](https://docs.guardrailsai.com/) - Enabling structured query output for LLMs consistently (ex. JSON)

-------------------------

lastmjs | 2023-11-30 20:14:02 UTC | #22

Sorry I've missed these meetings, I'd love to get these into my Calendar automatically somehow, is that possible?

-------------------------

patnorris | 2023-12-01 13:16:01 UTC | #23

Hi @lastmjs , does this event link work? https://calendar.google.com/calendar/event?action=TEMPLATE&tmeid=NHM4bTkxZGthaGdlY3I1cm5hYWFpajJha2dfMjAyMzEyMDdUMTcwMDAwWiBwYXRyaWNrLmZyaWVkcmljaDkzQGdvb2dsZW1haWwuY29t&tmsrc=patrick.friedrich93%40googlemail.com&scp=ALL

Otherwise, you can also send me your email address and I'll send you a direct invite :) Looking forward to having you!

-------------------------

JaMarco | 2023-12-01 15:39:07 UTC | #24

Is there a recording of this meeting?

-------------------------

JaMarco | 2023-12-01 15:44:31 UTC | #25

[quote="patnorris, post:20, topic:24621"]
[Singularity.Net](http://Singularity.Net) how are they decentralizing their marketplace?
[/quote]

What was the answer to this?

-------------------------

lastmjs | 2023-12-01 16:22:30 UTC | #26

It says "could not find the event": lastmjs@demergentlabs.org

-------------------------

patnorris | 2023-12-02 15:51:39 UTC | #27

I had planned to record it but unfortunately Google Meet didn't let me. We might have to switch the video call platform to be able to record the meetings going forward.

Concerning Singularity.Net, it was an open question for us how they are achieving decentralization and we didn't have an answer yet. I'm planning on looking into that a bit and will post what I find. If anyone knows more about them and their marketplace though, please share :)

-------------------------

patnorris | 2023-12-02 15:53:14 UTC | #28

ok, thanks, I just sent you an invite. Please let me know in case you didn't receive it.

If anyone else wants a direct invite, please share your email address (or DM me with it) and I'm happy to invite you too

-------------------------

patnorris | 2023-12-06 17:46:39 UTC | #29

as far as I can tell from the their website (e.g. https://dev.singularitynet.io/docs/ai-developers/marketplace/) and whitepaper (https://public.singularitynet.io/whitepaper.pdf), SingularityNet has an on-chain (Ethereum) registry of AI services and the app then combines that with off-chain metadata to display the different services available on the marketplace. The services in the registry are vetted by the SingularityNet foundation and seem to have to adhere to certain (interoperability) standards. Not sure they have to be decentralized though. So in general, it seems to me like the decentralization aspect is mostly the on-chain registry. Haven't seen anything too impressive yet, happy to learn more though if anyone has more insights.

-------------------------

patnorris | 2023-12-07 01:54:38 UTC | #30

Hi everyone, looking forward to our call tomorrow, Thursday 12/07 at 5pm UTC. This is the link to it: https://meet.google.com/ddh-jnpq-dji

Agenda-wise, I think we can take a few minutes in the beginning to share any cool (De)AI-related news we've come across. We can then discuss some of the action items; wish list of needed features/components on the IC, showcasing and promoting IC DeAI projects and developing the GPU subnet as a community effort. As it came up a few times now and there seems to be an interest from several of us, talking about implementation efforts and ideas for vector databases/embeddings within canisters would be another topic. Let's see how much we can cover :slight_smile: 

Would you like to see anything else on the agenda? Do you have any priorities in terms of topics to cover?

For any of the action items, it'd be great to decide if and how we'd like to tackle them as a group as well as see if anyone would like to take the lead on any of them. We could get more into "action mode" that way and push these items forward. What do you think?

See you all on the call,
Pat

-------------------------

patnorris | 2023-12-07 18:21:39 UTC | #31

Sorry I missed the 2min warning for the end of the call. Thank you for the call, I think it was a great discussion and I'm looking forward to continuing it!

As a next step, I'm proposing that we organize ourselves such that we can push the different action items we've been discussing further. From the discussions so far, they seem to be
* Showcasing and communicating DeAI projects and innovations on the IC (within the community and beyond)
* Working out unique value propositions for consumers and users of DeAI
* Educational materials and documentation
* Wish list of features and components that'd be beneficial for development (incl. GPU subnet as a community effort)
* Technical initiatives like on-chain vector databases

What do you think about these? Are any other items missing?

Would you like to work on any of these items or even take the lead?

And I unfortunately just noticed that the screen recording software I used didn't capture the audio correctly, so the recording is not very helpful. Any good solutions for the recordings anyone can propose?
These are the few notes I took:
* migrating libraries: size (below 2MB) -> zipping, no multi-threading
* FAISS: one dependency might be challenging
* Vector db: size limit challenging? e.g. Quadrant (in Rust)
* Wish list: dynamic library (shared across projects, instead of every project hosting library itself), native system AI/LLM, consumer-facing benefits consideration
* @ildefons showcase post: https://forum.dfinity.org/t/introduction-of-motokolearn-v0-1-on-chain-training-and-inference-of-machine-learning-models/25312

Please add to these notes with your own.

-------------------------

icpp | 2023-12-07 20:26:48 UTC | #32

> * migrating libraries: size (below 2MB) → zipping, no multi-threading

The wasm size limit will be removed soon. I heard in the Tooling WG today that chunked wasm upload will be available early next year.

So, I think it is worth the effort to look into porting some the following C++ libraries:
- OpenBLAS
- FAISS  (depends on a BLAS library)

-------------------------

icpp | 2023-12-07 20:38:50 UTC | #33

@ildefons 
Regarding collaboration on porting AI related C++ libraries, would you be OK if we discuss that in the [OpenChat C++ community](https://oc.app/community/cklkv-3aaaa-aaaar-ar7uq-cai/?ref=6e3y2-4yaaa-aaaaf-araya-cai)?

We have pretty lively discussions around issues and solutions for gaming related C++ libraries that will be relevant for the AI libraries as well.

-------------------------

ildefons | 2023-12-07 20:56:36 UTC | #34

100%. I will join the chat

-------------------------

hokosugi | 2023-12-09 04:17:07 UTC | #35

The story of deploying the [knowledge graph](https://www.youtube.com/watch?v=pZSh7C3-Jw8) on ICP that was mentioned in the SingularityNet collaboration, even if technically feasible, seems challenging. SingularityNet's sister team, openCog, will launch an alpha version of Atomspace in late Q1 2024, will ICP be involved in any way?

-------------------------

mnl | 2023-12-09 17:44:25 UTC | #36

regarding `wasmtime` benchmarking idea, I found some interesting snippets, not sure what to make of them yet:

- https://github.com/bytecodealliance/wasmtime/tree/4f2d634ca4291a09003eaba26f989cd544c1a289/crates/wasi-nn
- https://github.com/bytecodealliance/wasmtime/blob/4f2d634ca4291a09003eaba26f989cd544c1a289/crates/wasi-nn/src/witx.rs#L156
- https://github.com/WebAssembly/wasi-nn

-------------------------

patnorris | 2023-12-13 01:41:59 UTC | #37

Hi everyone, we've got a dedicated channel in the ICP Developer Community Discord now: https://discord.com/channels/748416164832608337/1184160869614108873
Thank you @domwoe for creating it! 
I hope we can use the Discord channel for all the interesting discussions, ideas and news we cannot fit into our meetings :)

-------------------------

patnorris | 2023-12-13 20:18:24 UTC | #38

Hi everyone, we'll have our next call tomorrow, Thursday Dec 14 at 5pm UTC. This is the link to the video call: https://meet.google.com/ddh-jnpq-dji
As a preparation, @icpp and I talked about the different action items and how we could tackle them as a group. Posting our ideas here and please add yours. We can also discuss these further tomorrow.

* Showcasing and communicating DeAI projects and innovations on the IC (within the community and beyond)
Let's all get our projects on Awesome IC (https://github.com/dfinity/awesome-internet-computer)
How about a GitHub repo for the DeAI WG where we can showcase the projects in more detail, we can link it back to Awesome IC

* Working out unique value propositions for consumers and users of DeAI
Async brainstorming via a Miro board where we can all add ideas, additional discussions via Discord
Dedicated meeting (one of our calls on Thursday where we focus on this topic)

* Educational materials and documentation
We could have a dedicated section on our GitHub repo (to give an overview and starting point, e.g. programming languages, examples, concepts, libraries to use)
Document learning journey (e.g. by someone getting started with DeAI or a DeAI project on the IC)

* Wish list of features and components that’d be beneficial for development (incl. GPU subnet as a community effort)
We could have a Wiki in our GitHub repo (so it can be easily edited in browser) for all of us to add to; how would we like to structure this list/the Wiki?
We've got some ideas from our discussions so far to start off with

* Technical initiatives like on-chain vector databases
We could have Wikis in our GitHub repo dedicated to each technical topic/initiative, also as a way of knowledge transfer between us, how would we like to structure this?

Looking forward to seeing everyone tomorrow!

-------------------------

patnorris | 2023-12-13 22:10:16 UTC | #39

During tomorrow's call, we'll also have a 10min presentation by @ildefons about his work on MotokoLearn and on-chain Machine Learning training plus inference (https://forum.dfinity.org/t/introduction-of-motokolearn-v0-1-on-chain-training-and-inference-of-machine-learning-models/25312). That'll be interesting to learn more about!

-------------------------

branbuilder | 2023-12-14 08:18:08 UTC | #40

Absolutely on board! ELNA.ai was made for this.

-------------------------

patnorris | 2023-12-14 18:14:26 UTC | #41

Thank you everyone for today's call!
Thank you @ildefons for walking us through MotokoLearn (https://github.com/ildefons/motokolearn). I think it's worth sharing your work also beyond the IC community as this must be one of the first, if not the first, on-chain training and inference libraries for some common Machine Learning techniques.

One point mentioned which I thought was interesting were cost comparisons and statistics on how expensive it is to run AI applications on-chain vs off-chain. That could also be helpful then to understand which use cases to focus on and plan the underlying hardware infrastructure around it.

As discussed, we'll move the calls to the Discord voice channel (https://discord.com/channels/748416164832608337/753880564947222568) going forward to have it close to our DeAI channel there and not being cut off after 1h :) thank you @mnl for proposing this idea.

It'd be great to have discussions around the different action items in the Discord and plan some next steps there as well. Any ideas or feedback you have, please also share them there or here (or also send me a direct message) :) Looking forward to continuing the conversation

-------------------------

ildefons | 2023-12-14 19:12:45 UTC | #42

Any suggestion on other forums I could share it?

-------------------------

icpp | 2023-12-17 00:02:29 UTC | #43

How can I join the discord channel?
Clicking the link above does not work for me.

-------------------------

patnorris | 2023-12-17 18:42:52 UTC | #44

Maybe hackernews and Telegram groups, there are ICP ones and also DeAI ones (not IC specific) that could be relevant. As this is Motoko specific, maybe you can also get in contact with @Seb, who's organizing Motoko programming bootcamps, to see if there's a good place to announce it there. On Awesome IC, there's a section for Motoko (https://github.com/dfinity/awesome-internet-computer?tab=readme-ov-file#motoko-1) and there's also an Awesome Motoko repo (https://github.com/motoko-unofficial/awesome-motoko)

-------------------------

patnorris | 2023-12-17 18:45:02 UTC | #45

Are you in the ICP Developer Community Discord (https://discord.gg/nGXAfgQu)? There's a section for Working-Groups and our channel is the latest added there so at the bottom of the list.
Does this link to our channel work? https://discord.gg/t5pgyyWy

-------------------------

icpp | 2023-12-17 23:30:03 UTC | #46

I am in now. Thanks for the link!

I was not using discord, in favor of OpenChat, which is working great for the C++ community, although not as easily discoverable for those in discord.

-------------------------

patnorris | 2023-12-20 16:36:32 UTC | #47

Summary: Meeting 2023.12.14
Project Overview and Technical Aspects: MotokoLearn is aimed at developing a library similar to Python's collect, catering to users who don't require GPUs. It deals with tabular data, not image or sound data, and uses machine learning models like ensemble trees. The technical details involve classifiers, regression models, binary trees, and handling instruction limits in the underlying protocol. There's also a discussion on the Motoko language used for development.

Challenges and Solutions: The conversation in the call highlights challenges like instruction limits and high computational costs. Solutions include using a weight function in the algorithm and chunking processes to fit within instruction limits. The discussion indicates a future improvement plan to accommodate larger sample sizes and reduce costs.

Platform and Integration Aspects: MotokoLearn is developed for the Internet Computer platform. There's a focus on how different hardware pieces, like GPUs, could be controlled and secured for AI applications on IC. Discussions on deploying Angular projects on IC and using Python packages suggest a versatile and developer-friendly approach.

Community and Collaboration: New participants introduce themselves, bringing diverse backgrounds from infrastructure, node provisioning, and software development. They express interest in decentralized AI, particularly regarding the intensity of training AI models and the costs associated with on-chain activities. The idea of a GPU subnet and decentralized control of hardware resources is discussed as a potential community initiative.

Future Plans and Next Steps: The group discusses transitioning their meetings to Discord for better integration and continuity. There's an intent to create a GitHub repository for the group, where they can share knowledge, showcase materials, and coordinate on technical initiatives like vector databases and the GPU subnet.

Security and Neutrality in AI Models: There's a conversation about the importance of having AI models trained within a secure and neutral infrastructure to avoid issues like Trojan networks, where models behave as expected except under certain triggers.

Decentralized Infrastructure and Node Providers: The discussion delves into the concept of decentralized infrastructure on the IC, where node providers contribute to a larger global platform. The idea is to extend control to individual GPU servers so that IC canisters can manage them securely.

In summary, the call covers a range of topics from technical details of MotokoLearn, challenges in AI model training and deployment, the potential of decentralized infrastructure in AI, to community collaboration and future initiatives. The focus is on leveraging the Internet Computer platform for efficient, secure, and cost-effective AI solutions.

-------------------------

patnorris | 2023-12-20 16:41:26 UTC | #48

Hi everyone, I'm looking forward to our next call tomorrow, Thu Dec 21 at 5pm UTC. We'll have the call on Discord this time, in the ICP Developer Community's voice channel: https://discord.gg/tNFkxXSZ 

Agenda-wise, these are some things we'd like to discuss:
Burn deep learning framework and similar initiatives (@mnl)
Next steps for action items (Brainstorming via Miro board, Awesome IC/DeAI, GPU subnet)

What else would you like to see on the agenda?

See you tomorrow!

-------------------------

patnorris | 2023-12-21 21:48:30 UTC | #49

Hi everyone, thank you for today's (2023.12.21) discussion. This is the generated summary:

The meeting encompassed a comprehensive discussion on the integration and deployment of AI models in decentralized environments, particularly focusing on the Internet Computer (IC) platform. Here's a consolidated summary of the entire meeting:

* @mnl led the initial discussion, highlighting challenges in deploying AI models, especially with C++ and Rust-based systems.
* Two AI frameworks, "Burn" (https://github.com/tracel-ai/burn) and "Candle," (https://github.com/huggingface/candle) were introduced. "Candle" is popular but lacks extensibility, while "Burn" offers more flexibility with open backends.
* The group discussed experimenting with these frameworks to enhance AI model integration with Rust and C++, with a focus on stable structures.
* Technical aspects, such as computational efficiency and the size of AI models, were debated, alongside considerations of computation costs for network and canister owners.
* The discussion shifted to the "Custom Canisters" feature, enabling custom canister types in dfx.json for user-friendly automated conversions.
* Marcin proposed a custom canister type for AI to streamline model deployment from platforms like Hugging Face.
* The group emphasized improving the developer experience in deploying AI models, regardless of whether they're on decentralized infrastructure.
* A potential AI-focused DAO (Decentralized Autonomous Organization) for infrastructure management was discussed.
* Challenges in using external orchestration platforms like Ray (https://github.com/ray-project/ray#readme, https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview#heading=h.iyrm5j2gcdoq) for distributed GPU compute within the IC were highlighted, considering network boundaries and data transfer limits.
* A participant introduced the Ionet Project, seeking clarity on its workflow, particularly regarding booking and deploying on a PPU (Processing Power Unit).
* @icarus , a systems engineering expert shared insights into the technical documentation of software and hardware, stressing the importance of security in decentralized systems.
* The idea of building a GPU subnet within the IC system was discussed, envisioning a secure and efficient model distinct from current Web 3 and Web 2 platforms.
* A request was made for a private session to delve deeper into the technicalities of building infrastructure like Ionet.
* Participants were reminded to contribute to testimonials and brainstorming sessions, focusing on the GPU discussion.
* Testimonials: see Marcin’s post and thread in our Discord channel: https://discord.com/channels/748416164832608337/1184160869614108873/threads/1187456278373597224
* Same ideas can be posted to @lastmjs ’s tweet: https://twitter.com/lastmjs/status/1737149036241563910
* Brainstorming with Miro: see @icpp ’s post in our Discord channel: https://discord.com/channels/748416164832608337/1184160869614108873/threads/1186701135822725120
* The meeting schedule for next week was left open due to the holiday season, encouraging asynchronous discussions on Discord.
* The meeting concluded with holiday wishes, acknowledging the productive discussions and looking forward to future collaborations.

Throughout the meeting, there was a clear focus on enhancing AI capabilities within a decentralized framework, balancing technical feasibility, security considerations, and ease of development.

-------------------------

patnorris | 2024-01-02 14:54:39 UTC | #50

Hi everyone, happy new year! I hope you all had a great transition into 2024 :slight_smile: 
We can pick up our regular calls this Thursday, Jan 4 again. As before, the calls will be on Thursdays at 5pm UTC and we'll use the voice channel in the ICP Developer Community Discord: ⁠
Discord invite link: https://discord.gg/QnV6TNgV
voice channel link: https://discord.gg/YAtmpQ48
Please let @icpp or me know if there's anything specific you'd like to talk about and have on the agenda. 
See you then!

-------------------------

apotheosis | 2024-01-04 04:05:31 UTC | #51

Hey! We plan on joining this working group going forward.
We are really interested in GPU subnet and/or using ICME's zk tech as middleware.

-------------------------

patnorris | 2024-01-04 08:56:38 UTC | #52

Great, looking forward to having you! If there's anything else you'd like to see on the agenda, please let us know.

-------------------------

patnorris | 2024-01-04 21:33:52 UTC | #53

Hi everyone, thank you for joining today. This is the generated summary of today's (2024.01.04) discussion:

Introductions and Project Overviews: Several individuals introduce themselves and describe their specific areas of interest or projects they're working on. This includes work on IoT, ICP (Internet Computer Protocol), AI, NLP (Natural Language Processing), and more.

Utilization of Tools and Platforms: The conversation kicks off with acknowledgments of the existing tools and platforms and how participants are engaging with them.

Blockers and Challenges: Participants discuss current challenges, such as instruction limits and memory constraints in AI development, and how these might be affecting their projects.

Technical Solutions and Ideas: There's an in-depth technical discussion about potential solutions to the challenges presented, including using specialized subnets for latency-sensitive applications, utilizing stable memory for large model weights, and the impact of instruction limits on AI capabilities.

Community and Action Items: The group also talks about leveraging forums and other community platforms to discuss these issues more broadly and gather support. They propose creating threads to discuss specific problems like instruction limits and memory issues.

Timeliness and Efficiency: There's a concern about the timeliness of responses, especially in voice applications, and how latency can affect user experience. The importance of balancing speed and accuracy in AI responses is highlighted.

Future Directions and Queries: The conversation turns to the future, considering how new features like websockets might influence development and the potential for continually evolving infrastructure to meet AI needs.

Integration of GPUs in Nodes for AI: The participants discuss the potential and challenges of integrating GPUs into the nodes of the network to facilitate more robust AI and machine learning tasks. They explore the implications, including the technical feasibility, consensus mechanism modifications, and the significant cost associated with high-end GPUs.

Data Privacy and Security: The conversation also focuses on ensuring stringent data privacy and security measures. Participants express the need to adhere to strict data regulations like the EU's and discuss using cryptographic keys and client-side encryption to enhance user data security. They emphasize the user's ownership and control over their data and the platform's inability to access or share this data without consent.

Technical and Financial Implications: There's a detailed discussion about the technical roadmap, the financial implications of adding GPUs to the nodes (referred to as Gen. 3 specs), and the need for replicated compute across nodes. The need for transparency in the development process and a clear understanding of the costs involved is highlighted.

Community Involvement and Feedback: The participants mention the importance of community involvement, brainstorming sessions, and feedback mechanisms. They discuss using Discord channels and forums for ongoing discussion and updates, reflecting a collaborative approach to problem-solving and innovation.

-------------------------

jeshli | 2024-01-05 03:07:43 UTC | #54

**Query Structure Modification**

To enhance AI capabilities and reinforce security on the Internet Computer (IC), I propose a modification to the current query structure. By introducing properties like `ic_cdk::query(cycles_fee=true)` or `ic_cdk::query(cycles_fee=true, composite_query=true)` , we can responsibly increase instruction limits per query—currently capped at 5 billion for queries, 20 billion for updates, and 200 billion for initialization. This change not only maintains essential safeguards against DDOS attacks but also significantly broadens the potential for AI on the IC. The impact on AI inference would be substantial, enabling larger model sizes, cost-effective resource utilization, and notably faster response times. Providing AI developers these tools will also prove to be a crucial step towards the efficient training of ML models on the IC.

-------------------------

jeshli | 2024-01-05 03:19:15 UTC | #55

**Proposal for GPU Integration in Dfinity's Gen 3 Nodes: Embracing Small-Scale GPUs for Scalability and Efficiency**

In the context of Dfinity's evolving technology, particularly for the upcoming gen 3 node specifications, I propose a strategic shift in the GPU integration approach. Instead of opting for the high-end, energy-intensive H100 GPUs, I recommend the adoption of many more but smaller, less powerful GPUs. This recommendation is grounded in the capabilities of the current gen 2 nodes, which are equipped with 500 GB RAM and robust processing power. These nodes adeptly parallelize processing and concurrently run numerous single-threaded WASM canisters, each limited to 4GB of RAM consumption.

While GPUs offer significant improvements in execution parallelization, their architecture does not support task-specific parallelization effectively. This limitation means that using GPUs with 80GB VRAM for a task that only requires 1GB would still monopolization the entire GPU resource. While the allure of state-of-the-art (SotA) hardware is understandable, the integration of any form of GPU into Dfinity's nodes would mark a significant advancement from their current capabilities.

Starting with smaller GPUs presents a scalable and cost-effective solution. It not only complements the existing parallel processing features of the nodes but also provides the flexibility to incrementally increase GPU power in response to growing demands and evolving task complexities. Most if not all deep learning models can be split across many GPUs. This approach is not just about upgrading hardware; it's about aligning technological enhancements with strategic goals for scalability, efficiency, and future growth.

-------------------------

icpp | 2024-01-06 03:27:46 UTC | #56

@jeshli ,
Those are great proposals. 

I wonder if your query proposal is a good candidate for a formal NNS proposal. 

Does anyone in the WG have experience with submitting an NNS proposal?

-------------------------

jeshli | 2024-01-06 03:43:50 UTC | #57

Thank you for finding the [Query Charging](https://forum.dfinity.org/t/community-consideration-explore-query-charging/19247) forum thread and [cross-posting](https://forum.dfinity.org/t/community-consideration-explore-query-charging/19247/31?u=jeshli). That thread provided insightful information on the topic:

1. **Current Situation**: As we know, canisters on the IC are only charged for updates, not for queries. Dfinity agrees that this leads to an imbalance as canisters with more update traffic bear higher costs, inadvertently subsidizing those with more query traffic. This discrepancy, which originated from technical challenges at IC's launch, contradicts the principle of fair resource consumption charging and is seen as unsustainable.
2. **Technical Background**: The absence of query charging initially stemmed from the complexity in achieving consensus among all nodes on the cycles to charge for queries. This is complicated by the fact that queries are executed by a single, non-replicated node.
3. **Gradual Implementation Proposal**: To lessen the impact on canisters unprepared for query charges, a gradual implementation of query charging is suggested. This would involve incrementally increasing query fees from 0% to 100% over several months and would start on one subnet and before pushing to the entire network.

It appears that the IC team is already exploring query charging, recognizing the need to avoid making queries entirely free. The gradual implementation approach seems prudent. The critical aspect now might be to discuss with Dfinity the idea of increasing the instruction limit. Ideally, the IC team, as protocol experts, could find an optimal solution where more instructions linearly scale with cycle costs. Since development is already underway, our primary role could be to emphasize the importance and urgency of this consideration. 

I would happily join you in any effort to go through a more formal process as well, because it could be a fun learning process.

-------------------------

icarus | 2024-01-06 13:06:00 UTC | #58

[quote="icpp, post:56, topic:24621"]
Does anyone in the WG have experience with submitting an NNS proposal?
[/quote]
@icpp and @jeshli 
I have submitted required Node Provider proposals to the NNS and while a governance proposal would be different in format I doubt it is any more difficult (as in "easy").

It is done using the dfx cmdline tool running in an env where a hotkey links the dfx proposal submission to your neuron with at least 10 ICP in it for the proposal cost (returned if the proposal is executed not rejected)

Happy to help with this and if we (the WG) want to submit the proposal when it is ready then I can offer my neuron and ICP for the deposit on behalf of the group.

-------------------------

icpp | 2024-01-06 18:27:02 UTC | #59

@icarus ,
Thanks for that explanation and offering the use of your neuron for this purpose. It indeed sounds straightforward. 

Let's do it!

Is there a template doc that we can fill out?

-------------------------

icarus | 2024-01-07 05:42:09 UTC | #60

The formal part of the process would be submitting an NNS proposal of type "Motion" under the topic "Governance" which has no payload data to be automatically executed (if accepted) but contains a simple motionText statement and a full description of the proposed intent to read and be accepted (or not) by the IC voting community.

Therefore the work mostly involves carefully describing the basis for, intent of, consequences, work required, etc of the proposal should it be accepted.

A clear example of this is illustrated in the [Community Consideration: Explore Query Charging](https://forum.dfinity.org/t/community-consideration-explore-query-charging/19247) thread itself which began with an RFD (request for discussion) then was formed into a draft of the proposal text  [here](https://forum.dfinity.org/t/community-consideration-explore-query-charging/19247/24) which was then formally submitted to the NNS [here](https://nns.ic0.app/proposal/?u=qoctq-giaaa-aaaaa-aaaea-cai&proposal=123481) as a proposal named "Motion for Query Stats Aggregation".
This was accepted by majority vote and is now in a status of "Executed" which forms a commitment by the IC community to support implementation of that motion in the form of @stefan-kaestle and his team coding the canister query stats feature to support query charging.

If we (as the DeAI WG or @jeshli as the lead on this particular topic) want to make a formal proposal for a specific query charging model then I think we can take this proposal 123481 as a process template and also refer to it in the new proposal to show it is compatible with the already executed proposal 123481

Other than that procedural work I think the best approach is exactly what @jeshli has already initiated in the query charging thread which is to explain our rationale, provide use case and analysis and ask for community feedback and discussion. Then write a proposal description that is specific enough to be clearly understood and executed (by IC dev team) but not so specific that unnecessary technical details are enforced on the implementation

So TLDR: keep going as we are and form a clear concrete proposal that builds on prior work and discussion that could actually be executed by IC devs

-------------------------

icarus | 2024-01-07 11:37:34 UTC | #61

[quote="jeshli, post:55, topic:24621"]
**Proposal for GPU Integration in Dfinity’s Gen 3 Nodes: Embracing Small-Scale GPUs for Scalability and Efficiency**

In the context of Dfinity’s evolving technology, particularly for the upcoming gen 3 node specifications, I propose a strategic shift in the GPU integration approach.
[/quote]
 Should we separate this proposal discussion out into a separate thread and link back to here as the origin of our discussion about GPU subnets, Gen3 replica aspects and requirements we are seeking from GPU compute calls from IC canisters? 

I also responded to the GPU paragraph of your post on the [Explore Query Charging](https://forum.dfinity.org/t/community-consideration-explore-query-charging/19247/35?u=icarus) thread.

@jeshli if you agree please go ahead so we can call in the relevant Dfinity engineering team contacts to ask questions about current specs and intentions.

We should also be aware that there is significant interest among the wider IC community about GPU enabled IC subnets due to the current focus on AI technology. I think we can facilitate effective information sharing with the community by keeping it focused on IC developer requirements for current on-chain AI canister development work such as yours, @icpp , ELNA @branbuilder , Kinic and others

-------------------------

patnorris | 2024-01-11 19:10:02 UTC | #62

Hi all, thank you for the call today. This is the generated summary:
* Amit from the Elna team introduces his team members and discusses their work on building an AI platform for agents.
* Various members, including Alex from the Elna team, introduce themselves and their work, focusing on areas like Natural Language Processing and large language models.
* Technical discussions touch on the development of an on- chain vector DB and the use of retrieval-augmented generation for their platform.
* The group reflects on previous meetings and ongoing project updates, discussing technical aspects like increasing stable memory and potential GPU canister usage.
* Technical challenges related to AI agents and the implementation of large language models are discussed, with emphasis on memory limitations and the need for increased instruction limits.
* Participants seek feedback on technical feasibility and anticipate future developments, including increased memory capacities. It is likely that the heap memory can be increased to 16GiB this year (from 4GiB currently).

* The conversation shifts to the broader ICP ecosystem, discussing the development and use of AI and machine learning models on-chain.
* There's a debate on charging for query calls, considering the implications for users and potential abuse.
* The need for educational resources for both developers and the general public is highlighted, emphasizing the importance of tutorials and learning materials.
* The importance of showcasing projects within the ICP ecosystem is discussed, including the idea of a dedicated website for this purpose.
* SEO concerns for sites hosted on ICP are mentioned, balancing technical showcases with practical deployment considerations.
* The conversation ends with a mention of developing an image recognition algorithm as an application on the ICP.

Next steps:
* Please all add your use cases, ideas and motivation to the different feature discussions:
* GPU: https://forum.dfinity.org/t/community-consideration-gpu-use-cases-and-specifications/26309/5?u=jeshli
* Query charging: https://forum.dfinity.org/t/community-consideration-explore-query-charging/19247/35
* vetKeys/privacy: https://forum.dfinity.org/t/threshold-key-derivation-privacy-on-the-ic/16560/143
* And let's brainstorm ideas for how we can provide educational materials for AI on the IC and showcase our projects and initiatives at the same time (please share any ideas here or on Discord: https://discord.com/channels/748416164832608337/1184160869614108873/1195076688300736592)

Thanks again @jeshli and @icarus for getting these threads going!

Please let us know if you have any ideas, feedback or questions. Looking forward to continuing the conversation!

-------------------------

hokosugi | 2024-01-14 03:28:38 UTC | #63

I have been thinking about why it should be DeAI and not AI.
I would like to add that no matter how meaningful and significant it is, it cannot take market share from existing centralized servers if the UX is poor, so I would like to add that, of course, it must clear technical hurdles, obstacles to smooth UX by being web3, and extra hassles.

DeAI will solve the challenges of centralized server-based AI:
* Digital sovereignty
* Privacy protection
* Improved security by eliminating intermediaries
* Non-transparent data processing and auditing
* Trust in server administrators
* Control of the server administrator.

These are much the same concepts as web3, which is trying to solve web2 issues, but they are more serious and must be solved considering the impact of AI on society at large.


What DeAI will bring:
* User-managed and driven AI
* AI that leverages users' privacy data
* Highly secure Serverless AI services
* Transparent data processing and auditing
* Power-distributed server management

If there are any points that we have missed or should add, please feel free to point them out in that case.

-------------------------

hokosugi | 2024-01-17 05:45:33 UTC | #64

[quote="patnorris, post:62, topic:24621"]
Participants seek feedback on technical feasibility and anticipate future developments, including increased memory capacities. It is likely that the heap memory can be increased to 16GiB this year (from 4GiB currently).
[/quote]

Is this likely to be achieved? My understanding is that to get heap memory above 4GB you need to change from 32-bit wasm to 64-bit wasm. the Dfinity roadmap says "[Potential future explorations](https://medium.com/dfinity/webassembly-on-the-internet-computer-a1d0c71c5b94)", so I don't think it is in the development stage yet! Or is there another way?

-------------------------

patnorris | 2024-01-17 13:40:59 UTC | #65

Thank you for sharing the link :) true, it'd be good to know then if the heap increase via 64-bit wasm might have made it to the development or planning stage yet and if we could maybe motivate the development further with our use cases. I'll put this on the agenda for our call tomorrow :+1:

-------------------------

jeshli | 2024-01-17 15:54:59 UTC | #66

According to the [Memory64 proposal overview on GitHub](https://github.com/WebAssembly/memory64/blob/main/proposals/memory64/Overview.md), the implementation status for 64-bit WASM in various platforms is as follows:

* Spec interpreter: Done
* V8/Chrome: Done
* Firefox: Done
* Safari: Status unknown
* WABT: Done
* Binaryen: Done
* Emscripten: Done

The technical foundation for 64-bit WASM is largely in place, with major browsers and tools already supporting it. Given the current trajectory of WASM-64's development, it seems that it will become part of the WebAssembly standard soon. As for Dfinity's ability to incorporate this change, my understanding is that it will not be overly burdensome and would begin immediately after 64-bit WASM is officially released.

-------------------------

hokosugi | 2024-01-17 21:48:42 UTC | #67

Thank you both very much.
I was not aware that the wasm standard had not yet been adopted, and since increasing heap memory is a lifeline for DeAI, I hope it will be a priority on Dfinity's development roadmap.
I would like to know what Dfinity thinks about the huge potential demand for AI that can be coupled with sovereignty cloud and digital sovereignty, which in turn should be considered a marketing advantage, differentiation from other chains, etc. I would like to know what Dfinity thinks about this.

-------------------------

patnorris | 2024-01-18 19:19:45 UTC | #68

Hi everyone, thank you for today's discussion. This is the generated summary (call on 2024/01/18):
* Meeting Structure and Topics: The group will aim for a structured approach for future meetings, focusing on specific topics each week, starting from basic to more advanced discussions. This structure aims to build a comprehensive knowledge base over time.
* Summarizing Meetings: Meeting summaries will be created for community contribution and to aid newcomers. This approach is to document discussions for future reference.
* Utilizing DFINITY Wiki and Other Platforms: While considering the DFINITY Wiki for formal documents, the group acknowledged the need for a more dynamic workspace. Discussion on various platforms for content development and hosting took place.
* Twitter Spaces for Outreach and Education: The group plans to leverage Twitter spaces for broader outreach and education. Regular Twitter space sessions are to be held for topic discussions, resource sharing, and audience engagement. ELNA will have a Twitter space tomorrow (Jan 19): https://twitter.com/elna_live/status/1747156897273364555
* Simplifying Technical Jargon: Emphasis was placed on making complex AI and blockchain concepts accessible to the public. Using analogies, infographics, and videos to simplify terminology was suggested.
* Content Creation for Educational Platforms: There was a proposal to write introductory articles on AI and machine learning on the ICP, to serve as starting points for new developers. These articles would be published across various platforms.
* GPU Integration in Infrastructure: The group discussed different models for GPU integration in ICP infrastructure. This included full integration into the consensus algorithm and using GPUs for query processing. The potential for off-chain GPU compute services was also explored.
* 64-bit WebAssembly (WASM) Discussion: The discussion touched upon the benefits and implications of moving to 64-bit WASM, specifically regarding heap memory and algorithmic limits.
* Economic Incentives for GPU Hardware: The idea of charging for queries and providing economic incentives for investing in GPU hardware was discussed. This is linked with the faster processing capabilities of GPUs.
* Potential NNS Proposal for Query Charging: The group considered submitting a proposal to the Network Nervous System (NNS) regarding query charging, but decided to wait for further information from the DFINITY team.
* Feedback Mechanism and Tech Stack Insights: The group showed interest in understanding different projects' tech stacks and the kind of workloads they intend to run, especially concerning training versus inference.
* CUDA Dependencies and Framework Abstraction: A technical discussion around CUDA dependencies and the abstraction provided by frameworks like PyTorch 2.0 and TensorFlow was brought up. The focus was on the ability of these frameworks to run computations on different hardware, like AMD, beyond the standard CUDA library.

Overall, the meeting emphasized structuring future discussions for comprehensive knowledge-building, exploring GPU integration in various models, simplifying AI and blockchain concepts for a broader audience, and probing into the potential of 64-bit WASM and its implications. The group also stressed the importance of creating educational content and exploring economic models for GPU investment.

-------------------------

lastmjs | 2024-01-19 06:14:06 UTC | #69


Have you missed 64 bit Wasm heap? Super important feature

-------------------------

patnorris | 2024-01-19 13:09:18 UTC | #70

Agreed, I think this might even be one of the main initiatives we as a group can tackle; to help with 64 bit Wasm heap (if only getting this feature potentially prioritized, maybe more). Would be a really exciting enhancement, most likely also beyond our projects :)

-------------------------

icpp | 2024-01-21 17:18:13 UTC | #71

@evanmcfarland ,
You mentioned that you like qdrant for your vector database in the RAG.

Have you also tried FAISS?
https://github.com/facebookresearch/faiss

I am having a very good experience with it, and am considering to try to port it to the IC, because it is a pure C++ library. 

@branbuilder ,
What vector database are you porting? Or are you building a new one from scratch?

-------------------------

evanmcfarland | 2024-01-23 16:59:03 UTC | #72

@icpp I'm sure FAISS would be an excellent starting point for many people's needs, but it does not fit my use case personally.

Since it's a 'vector library' instead of a 'database,' it's not by itself meant to work with changing datasets. In my app, users upload/delete books that get data added/deleted to/from the DB in real time so I have to stick with Qdrant.

The good news for me is that my outbound https requests to VectorDBs are proving affordable and fast enough for decent usability in the interim.

-------------------------

branbuilder | 2024-01-24 17:41:33 UTC | #73

Sorry my previous post was not live due to some network issue.

Yes we are trying to port "qdrant"
meanwhile we are also trying a minimal vector db from our side 

@evanmcfarland
FAISS looks promising, will have a try.

Its great if you can port it to the IC

In ELNA platform we are planing to support multiple solutions, will surely try to accommodate the same to ELNA platform ones you have ported it .
please have a look our video explanation about ELNA 
https://www.youtube.com/watch?v=crQ__sBYOX0&t=10s

-------------------------

jeshli | 2024-01-27 13:54:42 UTC | #74

@mnl [this is the fork of Tract Crate](https://github.com/modclub-app/tract-ic-ai) that I altered in order to run LLMs within a canister in a local environment. [This repo is the IC application of that Crate](https://github.com/modclub-app/rust-connect-py-ai-to-ic) which that I have used for testing locally and that I am actively improving. 

Honestly, this framework would likely be the best: https://crates.io/crates/ort as it is the most advanced Onnx Runtime Environment and comes with running examples that can easily download tokenizers and models from Hugging Face.

[Edit: repos have been moved to modclub repo umbrella]

-------------------------

mnl | 2024-01-25 18:18:30 UTC | #75

nice! 

regarding `ort` they don't list wasm-linux as compatible
 - https://github.com/pykeio/ort/blob/2fdba8f021924d40c6281427b4f8142e932d9bfb/docs/setup/platforms.mdx#L20
- https://github.com/pykeio/ort/issues/75

I wonder how much work it would be to get it to work...

I love they have the feature to be able to run stuff from huggingface directly

-------------------------

jeshli | 2024-01-25 18:47:04 UTC | #76

After I finish my first draft of my demo application I will start examining to see what aspects of their "feature to be able to run stuff from huggingface directly" that I can imitate and integrate into my tract fork. If nothing else, it is promising for achieving that aim with ONNX for either Burn-rs or Tract because there already exists for Onnx Runtime Environment.

-------------------------

mnl | 2024-01-25 18:57:35 UTC | #77

I don't necessarily think it has to be part of the ML library; the HF model could be converted into ONNX format during build step in dfx.json by a separate tool, eg https://huggingface.co/docs/transformers/serialization

-------------------------

patnorris | 2024-01-25 19:59:30 UTC | #78

Thank you all for today's discussion! This is the generated summary (2024.01.25):
* Introduction to tract-ic-ai by Jeshli (https://github.com/jeshli/tract-ic-ai): A pruned-down version of 'tract', which supports 85% of ONNX functionality, a generic machine learning model, and parameter storage template. This part focuses on the compatibility of AI models with ONNX format and the challenges of implementing certain functionalities.
* Optimizing 'tract' for AI Applications: The talk goes into the details of optimizing the 'tract' library for core neural network applications, focusing on pruning unnecessary functionalities like Random Forest and Fast Fourier Transform to make it more efficient for specific tasks.
* Large Language Models and ONNX Functionality: There's a detailed discussion about the difficulties of implementing large language models like GPT-2 in the 'burn' library due to the absence of necessary node functions, leading to a preference for pruning down 'tract' instead.
* Rust Connect py AI to IC (https://github.com/jeshli/rust-connect-py-ai-to-ic): Introduction of a library for demonstrating downloading GPT-2 from Hugging Face, segmenting it, and running it efficiently. This part of the conversation is about making AI models more accessible and deployable in decentralized networks.
* Front-End Development and Tokenizer Implementation: The discussion touches on front-end development, particularly implementing a tokenizer and the feasibility of running Rust code in the browser for AI applications.
* Community Collaboration and Sharing of Work: The call includes moments where participants discuss sharing their work on forums and collaborating on AI projects, reflecting the collaborative nature of the community.
* Interest in Impact of Decentralized AI: There is a keen interest in understanding and exploring the impact of decentralized AI, even from those without a deep background in machine learning or AI.
* Potential for GPU Environment Linked to Decentralized Networks: There's a discussion on the potential of connecting a GPU cluster to a decentralized network like the Internet Computer (IC) to leverage the advantages of smart contracts and enhance performance.
* Privacy Concerns with Decentralized AI and VetKeys: The conversation touches on the challenges of ensuring privacy in decentralized AI. Concerns are raised about the security of VetKeys and the possibility of memory snapshots extracting keys from malicious nodes.
* GPU Infrastructure and Confidential Computing: The discussion also delves into GPU infrastructure and the concept of confidential computing, which includes secure computation enclaves and encrypted computation. The potential of new features in AMD CPUs and NVIDIA GPUs for secure data processing is discussed.
* Challenges of DDoS Attacks on Decentralized Networks: The group mentions ongoing issues like DDoS attacks on decentralized networks, indicating that there are still challenges to be addressed in ensuring the security and reliability of such systems.
* Data Sharing in Multiplayer Mode: The conversation highlights the multiplayer mode of decentralized AI, where diverse players can benefit from shared data and native integration with blockchain networks. This leads to a discussion about how manufacturers and producers could use shared data for optimizing products and sales.
* Single Player Mode Advantages: The single player mode, focusing on individual users or companies, is discussed in relation to privacy and the reduction of costs by eliminating middlemen. The group talks about the potential for privacy improvements with upcoming features on the IC.
* Concerns About Encryption and Data Security: The participants discuss the inherent risks in data sharing, acknowledging that encryption is secure until it's broken. The conversation highlights that no platform, including the IC, is immune to future advances in decryption technologies.
* Personal AI Solutions on the IC: The IC is recognized for enabling personal AI solutions. Individuals can have their own AI models running in canisters they fully control, using virtualized hardware rented from the IC. This setup is seen as more feasible than setting up personal hardware for most people.
* Privacy Continuum and Control Over AI: The concept of a privacy continuum is introduced, suggesting that different solutions offer varying degrees of privacy. Having full control over one's AI provides a higher level of security and ensures that the results serve the user's best interests, free from bias.
* Anonymous Deployment of AI Models: A notable feature of the IC is the potential for anonymous deployment of AI models. Users can deploy canisters without attaching their identity or credit card information, which adds a layer of privacy since even if data is exposed, the identity of the individual remains unknown.
* Ongoing Discussion and Follow-Up: The participants express interest in continuing the discussion on the advantages of AI deployment on the IC. There's a plan for a follow-up post to keep the conversation going and gather more ideas on how to leverage the IC for AI advancements.

-------------------------

patnorris | 2024-02-01 22:22:39 UTC | #79

Thank you all for today's call (2024.02.01). We covered a lot of ground, this is the (comprehensive) summary:
* Decentralized AI and Agent Training: There was a strong focus on the potential for training AI models on the IC to cater to specific niches and preferences, including image generation with varying datasets to achieve targeted outcomes like emotions in faces. This underscored the IC's flexibility in AI model training.
* Decentralized Network Benefits: The benefits of a decentralized network in fostering a collaborative environment were highlighted. Such a structure supports the sharing and recording of contributions, enabling participants to be rewarded for their input and fostering a community-driven approach to AI development.
* Leveraging ICP Functionality: A significant portion of the discussion revolved around maximizing the use of Internet Computer Protocol (ICP) functionalities for project enhancement. There was a keen interest in exploring the platform's capabilities further.
* Decentralized Approach to Training Data: The importance of a decentralized method in gathering and utilizing training data was acknowledged, emphasizing the need for diverse and robust datasets for effective AI model development.
* Addressing Technical Challenges: Technical hurdles, especially those related to understanding and utilizing ICP functionalities, were discussed. The conversation extended to the need for resources, guidance, and a supportive community for developers on the IC.
* Future Directions and Support: There was a call for the development of resources, educational materials, and structures to better support AI project development on the IC. This includes detailed documentation, examples of AI applications, and discussions on hardware requirements and costs.
* Participants expressed the need for a collective repository or space to document and share experimental application limits and GPU pipeline setups, including specific machine types and configurations. They discussed the desire to scale up resources as more become available on the Internet Computer (IC), emphasizing experimental applications still in research mode that could benefit from expanded capabilities.
* The conversation shifted to the technical aspects of running AI-enabled codebases on traditional setups versus the potential of the IC, including the use of virtual machines with GPUs and local setups with advanced graphics cards. The group highlighted the importance of collecting data on resource consumption and performance metrics to better understand how current development and testing practices could translate to operations on the IC, focusing on security, resilience benefits, and the utilization of GPU subnets.
* Discussions also touched on the need for more detailed metrics regarding GPU usage, software stacks, model sizes, and data processing quantities. The idea of summarizing these metrics in a structured way, possibly through surveys or forums, was proposed to foster a broader community contribution.
* Furthermore, the group talked about the potential of using larger memory limits, like WASM 64, to significantly increase canister capabilities, particularly for AI model testing and deployment. There was a consensus on the importance of sharing resources, including benchmarks and code examples, to support the transition to more efficient architectures and the exploration of larger memory models.
* The call highlighted a collective interest in enhancing the IC's infrastructure to better support AI applications, including the need for more resources, better documentation of experimental limits, and the exploration of new technologies like WASM 64 for increased memory. Participants were encouraged to contribute data and insights to help shape the future development of AI capabilities on the IC.
* Community Engagement and Collaboration: The critical role of community engagement in driving AI innovation on the IC was underscored. Participants were encouraged to actively share their projects, questions, and insights, emphasizing a collective effort in exploring decentralized AI's possibilities.
* Exploration of Services and Databases: Interest was expressed in identifying and using existing IC services and databases to enhance AI applications, showing a proactive approach to leveraging existing resources.
* Vector DB Development and Marketplace for AI Tools: Initiatives like the development of a basic vector DB model and the creation of a marketplace for AI tools on the IC were discussed. These efforts aim to foster a rich ecosystem for AI development, enabling easier integration and monetization of AI tools and models.
* Composability, Interoperability, and Scalable Architectures: The discussions included the significance of composability and interoperability in AI applications on the IC, and future talks were planned around scalable architectures for deploying multiple canisters as per demand.
* Recording and Knowledge Sharing: The suggestion to record sessions or provide voice recordings to make discussions more accessible was made, highlighting the importance of archiving and sharing knowledge within the community.

-------------------------

patnorris | 2024-02-09 15:26:57 UTC | #80

Thank you for this week's session (2024.02.08)! Big thanks to @apotheosis who shared all his expertise on zkML and opML with us, and @branbuilder who talked about ELNA's vector db implementation 🙂 this is the generated summary (short version, link to a very long one below):
The group's discussion on Zero-Knowledge Proofs (ZKPs) and optimistic AI/ML on the Internet Computer (IC) highlighted the technology's potential for privacy and scalability in decentralized AI systems. ZKPs, particularly beneficial for privacy-preserving applications, allow for verifying data or computations without revealing underlying information. The IC's architecture is well-suited for ZKPs, offering advantages over platforms like Ethereum by facilitating on-chain storage and verification of proofs. The conversation also explored the challenges of deploying AI and ML models on the IC, such as the need for specialized GPU support and the limitations of WebAssembly for large AI models. Practical applications of ZKPs for AI on the IC were discussed, including privacy-preserving AI models and verifiable computation. The group acknowledged the technical challenges in implementing ZKPs but noted recent advancements that have improved efficiency. The discussion transitioned to vector databases on the IC, focusing on development efforts and the potential for scalable architectures to support AI applications. Enhancements in GPU support and increased instruction limits were identified as crucial for the performance of vector DBs on the IC. Participants expressed interest in sharing developments and collaborating on vector DB implementations.

Please find the detailed summary here: https://docs.google.com/document/d/1e9QImFf7cHmK9SFX2VaaA_bxUiZ7nBQQFruebBEGX84/edit?usp=sharing

-------------------------

icarus | 2024-02-15 13:15:07 UTC | #81

The newly introduced Arcmind AI project is highly relevant to readers of this forum thread. 
Go read about it over here https://forum.dfinity.org/t/arcmind-ai-autonomous-ai-agent-and-vector-db/27491

-------------------------

patnorris | 2024-02-15 21:30:14 UTC | #82

Hi everyone, thank you for today's call (2024.02.15). This is the generated summary (short version, please find a link to the comprehensive version below):
The Decentralized AI Technical Working Group for the Internet Computer's call focused on two main areas: establishing benchmarks for AI on the IC and presenting a Vector DB implementation by the ELNA team.

Benchmarking and AI Deployment on the IC:

* The session opened with introductions to foster community connections, followed by discussions on deploying TensorFlow models using Azle and TensorFlow JS, highlighting the IC's potential for decentralized AI services.
* @icpp detailed the memory requirements for large language models (LLMs), emphasizing the need for accurate benchmarking to understand computational demands and optimization strategies, including half-precision computation to reduce resource requirements.
* Technical challenges of AI deployment were discussed, focusing on memory limitations and the search for decentralized alternatives to centralized cloud services.
* The group explored technical solutions to these challenges, such as the potential shift to WASM 64 support for improved efficiency and the importance of collaborative tool development for model deployment.

Vector DB Implementation and Future Directions:

* The ELNA team (@branbuilder) introduced Vector DB, emphasizing its role in managing high-dimensional vectors for AI models and facilitating fast, accurate similarity searches essential for tasks like semantic search.
* A technical overview explained Vector DB's operation, including the use of the HNSW algorithm for efficient indexing and cosine similarity for measuring vector closeness.
* Future enhancements for Vector DB were discussed, including supporting various similarity measures and integrating more efficient vector embedding models to improve search capabilities.
* Challenges of deploying AI models on the IC due to computational constraints were acknowledged, with strategies discussed for optimizing deployments through smaller models or task segmentation.
* The session concluded with a focus on collaborative opportunities, the development of educational resources, and the potential for community-driven efforts to advance decentralized AI on the IC.

Long summary: https://docs.google.com/document/d/1hO1n3DLZpgsTA5P1pH9WfKo_se78W5gr0QOWyBKd2Q4/edit?usp=sharing

-------------------------

patnorris | 2024-02-22 23:15:17 UTC | #83

Hi everyone, thank you for today's call (2024.02.22). This is the generated summary: 
A new participant introduced themselves as a newcomer to the ICP ecosystem, expressing curiosity about ICP's developments in AI. The group's purpose was clarified as facilitating exchanges between projects working on decentralized AI on the ICP and as a resource for anyone interested in the topic. The discussion included the establishment of a repository to share resources, projects, and facilitate collaboration within the ecosystem (see the DeAI GitHub repo: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer).

The conversation then shifted to the potential applications and advantages of decentralized AI on the ICP (also see https://dscvr.one/post/1153371204873059295/unique-advantages-of-decentralized-aithese-are-the-results-of-ic), including privacy concerns (also see this link on consent receipts as mentioned around privacy regulations: https://www.ubisecure.com/data-protection/what-is-consent-receipt/), the ability to run AI components like language models on-chain, and the interest in community or co-ownership models. The importance of keeping training data confidential and the potential for zero-knowledge machine learning AI was also highlighted.

The group discussed the concept of AI-powered DAOs, the potential for AI to manage financial transactions and assets, and the integration of Bitcoin and Ethereum networks with the ICP for enhanced financial services. The idea of AI participating in decision-making processes, especially in crucial areas like finance and public good decisions, was explored. The call also touched on the importance of transparency and hack resistance in decentralized AI systems.

The conversation also included technical details about the ICP's upcoming features, such as end-to-end encryption for data privacy (https://internetcomputer.org/blog/features/vetkey-primer) and the development of AI models to detect DAO vote anomalies and secure online transactions. 

The participants discussed the limitations and potential of AI models on the Internet Computer (IC), including the challenges posed by instruction limits and the efficiency of using different programming languages for AI development on the IC. There was an acknowledgment of the need for slimmed-down models to work within the IC's current limitations, with a specific mention of the performance and usability constraints when working with GPT-2 models due to instruction limits.

A significant part of the discussion revolved around two main project ideas related to blockchain security and how these could be integrated into the upcoming hackathon. The first project idea involved creating public voting records on DAOs (Decentralized Autonomous Organizations) to facilitate analytics and possibly prevent malicious activities. The second idea focused on developing an AI model to detect blockchain hacks early by analyzing on-chain data and flagging suspicious activities.

The participants also considered the practical aspects of implementing these ideas, such as data generation and preparation of materials for the hackathon. There was a consensus on the importance of making these projects accessible to hackathon participants, possibly through a dedicated repository or as part of the hackathon's challenge descriptions.

Additionally, the conversation touched on the potential for collaboration with the Scalability and Performance working group to discuss scalable architectures for decentralized AI applications. This collaboration could lead to a joint session focusing on the technical feasibility and efficiency of deploying AI models on the IC, comparing different programming languages and frameworks.

The call also highlighted the value of visual and interactive presentations, such as demos and walkthroughs, to make complex technical topics more accessible and engaging for a wider audience. There was an interest in recording and sharing these presentations to build a knowledge base that could support the broader community of developers interested in decentralized AI on the IC.

-------------------------

patnorris | 2024-02-29 19:46:35 UTC | #84

Hi everyone, thank you for today's call (2024.02.29.). It was more informal as we were a smaller group. We thus mostly talked about the projects we're working on.

These are some specific notes relevant for future calls:
Interest was expressed in talking more about how external computing resources (i.e. outside the IC network, like dedicated GPU networks or traditional cloud service providers) could be integrated and complement the infrastructure available to IC devs in their projects.
Concerning the different performance bottlenecks we've been talking about as a group (e.g. instruction limits, memory, GPU), it was proposed that we could discuss a divide-and-conquer approach where we collaborate as a group and members contribute by running different experiments around the performance metrics. For benchmarking, we could use a recently released tool for IC canisters (https://github.com/dfinity/canbench/), and then put the different experiment results into the benchmarking section in our DeAI repo (https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/BenchmarkingStats)

Please let us know which future topics you'd like to see on the agenda and which type of calls you'd like to have.

-------------------------

abk | 2024-03-04 15:16:03 UTC | #85

Hey DeAI folks,
For the next Scalability & Performance WG we're going to invite you all to come and ask questions to a handful of the Dfinity experts on IC internals and performance. We'll have @christian, @ielashi, @stefan.schneider, and @ulan so please come with questions/comments/discussion points!

The date will be March 14th at 5:30pm-6:30pm CET ([zoom link](https://dfinity.zoom.us/j/98492107298?pwd=QlJQNnU5SmhKY01vbTRkYlEyZ2Rzdz09)). Note that the US starts daylight savings the Sunday before this, but not Europe so the time difference might be different than normally for some of you.

-------------------------

patnorris | 2024-03-04 16:10:58 UTC | #86

Hi @abk, thanks a lot for this great initiative, really looking forward to our session together!

-------------------------

patnorris | 2024-03-07 20:33:34 UTC | #87

Hi everyone, thank you for today's call and special thanks to @massimoalbarello who presented his and @ilbert 's work on integrating off-chain infrastructure! This is the generated summary (short version), please find the long version here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.03.07

[Massimo](https://forum.dfinity.org/u/massimoalbarello/summary) showcased the project he and [Luca](https://forum.dfinity.org/u/ilbert/summary) have been working on aimed at [enabling DAOs to manage off-chain web3 services](https://github.com/omnia-network/ic_akash) through an orchestrator canister on the ICP, focusing on decentralizing control and ensuring transparency by potentially governing it through a DAO. The project, still in early stages, looks to facilitate deployment on networks like Akash.

The discussion extended to integrating AI infrastructure on the ICP, highlighting the technical challenges of creating GPU subnets and decentralizing AI service management to enhance the platform's capabilities. Key topics included the complexity of managing resources across different networks, the engineering hurdles in GPU design for AI, the significance of a DAO gateway as a bridge to external compute resources, and the importance of confidential computing for off-chain AI models.

Future directions involve exploring GPU-enabled VMs secured by confidential computing, ensuring payments in ICP, and fostering a community-driven approach to development. A forthcoming joint meeting with the Scalability Group aims to delve into scalability issues, indicating an interest in collaborative efforts to address the challenges of decentralized AI and performance on the ICP.

-------------------------

skilesare | 2024-03-12 01:07:55 UTC | #88

Check out the modulus presentation at https://www.youtube.com/watch?v=JZGm1WNU7IE&t=24205s. Might be relevant.

-------------------------

patnorris | 2024-03-12 12:00:26 UTC | #89

Thank you for sharing!

Relevant indeed as we just had a zkML/oppML session the other week :slight_smile: 

I found this chart he drew pretty interesting 
![image|690x387, 50%](upload://8GYvKa3W9A8AuNhY2rLZZ0O1BbO.jpeg)
@apotheosis, where do you see the overhead of proving over computing currently and where do you think it's going (and which factor is realistic to achieve)?

-------------------------

apotheosis | 2024-03-12 17:55:35 UTC | #90

The techniques for increasing prover speed are well known at this point.

- Add more provers and parallelize. (Continuations, sharding)
- Hand-craft a scheme like Modulus does with GKR.
- Use zk-precompiles.

There are some smaller models that run locally and work fine for inference with ZK. There are some larger models that need to be run through specialized proving machine.

Proving 'training' is much further away.

-------------------------

rodoard | 2024-03-13 00:59:30 UTC | #91

where can we follow your status on Motoko ML?  I am doing a first dive into ML I am interested in developing a scientific model.

-------------------------

ildefons | 2024-03-13 10:08:00 UTC | #92

Thank you for your interest. The best way to follow the progress of Motokolearn is the github project page. Next is the linear algebra library. Hope soon I can dedicate more time and discuss more features.

-------------------------

jeshli | 2024-03-14 14:10:04 UTC | #93

ModClub recently published an article detailing my journey and experiments with AI on the Internet Computer. You can read about my experience here: [The Internet Computer's AI Frontier](https://medium.com/@modclub/the-internet-computers-ai-frontier-5cd5a3d7e473).

There's still a considerable path ahead in refining the libraries and enhancing the efficiency of our tools. One of my immediate objectives involves a thorough analysis of the tools at my disposal, using [Canbench](https://github.com/dfinity/canbench). This effort will provide the community with insights, which will be shared through in the benchmarks section of the [DeAI Working Group Git Repo](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/BenchmarkingStats).

-------------------------

patnorris | 2024-03-14 17:41:00 UTC | #94

Thank you for sharing the article which already has a bunch of interesting stats and insights and your initiative to collect more :muscle: Maybe we can coordinate our benchmark collections around Canbench

-------------------------

patnorris | 2024-03-21 20:45:17 UTC | #95

Hi everyone, thank you for today's call (2024.03.21). This is the generated summary (short version, please find the long version here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.03.21):
During the call, the DeAI Working Group delved into the potential and challenges of integrating AI models and specialized hardware with the Internet Computer (IC), exploring the utilization of both on-chain and side-chain resources. The discussion covered the significance of adopting common data formats like ONNX for AI model interoperability, and the role of ONNX runtime in facilitating AI deployments on the IC. The group emphasized the need for common tooling and protocols to aid dApp development, and explored federated learning as a method to train models collaboratively while keeping data private. The conversation also ventured into how the IC's tokenomics could incentivize contributions to decentralized AI training and applications. The importance of interoperability, standards for growth, and the exploration of future AI applications, including the Metaverse, were highlighted. The discussions pointed towards a shared desire for more transparency, better coordination, and clearer communication within the community to harness the IC's capabilities for complex computational tasks and to push the boundaries of what's currently possible with GPU acceleration and AI integration.

-------------------------

Samer | 2024-04-02 13:30:36 UTC | #96

When is the next meeting?

-------------------------

evanmcfarland | 2024-04-02 13:47:40 UTC | #97

5 PM UTC every Thursday. https://discord.gg/tNFkxXSZ

-------------------------

olahmoon | 2024-04-02 15:12:41 UTC | #98

that's super interesting, we want to implement on a projects we started ckde.fi for automation

-------------------------

patnorris | 2024-04-03 10:04:30 UTC | #99

Hi @Samer and @olahmoon , please join us for one of our meetings (the next one will be tomorrow) if you're available. Would be great to get to know you and your projects and hear about your interests in DeAI :) this is our channel in the IC dev's Discord: https://discord.gg/9UD5rVKc Maybe talk to you soon

-------------------------

patnorris | 2024-04-04 19:20:39 UTC | #100

Hi everyone, thank you for today's call (2024.04.04). This is the generated summary (short version, please find the long version here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.04.04):
The DeAI Working Group call for the Internet Computer was a comprehensive discussion that began with an introduction by @SamDrissi , who shared his project's aim to develop AI applications for intellectual property education and processes on the Internet Computer platform (https://forum.dfinity.org/t/crafting-tomorrows-ip-standards-today-with-bipquantum-on-icp/29312/3). The group explored the role of AI in providing consultations and establishing proof of ownership for intellectual properties, alongside the challenges of data accessibility and the evolving legal landscape for AI-generated content. A significant focus was placed on the potential for improving AI performance through model quantization and the utilization of SIMD instructions, discussing the current capabilities and future possibilities of Wasm and the Internet Computer's hardware architecture. The conversation also covered the trend towards integrated CPU and GPU architectures for more efficient AI model execution and speculated on future hardware developments that could further enhance the platform's AI capabilities. The call concluded with an open invitation for future topic suggestions and collaborative contributions, emphasizing the group's interest in benchmarking and sharing data to better understand and communicate the Internet Computer's potential for AI applications.

shared during the call:
https://forum.dfinity.org/t/crafting-tomorrows-ip-standards-today-with-bipquantum-on-icp/29312/3
https://au.mathworks.com/company/technical-articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html
"Support for CPU acceleration. It will be possible for smart contract AIs to export data for processing directly on the hardware of node machines. This involves optimizations that allow the models to run deterministically, using newly developed fundamental AI science, and security checks that ensure smart contracts can’t cause non-determinism, since this would cause subnets to stop running. (We already have these techniques working)."
SIMD - Single Instruction Multiple Data (https://github.com/WebAssembly/simd/blob/main/proposals/simd/ImplementationStatus.md)
Advanced Vector Extensions
https://github.com/WebAssembly/wasi-nn

-------------------------

ulan | 2024-04-18 07:37:21 UTC | #101

@jeshli, @mnl: Raheel and Dominic pointed me to this thread and the discussion about ONNX runtimes: Tract and Burn. I thought it might be useful for the community to share my findings:

- I managed to get the upstream Tract working on IC without any changes (i.e. without forking).
- The approach relies on the [wasi2ic](https://github.com/wasm-forge/wasi2ic) community project.
- I first compile Tract to `wasm32-wasi` and then use `wasi2ic` to convert the binary to `wasm32-unknown-unknown` that is compatible with the IC.
- Example usage: [code](https://github.com/dfinity/examples/blob/master/rust/image-classification/src/backend/Cargo.toml#L19)
- Build steps of the example: [code](https://github.com/dfinity/examples/blob/6c38a5d4def5da7308ae1c7b61de62045876868a/rust/image-classification/dfx.json#L9)

Did anyone manage to compile Burn to IC? I would be curious to compare its performance with Tract.

-------------------------

