icarus | 2024-04-18 09:01:03 UTC | #102

That is very cool and fully in the wheel-house of the DeAI working group... I know several people who will be excited to try this and see it working. @patnorris 
For context to all following this WG thread, here is a link to the docs in the tract github repo
  [Intro to Tract](https://github.com/sonos/tract/blob/main/doc/intro.md)
and the TLDR on it is 

> tract is a neural network inference library. It takes trained networks from higher-level frameworks (Tensorflow, PyTorch, etc.), converts them to an intermediate representation and runs them on the end-user data. It is designed to be very portable and embedding friendly. We believe in running Neural Network Inference on the Edge, on a browser or a small embeddable CPU.

> * tract-onnx is a Rust library that can load and run an ONNX network. About 85% of ONNX operators are supported.
> * tract-tensorflow is a Rust library that can load and run a TensorFlow 1 network. Because of the huge size of TensorFlow, a smaller portion of the operator set is supported.
> * tract-nnef is a Rust lbrary that can load and run NNEF networks. Most of NNEF is supported (missing deconv, ROI operations and quantization).
> * tract is the main command line interface (can be installed with "cargo install"). It can load network in any of the previously listed formats, dump them in a user friendly form, bench and profile a network. Additionaly, the tract command line can be used to convert a network to NNEF (with some extensions). tract-nnef is significanly smaller and lighter to start than tract-onnx or tract-tensorflow, so this conversion is useful for embedded situations.

-------------------------

patnorris | 2024-04-18 10:13:10 UTC | #103

Thank you, this work looks exciting!

And thanks for the docs @icarus As you said, I agree that the group would surely be interested in seeing this in action and learning more about it. 
If you're interested and available @ulan , it'd be great to have you on one of our group calls and show your work :+1:

-------------------------

ulan | 2024-04-18 10:20:16 UTC | #104

Thanks for the invite @patnorris! I will join the session that's scheduled today.

-------------------------

patnorris | 2024-04-18 10:56:18 UTC | #105

Awesome, looking forward to it :muscle:

-------------------------

jeshli | 2024-04-18 13:39:14 UTC | #106

I'm fond of your approach. I am happy to have it as a resource.

-------------------------

q2333gh | 2024-04-18 15:28:21 UTC | #107

Im using gpt for 1 hour add up . every day.

After i learn the course : [Generative AI for Everyone - DeepLearning.AI](https://www.deeplearning.ai/courses/generative-ai-for-everyone/)

I find that  AI is so useful in many aspect of life.

And i also like blockchain with AI. that will be even cooler !

-------------------------

patnorris | 2024-04-18 15:57:02 UTC | #108

agreed, it'll be amazing to see all the ways that AI can be useful and important to us!

If you like to join our conversations, please join us in our IC Discord channel: https://discord.gg/zVdHC3S5

And on Thursdays, we have calls in the IC Discord voice channel if you like to join there too.

-------------------------

patnorris | 2024-04-18 19:41:54 UTC | #109

Hi everyone, thank you for today's call (2024.04.18). This is the generated summary (short version, [please find the long version here]( https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.04.18)): 
In today's DeAI Working Group meeting for the Internet Computer, the discussions primarily revolved around optimizing WebAssembly and AI deployment on the platform. Ulan shared insights on using the [Tract framework](https://forum.dfinity.org/t/technical-working-group-deai/24621/101) and the community project [wasi-to-ic](https://forum.dfinity.org/t/introducing-wasi-for-ic/18583?page=3), as detailed on the forum, to adapt code for Internet Computer's environment. The group delved into technical strategies for overcoming challenges with non-deterministic behaviors in floating-point operations and explored enhancements through SIMD instructions for better performance, potentially achieving up to a tenfold improvement.

A significant part of the conversation addressed the need for higher instruction limits in query operations to efficiently run large language models (LLMs) and discussed architectural approaches for scaling AI deployments via [multiple canisters managed by a control canister](https://forum.dfinity.org/t/technical-working-group-scalability-performance/14265/90?u=icpp). This architecture was part of a [hackathon project](https://github.com/onicai/onicaiGoes2024OxfordBH).

The session highlighted ongoing efforts to refine benchmarks and encouraged further discussion on leveraging new optimizations to boost the performance and scalability of AI models on the platform. The participants expressed interest in continuing to explore these areas, particularly in how they could enhance the deployment and usability of LLMs in different scenarios, including on devices through browsers, which presents unique challenges such as initial download times and computational limits based on device capabilities.

-------------------------

patnorris | 2024-04-18 19:58:48 UTC | #110

Hi @ulan , thank you for joining today's call, it was great hearing about your work.

As discussed, [this is the call summary from two weeks ago](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.04.04) where @icarus shared a lot of insights from his research on WebAssembly SIMD.

[This is the link](https://github.com/icppWorld/icpp_llm/tree/main/llama2_c) to @icpp 's repo of the on-chain LLM that we also use as part of the hackathon project I demoed. If you have any questions around this @icpp and I are happy to help.

And for the mentioned benchmarking standard, this is a [first simple draft for the template](https://docs.google.com/document/d/1ppLYR_soHmQYd48AfR0CGFpAWMA9WKI2s-lr7HdGc1M/edit). Happy to hear everyone's feedback and ideas :+1:

-------------------------

q2333gh | 2024-04-19 01:30:20 UTC | #111

llama3-8B is released. got performance equally to llama2-70B.What a great things happening!

But i guess its still not quite good to run it on ic canister.

-------------------------

icpp | 2024-04-19 02:07:09 UTC | #112

I think running an 8B model is not too far away in the future ü§ûü§û

-------------------------

ulan | 2024-04-19 06:45:30 UTC | #113

Thanks, @patnorris!

> And for the mentioned benchmarking standard, this is a [first simple draft for the template ](https://docs.google.com/document/d/1ppLYR_soHmQYd48AfR0CGFpAWMA9WKI2s-lr7HdGc1M/edit). Happy to hear everyone‚Äôs feedback and ideas

I left a comment about using the same execution/testing environment (e.g. PocketIC) and providing a script to run the experiment.

For the Wasm benchmarking suite I had the following in mind:

- it should be language agnostic: Rust, Motoko, Azle, Kybra, C++
- it should have a script to build from the source code and produce a single canister.
- the canister should have a single public endpoint `run()` that executes the actual experiment.
- the canister can use `canister_init()` to initialize its data structures for the experiment.

-------------------------

ulan | 2024-04-19 09:35:07 UTC | #114

@jeshli: in case you are planning to use wasi2ic in production, please take a look at this change in the example code: https://github.com/dfinity/examples/pull/850

By default the WASI polyfill library is going to use the canister's stable memory to store the file system. If the canister already uses its stable memory for other purposes this may lead to data corruption. The change I linked passes a virtual stable memory to WASI.

@sgaflv: wdyt about applying the same change to `demo1` example of `wasi2ic` such that people use the library safely?

-------------------------

sgaflv | 2024-04-19 20:58:16 UTC | #115

[quote="ulan, post:114, topic:24621"]
wdyt about applying the same change to `demo1` example of `wasi2ic` such that people use the library safely?
[/quote]
I think the demo1 example, which is the "Hello World" example, needs to be as simple as possible. While the custom memory example should be shown in a separate non-trivial demonstration.

-------------------------

ulan | 2024-04-22 08:49:44 UTC | #116

> I think the demo1 example, which is the ‚ÄúHello World‚Äù example, needs to be as simple as possible.

Generally, I agree that examples should be simple, but demo1 right now has a dangerous code pattern.

People usually copy/paste code snippets from examples (like I did for the image classification demo). If they put the code from demo1 into their existing canister, then they are going to lose all their data in stable memory. So demo1 at least needs a comment to explain this risk or should use a safe pattern.

-------------------------

icme | 2024-04-29 04:12:45 UTC | #117

Saw this paper that recently came out on using Six-bit quantization (FP6) to reduce the size of large language models and increase inference throughput on A100 GPUs.

Some of it is a bit over my head, but figured it could be worth looking into.

TLDR Tweet
https://x.com/rohanpaul_ai/status/1784599257384727044


Research Paper
https://arxiv.org/abs/2401.14112

-------------------------

icarus | 2024-04-30 10:14:47 UTC | #118

Hi @ulan , during the DeAI Working Group meeting last week I mentioned during our discussion a WASM proposal for extending SIMD support to include vectors longer than 128-bits. It was called "flexible-vectors" and takes the approach of adding instruction bytecodes that don't specify a vector instruction bit-width; instead the instructions are specific to the SIMD lane width and count leaving the vector width as a runtime (startup) configuration to be read from the CPU architecture (which  would be 512 bits wide for AVX-512 SIMD extensions on the AMD EPYC CPU).

I searched for but couldn't find any evidence of an implementation in any WASM runtimes. The proposal is fully specified originally by an Intel employee, so possibly there is an unpublicised test implementation somewhere.

Linked at the end of our DeAI WG Discord channel meeting discussion thread here (@patnorris had a question for you there too) : https://discord.com/channels/748416164832608337/1184160869614108873/1233526865038282806

https://github.com/WebAssembly/flexible-vectors

-------------------------

ulan | 2024-04-30 10:28:45 UTC | #119

Thanks @icarus. There is more info in the issue tracker of that repository: https://github.com/WebAssembly/flexible-vectors/issues/60

Looks like it was presented last year, but the poll to move to phase 2 "was inconclusive". I'll take a look.

BTW, I will miss the upcoming session this Thursday because I will be on holidays.

-------------------------

sgaflv | 2024-05-06 14:02:30 UTC | #120

[quote="ulan, post:114, topic:24621"]
@sgaflv: wdyt about applying the same change to `demo1` example of `wasi2ic` such that people use the library safely?
[/quote]
Alright, I've opened the [issue](https://github.com/wasm-forge/demo1/issues/1) in demo1 repo

-------------------------

Samer | 2024-05-21 16:34:00 UTC | #121

https://arxiv.org/abs/2404.19756
https://www.youtube.com/watch?v=AUDHb-tnlB0

Just want to throw this in here. Not an expert, but from what I gather KANs may perform better than Multi-Layer Perceptrons.

It goes without saying that this working group should anticipate the Neural Networks of tomorrow and invest resources in anticipation of latest trends.

-------------------------

Samer | 2024-05-21 21:42:32 UTC | #122

Someone linked this critique of KANs on the MLST discord
https://vikasdhiman.info/reviews/KAN_a_review.pdf

-------------------------

zensh | 2024-05-24 14:28:11 UTC | #123

Great topic! I took some time to read through it and learned a lot about DeAI.

We developed a DeAI canister based on Huggingface's Candle framework and tried running a 0.5B parameter, 1.2G data LLM.

It can run in a local cluster with adjusted instruction limits, and here are the results:
```sh
dfx canister call bwwuq-byaaa-aaaan-qmk4q-cai update_chat '(record {prompt="Nice to chat with you, Please introduce yourself."})'
(
  variant {
    Ok = record {
      instructions = 1_128_679_236_950 : nat64;
      tokens = 43 : nat32;
      message = "\nHello! I am Panda Oracle, a giant panda with human intelligence. I am here to help you with any questions or concerns you may have. How can I assist you today?\n";
    }
  },
)
```

Source code: https://github.com/ldclabs/ic-panda/tree/main/src/ic_panda_ai

https://x.com/ICPandaDAO/status/1794012049380679894

-------------------------

jeshli | 2024-05-24 15:07:45 UTC | #124

Thank you for sharing. I have also been experimenting with candle and believe it to be an excellent crate for streamlining ML on the IC.

-------------------------

icpp | 2024-05-25 22:17:10 UTC | #125

[quote="zensh, post:123, topic:24621"]
ca
[/quote]

Awesome accomplishment!!

Can you explain more whay you mean with 'adjusted instructions limit' ?

Also, are you doing query or update calls ?

-------------------------

zensh | 2024-05-26 01:57:58 UTC | #126

[quote="icpp, post:125, topic:24621"]
Can you explain more whay you mean with ‚Äòadjusted instructions limit‚Äô ?
[/quote]

https://github.com/ldclabs/ic/commit/27760a5887c513c503bf670e10d79a6eefc18ac3

Tool to build local IC replica and dfx:
https://github.com/ninegua/ic-nix

I've tested it, and the query call works:
https://github.com/ldclabs/ic-panda/blob/main/src/ic_panda_ai/src/api_query.rs#L40

However, changing the instruction limit for query calls is a bit tricky. You need to modify the `max_query_call_graph_instructions` parameter in the ic.json5 file generated during runtime.

-------------------------

icpp | 2024-05-26 22:17:10 UTC | #127

Thank you for those details!

-------------------------

TusharGuptaMm | 2024-05-27 07:13:08 UTC | #128

Our team @ RuBaRu is taking baby steps towards developing OnChain AI services. These innovations aim to enhance user experience and bring greater transparency to content distribution and discovery. Additionally, our AI will analyse and tag rich media, such as images and videos, with content types. I believe we have a long road ahead, but it's a solid start towards keeping the ecosystem fully OnChain. We welcome your ideas and feedback as we advance in the SocialFi space.

Read more: https://forum.dfinity.org/t/rubaru-a-fully-on-chain-regenerative-creators-consumers-economy/24803/7?u=tusharguptamm

-------------------------

berestovskyy | 2024-05-29 16:27:05 UTC | #129

Hey folks,
The WebAssembly SIMD is available on the mainnet and the latest `dfx`:

1. To install the latest `dfx` with SIMD support: `dfxvm install 0.20.2-beta.0`

2. The Rust SIMD example, which compares naive, optimized, auto-vectorized, and SIMD intrinsic matrix multiplication: https://github.com/dfinity/examples/tree/master/rust/simd

3. A bit of SIMD documentation on the IC: https://internetcomputer.org/docs/current/developer-docs/smart-contracts/advanced-features/simd/

Let me know if you have any questions,
and enjoy!

-------------------------

TusharGuptaMm | 2024-05-29 16:30:59 UTC | #130

This is amazing! Will give it a try!

-------------------------

icarus | 2024-05-29 18:20:19 UTC | #131

That is welcome and much anticipated great news!
Thanks for sharing it here, I will copy it to the DeAI WG Discord channel and start spreading the word!

Many thanks to you and other Dfinity team members who have worked on the SIMD support and pushed it along @berestovskyy

-------------------------

cryptoschindler | 2024-05-30 13:10:08 UTC | #132

Dear WG members,

please add any missing resources to this page :) 

https://internetcomputer.org/docs/current/developer-docs/ai/overview

-------------------------

zensh | 2024-05-30 13:59:31 UTC | #133

A pull request to add ICPanda AI canister: https://github.com/dfinity/portal/pull/3004

-------------------------

patnorris | 2024-06-05 08:21:40 UTC | #134

Hi everyone, this Thursday we'll have our joined call with the DFINITY growth team üôÇ We'll discuss how to best collaborate, so please bring all your ideas :rocket: As usual,  it's on Thursday at 6pm CET and in the ICP Developer Discord. This is a link to the event: https://discord.com/events/748416164832608337/1245651401988902964
Looking forward to the call!

-------------------------

patnorris | 2024-06-06 19:22:14 UTC | #135

Hi everyone, thank you for today's call and the great discussion! Special thanks to @jennifertran for putting it together üôÇ This is the summary of the call, looking forward to collaborating on the next steps and ideas we had üí™
Long summary here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.06.06

Short summary: 
The DeAI Working Group call for the Internet Computer focused on improving AI project marketing and documentation and was a joined session with DFINITY team members across growth, marketing and adoption. Participants discussed the need for better marketing strategies, comprehensive AI documentation, and practical use cases to demonstrate ICP's benefits. Collaborative efforts were emphasized, including creating a decentralized AI manifesto, collecting success stories, and exploring monthly public updates. The call concluded with actionable steps to support ecosystem teams, enhance marketing resources, and ensure the successful promotion of decentralized AI on the Internet Computer.

-------------------------

catpirate3 | 2024-06-13 17:30:22 UTC | #136

Are the meetings being recorded? or just the summary?

-------------------------

patnorris | 2024-06-13 18:31:25 UTC | #137

Hi there, the meetings are not recorded, but we have all summaries so far in here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings

-------------------------

patnorris | 2024-06-13 19:44:17 UTC | #138

Hi everyone, thank you for today's call! Special thanks to @gip for sharing insights on his exciting work üôÇ This is the short summary (long summary here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.06.13):
In today's call of the DeAI Working Group for the Internet Computer, several key initiatives were discussed. The team plans to develop a survey to gather information from ecosystem teams building AI projects, aiming to create a tailored marketing strategy. The importance of tracking usage statistics for decentralized AI applications was emphasized to demonstrate activity and traction to potential partners. Updates were provided on the scheduling of DeAI live streams and organizing a DeAI panel for the Chain Fusion Day event at EthCC. A proposal to draft a decentralized AI manifesto was made to outline principles and goals. The group considered creating a chart to rate projects on their actual use of AI technology, enhancing transparency. Jessie presented a draft outline for new documentation pages to improve decentralized AI documentation. Metrics for tracking developer engagement were discussed.
Giles shared his challenges in fine-tuning a large language model (LLM) on the Internet Computer, highlighting the approach of dividing the model into smaller segments. Technical issues like efficient data loading across canisters and handling large data files were discussed, with suggestions to use separate endpoints to load weights. The discussion emphasized the importance of data privacy in AI applications and the potential of the Internet Computer in collaborative AI development. It was noted that specialized tasks might not require extremely large models, with more compact models being sufficient for many use cases. The group touched on current capabilities of the IC in running LLMs and the need for efficiency improvements. Discussions included optimizing Rust code using SIMD, creating a reusable library in Rust, and managing large models by splitting them into canisters. The challenges and potential of utilizing GPUs for AI on the Internet Computer were briefly discussed. The call concluded with a reminder of the upcoming scalability and performance working group meeting, encouraging participants to share topics and initiatives for future sessions.

-------------------------

catpirate3 | 2024-06-13 18:42:58 UTC | #139

Please do record the future discussions @jennifertran, thanks.

-------------------------

icpp | 2024-06-16 16:13:52 UTC | #141

@patnorris ,
I like to bring awareness to the project of @ktimam , who is porting Flashlight to the IC. This is a C++ machine learning library, and it includes a port of openblas, fftw & arrayfire. A truly amazing effort.

We adapted iccp-pro to support static libraries and C++17, and he is now able to build the wasm that should be able to run on the IC, but he is running again into the dreaded maximum on allowed globals.

He already entered a request to get it increased, [here](https://forum.dfinity.org/t/wasm-module-defined-globals-which-exceeds-the-maximum-number-allowed-200/10834/28?u=icpp), but I like to also give this issue visibility through communication from our DeAI working group with DFINITY scaling team.

-------------------------

patnorris | 2024-06-17 17:08:10 UTC | #142

Thank you @icpp and sounds exciting @ktimam!

Cool, happy to talk more about this in the group and see how we can support communication and progress on this :rocket:

And @ktimam if you're interested in discussing your work with or maybe giving a demo to the DeAI group, please let me know and we'll arrange :+1: happy to learn more

-------------------------

ktimam | 2024-06-17 22:36:57 UTC | #143

Thanks @icpp for your support and for all the work on icpp-pro.

Thank you @patnorris, it would be my pleasure to share my progress with you.

-------------------------

patnorris | 2024-06-19 21:18:21 UTC | #144

awesome, looking forward to it! We have a call every Thursday at 6pm CET in the IC Dev Discord. This is our DeAI WG channel there: https://discord.gg/4UZ6XEpYuS
Would you be interested to share your work during one of our upcoming calls, e.g. next week Thursday or the Thursday after?

-------------------------

ktimam | 2024-06-19 21:25:22 UTC | #145

Next week would be great. Sent you on discord.

-------------------------

patnorris | 2024-06-20 18:09:12 UTC | #146

Hi everyone, thank you for today's discussion! This is the summary of the call (long summary here: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.06.20):
In today's call of the Decentralized AI Working Group for the Internet Computer (IC), discussions focused on enhancing developer documentation and planning future topics. The group emphasized creating structured AI documentation with sections on overview, workflows, tooling resources, and examples to meet developers‚Äô specific goals. They discussed content suggestions, encouraging developer contributions, and aligning documentation with industry standards and ICP capabilities. Technical considerations included merging machine learning and inference workflows and detailing current and future capabilities to set expectations. Innovative AI concepts and iterative improvements based on feedback were highlighted. Future topics will cover current and future AI capabilities, speculative AI model re-architecting, regulatory impacts, and comparative analysis with similar projects. Action items include revising and posting the documentation outline for feedback, and planning discussions on GPU determinism, LLM sizes on ICP, training data pathways, and global AI governance regulations.

-------------------------

patnorris | 2024-06-20 18:10:53 UTC | #147

And what else do you all think should be included in the new DeAI Dev Docs?
If you get the chance, please provide your feedback on the outline that @Jessie created: https://docs.google.com/document/d/1tf6kbzE2Vqxb5I6JUWZzJJpQ0qmGXkOfNqcU4yBWOSQ/edit would be great to have everyone's ideas includedüí™

-------------------------

icpp | 2024-06-21 00:25:40 UTC | #148

@Jessie ,
The outline of the new DeAI Dev Docs looks great!

Excited to contribute.

-------------------------

icpp | 2024-06-23 12:06:31 UTC | #149

FYI, regarding the issues that @ktimam ran into, he was able to resolve them after Ulan explained that the wasm contained many exported globals that were unused. @ktimam was then able to correct the wasm, by hacking into it manually, and get it running on mainnet. :tada:

To fix the issue of those unused globals systematically, two things are planned:
- I plan to update icpp-pro to run a post build wasm optimization tool. Ulan made a recommendation ([here](https://forum.dfinity.org/t/wasm-module-defined-globals-which-exceeds-the-maximum-number-allowed-200/10834/33?u=icpp)) what to run and I am investigating right now how to integrate it.
- DFINITY team is planning to update the ic-wasm tool to always strip out unused globals ([here](https://forum.dfinity.org/t/wasm-module-defined-globals-which-exceeds-the-maximum-number-allowed-200/10834/34?u=icpp))

So, many thanks to @ktimam for pushing the boundaries, and helping improve the IC and tooling.

-------------------------

pu0238 | 2024-06-24 01:25:52 UTC | #150

Sure, here is the corrected version:

"Hi, this SIMD support doesn't really work for me, or there are some unsupported features that Candle uses. :("

![image|317x499](upload://9jU53rrnvgYO9zMwtEuHredJF9W.png)

-------------------------

berestovskyy | 2024-06-24 06:58:39 UTC | #151

Hey pu0238,
I'm not sure where the screenshot comes from, but please make sure the `dfx` version is [at least 0.20.2-beta.0](https://forum.dfinity.org/t/technical-working-group-deai/24621/129) and the [SIMD instructions are enabled](https://internetcomputer.org/docs/current/developer-docs/smart-contracts/advanced-features/simd/#example) on the project/function level.

-------------------------

therealbryanho | 2024-06-25 15:09:40 UTC | #152

Document outline looks promising!

-------------------------

pu0238 | 2024-06-26 18:56:38 UTC | #153

I am using 0.20.2-beta.0, and I added SIMD support to `rustflags` in Cargo as shown in the example below. However, libraries from CanDE still do not compile unfortunately. 

https://internetcomputer.org/docs/current/developer-docs/smart-contracts/advanced-features/simd/#example

-------------------------

berestovskyy | 2024-06-26 19:00:42 UTC | #154

Push your changes somewhere, and I'll have a look.

-------------------------

patnorris | 2024-06-27 14:46:51 UTC | #155

:muscle: we'll get a demo by @ktimam today during our WG call. And @gip will share his progress on modular transformer models on the IC too :) See you all then

-------------------------

patnorris | 2024-06-28 04:06:19 UTC | #156

Hi everyone, thank you for today's call (2024.06.27). This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.06.27):
In the DeAI Working Group's call for the Internet Computer, key points included highlighting AI product success stories via a shared survey to strategize marketing and measure metrics for partnerships, aiming to position ICP prominently in the AI space with data partners like CoinMarketCap. Kiko presented his integration of the Flashlight library into ICP, sharing his experiences and challenges in bringing C++ projects, such as a soccer simulator and a physics engine, to the platform, and demonstrated training a neural network on-chain. The discussion covered the use of SIMD instructions to enhance GPT-2 transformer performance, resulting in a 1,230% efficiency increase, and Jeshli‚Äôs plan to publish related code and tokenizers. Efforts to bring the LLaMA model to ICP were detailed by Giles, showcasing modular implementation in Rust and the feasibility of running computations on-chain despite latency issues. Community support, funding for computational tasks, and future directions focused on code optimization and deploying the first open-source LLaMA model on-chain were also discussed.

Special thanks to @ktimam and @gip for sharing their exciting work!

-------------------------

patnorris | 2024-07-01 15:04:19 UTC | #157

As mentioned in the last summary, please participate in the [survey to highlight DeAI success stories here](https://docs.google.com/forms/d/e/1FAIpQLSeZU-6BkFXoNPB9i1P3g8Wl6hjIA_wrkFOaBpu4yzKgRjMumA/viewform) 
and please see its forum thread:
https://forum.dfinity.org/t/featuring-success-stories-of-deai-products-on-icp/32523

Looking forward to seeing all our community success stories being covered :) 

Thank you @jennifertran for this opportunity!

-------------------------

patnorris | 2024-07-04 19:33:09 UTC | #158

Hi everyone, thank you for today's call (2024.07.04). This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.07.04):
In discussing the implementation of larger language models on the Internet Computer (IC), progress was highlighted in creating canisters for tensor computations despite memory constraints. Key discussions included the slow manual reimplementation of models for IC and the suggestion to use tools like PyTorch and TinyGrad for efficiency. Challenges with stable memory due to bugs and inefficiency were addressed, with proposed direct memory management improvements. The benefits of using frameworks like PyTorch and ONNX were emphasized, along with the potential of the Rust library Candle. Horizontal scaling, federated learning, and the IC's decentralized data ownership were explored as future directions. Action items include experimenting with tools like TinyGrad and federated learning, improving stable memory handling, and fostering developer collaboration.

links shared during the call:
* https://docs.google.com/forms/d/e/1FAIpQLSeZU-6BkFXoNPB9i1P3g8Wl6hjIA_wrkFOaBpu4yzKgRjMumA/viewform
* https://github.com/gip/yllama.rs
* https://github.com/gip/yllama.oc
* https://www.youtube.com/watch?v=13pnH_8cBUM&list=WL&index=102
* https://woglz-oqaaa-aaaal-act2q-cai.icp0.io/
* https://github.com/jeshli/rust-connect-py-ai-to-ic
* https://github.com/jeshli/tract-ic-ai
* https://github.com/tinygrad/tinygrad
* https://github.com/huggingface/candle
* https://0g.ai/

-------------------------

SolarShogun | 2024-07-04 23:15:31 UTC | #159

Would love to learn more about how ICP can support federated AI and crowdsource fine tuning models! Also how to set up ICP-network-owned parallel processing / supercomputing

-------------------------

patnorris | 2024-07-05 13:43:14 UTC | #160

Hi there, yes, super fascinating topic. There is this thread as a starter: https://discord.com/channels/748416164832608337/1184160869614108873/1249136286279794688
happy to also discuss more on this with you :+1:

-------------------------

ulan | 2024-07-10 15:21:38 UTC | #161

Hi folks!

The recent `dfx 0.22.0-beta.0` comes with the floating point optimization and it also has Wasm SIMD (corresponds to the replica version https://dashboard.internetcomputer.org/proposal/130984)

If you see any performance improvement going from `dfx 0.20.1` to `dfx 0.22.0-beta.0` with Wasm SIMD and you project is open source, I'd be happy to mention your project and improvements in the upcoming blog post about these optimizations.

-------------------------

icpp | 2024-07-10 18:03:14 UTC | #162

Hi @ulan,

I plan to run performance tests over the weekend with my LLMs. Is that still on time for your blog post, or do you need it sooner?

-------------------------

ulan | 2024-07-10 18:22:27 UTC | #163

Hi @icpp,

We wanted to time the publication with Dom recording a new demo. Tentatively that is planned on Friday, but I don't know for sure.

In any case, I can edit the blog post and add your number after publication.

-------------------------

gip | 2024-07-10 19:53:20 UTC | #164

[quote="ulan, post:161, topic:24621"]
The recent `dfx 0.22.0-beta.0` comes with the floating point optimization and it also has Wasm SIMD (corresponds to the replica version
[/quote]

I'd like to try and enable SIMD on [yllama.oc](https://github.com/gip/yllama.oc). All I need to do is to move start using `dfx 0.22.0-beta.0` and the rest will be taken care of? If not is there some docs on how to enable SIMD?

-------------------------

ulan | 2024-07-11 10:06:07 UTC | #165

@gip: you need also enable it on the compiler side using compiler-specific flags. For example here is how it is done in rust: https://github.com/dfinity/examples/blob/master/rust/simd/.cargo/config.toml
Put
```
[target.wasm32-wasi]
rustflags = ["-Ctarget-feature=+simd128"]
```
in .cargo/config.toml 

or 
```
[target.wasm32-unknown-unknown]
rustflags = ["-Ctarget-feature=+simd128"]
```
depending on your Wasm target.

You can also set the environment variable like this before bulding:
```
export RUSTFLAGS=$RUSTFLAGS' -C target-feature=+simd128'
```

If you're lucky, then the compiler will do the magic and autovectorize the hot path. Otherwise, you might need to use the Wasm SIMD instructions manually. For example, here is what I did for Sonos Tract: https://github.com/sonos/tract/pull/1420

-------------------------

patnorris | 2024-07-11 18:47:53 UTC | #166

Hi everyone, thank you for today's call (2024.07.11). Special thanks to Vincent and @icarus for sharing their work and insights! This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.07.11):

In today's DeAI Working Group call, Vincent presented his master thesis work, focusing on encryption and machine learning models deployed on the Internet Computer, sharing a prototype link (https://zvrui-xqaaa-aaaao-a3pbq-cai.icp0.io/) and the code repository (https://github.com/vince2git/ic-secure-ml). Key discussions covered encryption key exchanges, secure data handling, and using the linfa rust library for benchmarking machine learning models. Vincent detailed the architecture involving t-SNE for clustering and neural network training on the MNIST dataset. @icarus shared insights from the Node provider ICP lab event in Zurich, discussing hardware options, including AMD CPUs with SIMD extensions and integrated CPU-GPU architectures for AI on the IC blockchain (https://forum.dfinity.org/t/technical-working-group-node-providers/30255/7?u=icarus). Challenges of accessing GPU functionality from the WASM runtime and potential solutions through host functions were explored. Future directions emphasized collaboration between node providers and Dfinity engineers, forming a working group to focus on accelerated hardware integration. Open discussions addressed hardware setups, high-level host functions, ONNX for AI model deployment, and the importance of supporting frameworks like PyTorch. Community involvement was encouraged to share benchmarks, explore parallelism, and contribute to ongoing discussions for collective problem-solving.

### Links shared during the call:

* https://zvrui-xqaaa-aaaao-a3pbq-cai.icp0.io/
* https://github.com/vince2git/ic-secure-ml
* https://forum.dfinity.org/t/technical-working-group-node-providers/30255/7?u=icarus
* [https://wasmedge.org/book/en/sdk/rust/table_and_funcref.html?highlight="host%20function"#define-host-function](https://wasmedge.org/book/en/sdk/rust/table_and_funcref.html?highlight=%22host%20function%22#define-host-function)
* https://github.com/modclub-app/rust-connect-py-ai-to-ic/tree/main/python
* Support for DeAI projects: https://docs.google.com/forms/d/e/1FAIpQLSeZU-6BkFXoNPB9i1P3g8Wl6hjIA_wrkFOaBpu4yzKgRjMumA/viewform
* DeAI manifesto: https://docs.google.com/document/d/1GM_QzDhU2DAzWm3C5RzU861qEje2lzy7Un_l_dYdhvc/edit#heading=h.8jg466jkf1fh
* DeAI docs: https://docs.google.com/document/d/1tf6kbzE2Vqxb5I6JUWZzJJpQ0qmGXkOfNqcU4yBWOSQ/edit#heading=h.at20hu62va02
* DeAI repo for PR (to add your project): https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/Projects

-------------------------

JaMarco | 2024-07-11 19:40:31 UTC | #167

would this be relevant to DeAI on IC? https://x.com/PrimeIntellect/status/1811444263999205504

-------------------------

gip | 2024-07-11 19:51:12 UTC | #168

@ulan Tried to use SIMD but I get the error below. Any idea?

```
Caused by: The replica returned a rejection error: reject code CanisterError, reject message Error from Canister bd3sg-teaaa-aaaaa-qaaba-cai: Canister's Wasm module is not valid: Wasmtime failed to validate wasm module wasmtime::Module::validate() failed with SIMD support is not enabled (at offset 0x1e819f).
This is likely an error with the compiler/CDK toolchain being used to build the canister. Please report the error to IC devs on the forum: https://forum.dfinity.org and include which language/CDK was used to create the canister., error code None
dfx --version
dfx 0.22.0-beta.0
```

-------------------------

gip | 2024-07-11 22:24:27 UTC | #169

Note: the error I reported above about SIMD not enabled is transient and appears / disappears depending on the build. I finally got a build that works without changing anything.

[quote="ulan, post:165, topic:24621"]
If you‚Äôre lucky, then the compiler will do the magic and autovectorize the hot path.
[/quote]

I wasn't lucky :) And I had to implement a specialized function to make it work: https://github.com/gip/yllama.rs/blob/main/ymath/src/lib.rs#L64-L96

That worked, I was able to see the SIMD instructions and computation were definitely faster. For posterity this is how the inner loop with SIMD looks now:
```
            v128.const i32x4 0x00000000 0x00000000 0x00000000 0x00000000
            local.set 8
            i32.const 0
            local.set 4
            loop  ;; label = @5
              local.get 8
              local.get 1
              v128.load align=4
              local.get 2
              v128.load align=4
              f32x4.mul
              f32x4.add
              local.get 1
              i32.const 16
              i32.add
              v128.load align=4
              local.get 2
              i32.const 16
              i32.add
              v128.load align=4
              f32x4.mul
              f32x4.add
              local.set 8
              local.get 1
              i32.const 32
              i32.add
              local.set 1
              local.get 2
              i32.const 32
              i32.add
              local.set 2
              local.get 4
              i32.const 8
              i32.add
              local.tee 4
              i32.const 4096
              i32.ne
              br_if 0 (;@5;)
            end
```

Speed improved dramatically thanks to SIMD and inference is now 22 s / token. A detailed look at the timing below shows that the overhead introduced by the protocol (consensus, inter-canister calls, wasm, ...) is now what's dominating.

![Screenshot 2024-07-11 at 2.11.10 PM|690x339, 50%](upload://43kSDIfzNAhlKqd1te4s7qdpGe5.jpeg)

It seems unlikely that ICP can become a high-performance target for AI workloads without architectural changes and focus on the tooling at this point. However, there will still be applications where consensus is necessary. Regardless, my investigation is now complete and has been interesting.

-------------------------

ulan | 2024-07-12 07:56:40 UTC | #170

[quote="gip, post:169, topic:24621"]
Note: the error I reported above about SIMD not enabled is transient and appears / disappears depending on the build. I finally got a build that works without changing anything.
[/quote]

My guess would be that you probably had an old `dfx` started and running in the background. When changing `dfx` versions, I always do 1) `dfx stop`  2) change the version 3) `dfx start`.

> Speed improved dramatically thanks to SIMD and inference is now 22 s / token.

Nice! What was the number before?  I see potential for more improvement in

```
f32x4(v0.get(j + 0), v0.get(j + 1), v0.get(j + 2), v0.get(j + 3))
```
If you can re-arrange the data layout to load all four values with a single instruction:
```
v128_load(v0 as *const v128);
```

Then it should become even faster. However, that's a non-trivial change.

-------------------------

ulan | 2024-07-12 07:59:04 UTC | #171

[quote="gip, post:169, topic:24621"]
It seems unlikely that ICP can become a high-performance target for AI workloads without architectural changes and focus on the tooling at this point.
[/quote]

+1, that's what DFINITY is planning to explore with the "Gyrotron" milestone on the roadmap: https://internetcomputer.org/roadmap#Decentralized%20AI-Gyrotron

-------------------------

ulan | 2024-07-12 08:20:40 UTC | #172

Would you like a reference to your project in the upcoming blogpost about Wasm performance improvements?

If so, then a linkable github document (either a separate file or a linkable section the main README) with the following info would help a lot:

* A brief description of your project and the benchmark you did.
* Measurements numbers with the old dfx and the new dfx. Ideally these measurements are only about Wasm execution (i.e. don't include consensus and inter-canister calls). The simplest way to do it is to call `ic_cdk::api::performance_counter(0)` to get the number of executed Wasm instructions at the end a single step (execution). Here is an example: https://github.com/dfinity/examples/blob/master/rust/image-classification/src/backend/src/lib.rs#L91

-------------------------

gip | 2024-07-12 15:35:07 UTC | #173

[quote="ulan, post:170, topic:24621"]
Nice! What was the number before? I see potential for more improvement in

```
f32x4(v0.get(j + 0), v0.get(j + 1), v0.get(j + 2), v0.get(j + 3))
```

If you can re-arrange the data layout to load all four values with a single instruction:

```
v128_load(v0 as *const v128);
```

Then it should become even faster. However, that‚Äôs a non-trivial change.
[/quote]

The generated code (shared above) is using `v128.load`.

[quote="ulan, post:170, topic:24621"]
+1, that‚Äôs what DFINITY is planning to explore with the ‚ÄúGyrotron‚Äù milestone on the roadmap: [Roadmap | Internet Computer ](https://internetcomputer.org/roadmap#Decentralized%20AI-Gyrotron)
[/quote]
I've looked at that roadmap but the compute is only part of it. I would hate to see the Dfinity team build a system that is only used for toy projects, so here are my thoughts and I would really encourage to think about **data** and not compute first. 
* How will people get the data to the system? You probably want a data lane / data nodes that also will make economic sense (I spent 50TC uploading data and it was so slow). Consensus gating data loading makes little sense, there are other solutions to ensure data integrity. Basically every single system online (including a simple web app) has highly optimized data layers and the ICP needs it too I believe.
* How will the system ensure a high data bandwidth. Performance these days are in hundreds / thousands of GB/s of data crunched. You probably want to target 10_000 GB/s (aggregated) to make sure you're building for the future.
* Tools & frontend
* Compute is probably the easy part even though GPU reliability issues will be an issue at scale.

I know it's a lot of investment but having a realistic view of what is needed to win is also important.

[quote="ulan, post:170, topic:24621"]
Would you like a reference to your project in the upcoming blogpost about Wasm performance improvements?
[/quote]
I'd like to, not sure when I'll have time to do these benchmarks though.

-------------------------

zensh | 2024-07-12 16:10:10 UTC | #174

[quote="gip, post:173, topic:24621"]
How will people get the data to the system? You probably want a data lane / data nodes that also will make economic sense (I spent 50TC uploading data and it was so slow). Consensus gating data loading makes little sense, there are other solutions to ensure data integrity. Basically every single system online (including a simple web app) has highly optimized data layers and the ICP needs it too I believe.
[/quote]

I developed a general file service called ic-oss for uploading large files. It uses concurrent uploading and takes about 20 minutes to upload 1GB of data.

https://github.com/ldclabs/ic-oss

The key code and the ic-oss-cli upload tool were used in the ic_panda_ai project.

https://github.com/ldclabs/ic-panda/blob/main/src/ic_panda_ai/README.md

I will soon extract the core large file handling logic from ic-oss into a standalone library for easy integration into other projects.

-------------------------

ulan | 2024-07-12 16:42:29 UTC | #175

Thank you. That's a great feedback! 

I agree that data is important. Improving data bandwidth is tracked in another milestone: https://internetcomputer.org/roadmap#Compute%20Platform-Stellarator

> Improved consensus throughput and latency
>
> Improved consensus throughput and latency by better, and less bursty, node bandwidth use. Achieved through not including full messages, but only their hashes and other metadata, in blocks.

The milestone is not AI-specific because it will help almost all applications.

> * How will the system ensure a high data bandwidth. Performance these days are in hundreds / thousands of GB/s of data crunched. You probably want to target 10_000 GB/s (aggregated) to make sure you‚Äôre building for the future.

Do you mean GPU-memory bandwidth or network bandwidth? What kind of applications and scale do you have in mind?

Initially, we would be quite happy if we could support inference/tuning of LLMs and then think about training of mid-size models. Training of large models is very far in the future.

What is also important is finding use cases that benefit from onchain AI. 

> * Tools & frontend

100% agree.

> I developed a general file service called ic-oss for uploading large files.

Nice!

-------------------------

jeshli | 2024-07-12 17:57:20 UTC | #176

[quote="zensh, post:174, topic:24621"]
I will soon extract the core large file handling logic from ic-oss into a standalone library for easy integration into other projects.
[/quote]

I published a crate similar to what you are describing, [ic-file-uploader](https://crates.io/crates/ic-file-uploader), so I wanted to share.

-------------------------

zensh | 2024-07-14 15:51:31 UTC | #177

[quote="zensh, post:174, topic:24621"]
I will soon extract the core large file handling logic from ic-oss into a standalone library for easy integration into other projects.
[/quote]

@gip 
In the `ic-oss` project, I implemented a library named `ic-oss-can`, which contains a Rust macro. It can quickly add the large file functionality to your canister, so that large files can be uploaded at high speed using `ic-oss-cli`. Here is a reference example for usage:

https://github.com/ldclabs/ic-oss/tree/main/examples/ai_canister

-------------------------

gip | 2024-07-15 16:26:47 UTC | #178

[quote="zensh, post:177, topic:24621"]
@gip
In the `ic-oss` project, I implemented a library named `ic-oss-can`, which contains a Rust macro. It can quickly add the large file functionality to your canister, so that large files can be uploaded at high speed using `ic-oss-cli`. Here is a reference example for usage:
[/quote]

[quote="jeshli, post:176, topic:24621"]
I published a crate similar to what you are describing, [ic-file-uploader ](https://crates.io/crates/ic-file-uploader), so I wanted to share.
[/quote]

Thanks - how more efficient and fast and/or less costly than the naive upload that I implemented in a few lines of code? https://github.com/gip/yllama.oc/blob/main/src/yblock/yblock.did#L21

The real question though, it is be able to make some data available to more than one canister fast. On Web2 (most of servers are stateless and data lives in data nodes and in caching nodes) this pattern is everywhere and the programming model of the ICP is clearly behind in that respect imo.

-------------------------

gip | 2024-07-15 16:34:39 UTC | #179

[quote="ulan, post:175, topic:24621"]
Do you mean GPU-memory bandwidth or network bandwidth? What kind of applications and scale do you have in mind?

Initially, we would be quite happy if we could support inference/tuning of LLMs and then think about training of mid-size models. Training of large models is very far in the future.

What is also important is finding use cases that benefit from onchain AI.
[/quote]

I'm talking about GPU bandwidth. Basically GPU on ICP will probably be ready in 1 to 2 years. The next generation of GPUs will land soon with a vastly improved bandwidth and I would want to build for the future. Of course on-chain access to data is a key point.

-------------------------

jeshli | 2024-07-15 19:07:52 UTC | #180

The `ic-file-uploader` crate does not increase upload bandwidth, but it simplifies the process of uploading large files into canisters. The crate breaks the file into 2MB chunks and uploads them sequentially.

Several teams have implemented similar code, so I decided to create a formal crate to streamline this functionality. I also plan to add automatic resume functionality for interrupted uploads in future updates.

I was sharing this with @zensh and for the benefit of future readers. If you have any suggestions, feel free to reach out.

-------------------------

KinicDevContributor | 2024-07-17 05:07:31 UTC | #181

I made a uploader to upload 70GiB to canister. To upload 70GiB it took for 12 hours and costed 200T cycles.

`https://github.com/ClankPan/ic-vectune/blob/67a3881bc317ca7ee5b36b053b72fc0200100927/bin/tool/src/main.rs#L92`

it also has the Interrupt feature.

-------------------------

gip | 2024-07-17 17:10:48 UTC | #182

[quote="KinicDevContributor, post:181, topic:24621"]
I made a uploader to upload 70GiB to canister. To upload 70GiB it took for 12 hours and costed 200T cycles.
[/quote]

I thought the max storage per canister was something around 4 GB. Were the 70GB you are talking about consumed and not stored? Curious what was the use case?

-------------------------

jeshli | 2024-07-17 23:56:42 UTC | #184

Stable memory has a 400GB capacity per canister

-------------------------

zensh | 2024-07-18 00:02:10 UTC | #185

Wasm heap memory, per canister: 4GiB
Wasm stable memory, per canister: 400GiB

It's possible to upload a single 400GB file using ic-oss, and it uses concurrent upload technology with ReadableStream. Maybe the upload speed will be faster. 

https://github.com/ldclabs/ic-oss/blob/main/src/ic_oss/src/bucket.rs#L373

-------------------------

gip | 2024-07-18 02:24:08 UTC | #186

[quote="zensh, post:185, topic:24621"]
Wasm heap memory, per canister: 4GiB
Wasm stable memory, per canister: 400GiB
[/quote]

I guess the question is how fast can the software get access to the memory. For the heap it's RAM so fast. Is stable loaded on demand from disk?

-------------------------

zensh | 2024-07-18 03:22:43 UTC | #187

Running an AI model still requires loading the model file from stable memory to heap memory, so it is still subject to the current 4GB limit of WASM32.

-------------------------

KinicDevContributor | 2024-07-18 06:00:56 UTC | #188

Our use case is deploying a vector database which index size is over 70GiB :+1:

-------------------------

KinicDevContributor | 2024-07-18 06:03:28 UTC | #189

[quote="zensh, post:185, topic:24621"]
It‚Äôs possible to upload a single 400GB file using ic-oss, and it uses concurrent upload technology with ReadableStream. Maybe the upload speed will be faster.

https://github.com/ldclabs/ic-oss/blob/main/src/ic_oss/src/bucket.rs#L373
[/quote]

I'm using tokio's stream_iter for parallelization, but it's possible that your implementation is faster.
If you have the opportunity to upload tens of GiB, let's compare!

-------------------------

patnorris | 2024-07-26 07:10:07 UTC | #190

Hi everyone, thank you for yesterday's call (2024.07.25). This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.07.25):
DFINITY is expanding its AI team by hiring an AI engineer specializing in GPU optimizations and deep learning libraries to work alongside the current team member, Islam, who has extensive experience in the Internet Computer (IC) and AI. The DeAI group discussed networking to find suitable candidates and expressed enthusiasm for the new role. The team is transitioning with the help of Ulan, planning milestones for GPU and CPU optimizations on IC, and working on various demos such as face recognition and GPT. Members shared interests in decentralized AI and gaming applications. Tim presented Hypercerts from Protocol Labs, which use semi-fungible certificates to incentivize and validate impact in projects like public goods and environmental remediation. The group discussed future collaborations, emphasizing decentralized AI development and community-driven efforts. Action items include sharing the job opening, continuing demo development, and exploring Hypercerts' applications in AI projects.

Special thanks to Tim for sharing Hypercerts with usüí™

Links shared during the call:

* DeAI manifesto: https://docs.google.com/document/d/1GM_QzDhU2DAzWm3C5RzU861qEje2lzy7Un_l_dYdhvc/edit#heading=h.8jg466jkf1fh
* DeAI docs: https://docs.google.com/document/d/1tf6kbzE2Vqxb5I6JUWZzJJpQ0qmGXkOfNqcU4yBWOSQ/edit#heading=h.at20hu62va02
* DeAI repo for PRs: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/Projects
* ICP game: https://woglz-oqaaa-aaaal-act2q-cai.icp0.io/
* See PDF with info on Hypercerts in this week's folder: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.07.25 and more links around what Tim shared below:
* https://capitalinstitute.org/wp-content/uploads/2015/04/2015-Regenerative-Capitalism-4-20-15-final.pdf
* https://en.wikipedia.org/wiki/Leucaena_leucocephala
* https://giveth.io/project/greenpill-brasil?tab=updates

-------------------------

patnorris | 2024-08-02 10:38:54 UTC | #191

Hi everyone, thank you for yesterday's call (2024.08.01) and special thanks to Krish for sharing his work. This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.08.01):

In the latest DeAI call, the success of the recent Web3 AI hackathon was highlighted, which saw high-quality submissions focused on AI development using web technologies. Winning projects, such as DeDa, a decentralized data marketplace, Clanopedia and ICMLPL, which integrates the Flashlight deep learning library, will continue as developer grantees. The community is encouraged to participate in weekly calls for knowledge exchange, utilize the decentralized AI channel on Discord, and explore support programs like the Olympus accelerator for further growth and investment opportunities.

### Links shared during the call:

* https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/Projects
* https://airtable.com/appJkokU1T66Nuovf/shr86e6R9zjAwWF01

-------------------------

patnorris | 2024-08-08 18:12:02 UTC | #192

Hi everyone, thank you for today's call (2024.08.08). Special thanks to @icarus for sharing his latest research on viable GPU hardware options for the IC! This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.08.08)): 

During today's DeAI Working Group call for the Internet Computer, the primary focus was on discussing hardware options for optimizing AI workloads. The group explored various GPUs and AI accelerator cards, considering factors such as cost, power efficiency, and suitability for both inference and training. The discussion highlighted the potential of Tens Torrent's wormhole cards, which offer a balance of performance and affordability, and emphasized the importance of an open-source software stack for flexibility and problem-solving. There was also mention of ongoing efforts to integrate GPU functionalities into the IC's host functions and the potential need for customized hardware solutions to support AI-driven tasks effectively.

Links shared during the call:
* https://tenstorrent.com/hardware/wormhole 
* https://www.esperanto.ai/ 
* DeAI manifesto to contribute to: https://docs.google.com/document/d/1GM_QzDhU2DAzWm3C5RzU861qEje2lzy7Un_l_dYdhvc/edit#heading=h.8jg466jkf1fh 
* DeVinci alpha release: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io/

-------------------------

laska189345938458347 | 2024-08-10 23:32:57 UTC | #193

Hello! How i can join to next call?

-------------------------

patnorris | 2024-08-12 10:21:28 UTC | #194

Hi there, we meet each Thursday in the ICP Developer Community Discord. We use the voice channel for the call: 
https://discord.gg/mgAEqcvM
https://discord.com/channels/748416164832608337/753880564947222568
Looking forward to having you :+1:

-------------------------

patnorris | 2024-08-15 20:50:08 UTC | #195

Hi everyone, thank you for today's call (2024.08.15). This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.08.15)):

During the DeAI Working Group call, a presentation was given on Clanopedia, a decentralized, verifiable data source built during the Web3 AI hackathon, focusing on creating a collective knowledge base using a vector database architecture stored on the Internet Computer Protocol. The discussion then shifted to documentation updates, with an extension of the deadline for contributions, and the team explored ongoing work on AI transformers for specific use cases, emphasizing the importance of community collaboration. The call also featured an in-depth discussion on AI regulation, particularly the European Union's AI Act, its implications for high-risk AI applications, and the evolving landscape of global AI governance. The session wrapped up with reflections on the need for future workshops, code walkthroughs, and regular updates on AI regulations, highlighting the importance of staying informed on legal developments as AI technology advances.

links shared during the call:
* DeAI docs to contribute to: https://docs.google.com/document/d/1tf6kbzE2Vqxb5I6JUWZzJJpQ0qmGXkOfNqcU4yBWOSQ/edit?usp=sharing 
* DeAI manifesto to contribute to: https://docs.google.com/document/d/1GM_QzDhU2DAzWm3C5RzU861qEje2lzy7Un_l_dYdhvc/edit#heading=h.8jg466jkf1fh 
* DeAI repo for project additions (as PR) and existing projects (incl. Vector dbs): https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/Projects
* Vector db by Kinic DAO: https://github.com/ClankPan/ic-vectune/tree/develop/kinic_db_instance

Please find Tim's slides on AI regulation in the repo's folder for today's meeting too: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/blob/main/WorkingGroupMeetings/2024.08.15/AI%20Law%20Regulation%20Compliance%20-%20ICP%20DeA%20WG%20Briefing%201_1.pdf

special thanks to @tinybird for the demo on Clanopedia and Tim for sharing his expertise on AI regulationsüí™

-------------------------

patnorris | 2024-08-22 18:26:09 UTC | #196

Hi everyone, thank you for today's call (2024.08.22). This is the generated summary (short version, [please find the long version here](https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/WorkingGroupMeetings/2024.08.22)): 
The DeAI Working Group for the Internet Computer discussed several key topics, including the one-year anniversary of running LLMs on the ICP, promotional activities, and the potential for organizing an AI symposium. The meeting also reviewed various AI projects on the Internet Computer, covering different programming languages and frameworks, such as TypeScript, C++, Python, and Rust, and discussed potential integrations with Awesome ICP and decentralized AI documentation. The group emphasized the importance of sharing and curating resources, training data, and ensuring comprehensive and up-to-date documentation for AI on ICP. Future discussions will focus on expanding these resources and fostering collaboration.

Links shared during the call:

* DeAI event in Palo Alto: https://decrypt.co/240216/palo-alto-ai-x-web3-summit-to-debut-at-stanford-university-this-october

* Updated outline of AI Documentation: https://docs.google.com/document/d/1tf6kbzE2Vqxb5I6JUWZzJJpQ0qmGXkOfNqcU4yBWOSQ/edit
* DeAI manifesto to contribute to: https://docs.google.com/document/d/1GM_QzDhU2DAzWm3C5RzU861qEje2lzy7Un_l_dYdhvc/edit#heading=h.8jg466jkf1fh
* DeAI repo: https://github.com/DeAIWorkingGroupInternetComputer/DeAIWorkingGroupInternetComputer/tree/main/Projects
* DeAI projects on Awesome ICP: https://github.com/dfinity/awesome-internet-computer
* Training data related article: https://arxiv.org/abs/2407.14933
* Open dataset: https://allenai.org/dolma

-------------------------

JaMarco | 2024-08-27 04:27:07 UTC | #197

https://x.com/NousResearch/status/1828121648383566270

-------------------------

JaMarco | 2024-08-27 20:58:50 UTC | #198

https://x.com/AlexanderJLong/status/1828147063533744256

-------------------------

