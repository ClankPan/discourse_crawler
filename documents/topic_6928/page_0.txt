ililic | 2021-09-02 22:17:29 UTC | #1

*Boundary nodes throttling users access to query and update calls of canisters on subnet pjljw*

# Summary

An NFT project launched fully on 2021-09-01 20:00 UTC with earlier waves at 2021-09-01 16:00 UTC and 2021-09-01 19:00 UTC. The first wave at 16:00 UTC kicked off an increased volume of traffic to the subnet pjljw.

![|624x137](upload://8bmpnSUrAh8omZbl4bJVjxMyk0B.png)

This traffic increased steadily and started to hit against the rate limits configured on the boundary nodes. The boundary nodes began limiting requests to canisters on the subnet. This caused impact to the project’s canister (responsible for the vast majority of the traffic):

![|624x249](upload://s9BDNjEEeoxcPih41zVMMYT9pqq.png)

As well as other canisters on the subnet.![|624x248](upload://8K0QoIqEVhE0hP29yWdgoJRsnOY.png)

The effective impact of this was that many users were unable to load applications on the webpage or interact with canisters as the majority of messages during this period of time were either rate limited or rejected by the replica. In the project’s case, many users were unable to load the application fully and claim their NFT.

During the peak period at 20:00 UTC, traffic increased dramatically, peaking at just over 38k requests/sec.

![|624x137](upload://kYlBpIu4kMHeKjLhflo3Lhphdje.png)

During earlier waves, the replicas and subnet behaved normally. During the peak period, an influx of heavy update traffic (202 call submit) came in going from 18 updates/s to over 1k updates/s to the subnet.

![|624x224](upload://2vdBchwbjbm4AbKbCFEB1oT2AAL.png)

Close up for the critical time period:

![|624x240](upload://wiE2L9YbZWvWpN3X3VF2buLuQY7.png)

This caused a dip in finalization rate from around 1 block/s to a bit over 0.3 blocks/s.

![|624x239](upload://bo655Jr3LAh9yBX3EROw46bn2Fo.png)

During this period, heavy ingress message throttling was observed, reaching levels of over 50 messages/s.

![|624x237](upload://xvoyWUO2C8PF1XMIEnx6ZrzZVOi.png)

Execution round instruction went to over 1.8 billion and execution rounds were taking around 2.5 seconds (with large outliers).

![|624x231](upload://6gWtKovbGGm6eZZNokjyss32oTz.png)

![|624x243](upload://gkCsGMD7R3uXMIJclP2AeKex26W.png)

During the peak period, client side (browser console log) visiting the project’s asset canister, directly served from the IC, shows a ton of error code 500 returned from boundary.ic0.app. Its frontend requires many assets to load to become fully functional: this explains why so many users were not able to do anything except keep reloading the page.

# Timeline (UTC)

* 2021-09-01T16:00 - The first wave of NFT claiming and countdown starts.
* 2021-09-01T16:15 - Rate limiting by the boundary nodes is observed on query calls, a pattern which continues to increase as 20:00 approaches.
* 2021-09-01T19:00 - The second wave of NFT claiming occurs; this causes further increases in traffic, but update volume is still low as this group has a limited number of participants.
* 2021-09-01T20:00 - The main drop of NTFs occurs and traffic dramatically increases, peaking at 38k req/s to the boundary nodes. Subnet pjljw finalization rate drops to 0.3 blocks/s
* 2021-09-01T20:05 - The ICA dashboard is observed to fail to load during this period.
* 2021-09-01T20:40 - Traffic tapers off as the drop ends and project shows sold out. Traffic drops to 10k req/s and continues to drop as time goes on.
* 2021-09-01T20:45 - The ICA dashboard returns to normal operation.
* 2021-09-01T20:45 - Subnet pjljw returns back to normal finalization rate.

# What caused the disruption?

High query load to a few canisters caused the rate limiting. The high update load caused the subnet performance to degrade afterwards. These two factors resulted in the project’s applications along with other canisters on the subnet being inaccessible to many users.

# What went wrong?

The IC replicas were not serving real user traffic anywhere close to the theoretical maximum rate.

# What went right?

The subnet pjljw continued processing queries and updates and did not fail despite the high traffic. The boundary nodes also continued successfully serving traffic. The rate limiting protections worked well and protected the subnet from the bulk of the traffic, which unfiltered could have caused more disruption in the subnetwork replicas.

# Follow-up action Items

* Improve documentation on how to scale decentralized applications on the IC.
* Enable (standards compliant) caching of HTTP on the boundary nodes and communicate best practices to developers.
* Evaluate query API call result caching on replicas within a block interval.
* Use more threads for Execution (currently we are single threaded using 1 of 64 cores).
* Load test and tune the rate limits based on more realistic traffic loads.

-------------------------

lastmjs | 2021-09-02 22:46:07 UTC | #2

I was experiencing network request errors on the NNS app and Distrikt I believe around the same time periods. Were other subnets affected? It seemed like almost a general slowdown, is there any evidence for this?

-------------------------

BHare1985 | 2021-09-02 23:03:40 UTC | #3

The follow-up action items seem to suggest that this could of been avoided by the developer. 
Can someone go into this further?

The rate limiting is interesting. I can understand a rate limit per an IP, but what is the purpose of a general rate limit?

The only appeal to me for IC right now (because it's quite centralized in practice) is the ability to not have to worry about handling the complexity of dev operations relating to scaling, uptime, and performance. Let's hope this never happens again on the IC because it looks really bad given how the technology was touted and how load balanced static-content competitors could of handled this load.

-------------------------

skilesare | 2021-09-02 23:03:30 UTC | #4

[quote="BHare1985, post:3, topic:6928"]
Let’s hope this never happens again on the IC because it looks really bad given how the technology was touted and how load balanced static-content competitors could of handled this load.
[/quote]

I think your expectations are a bit high for a beta product.  This will likely happen many more times as performance is dialed in and people best understand how to use the IC.  This happens to all networks and while there are some optimizations to be done here, on of the lessons is don't do a hype event where you steal hundreds of thousands of people's time trying to give them something for free only during a certain window.  Anyone remember what Cryptokitties did to the ETH network?  Or seen the gas prices lately?  Keep in mind that all those requests went through and all the NFTs were distributed at a constant cycle rate...that seems like a win too.

> The follow-up action items seem to suggest that this could of been avoided by the developer.
Can someone go into this further?

Have people register over a long period of time and then use the random beacon to distribute and you avoid this entire situation.

-------------------------

northman | 2021-09-03 02:52:48 UTC | #5

The transparency is appreciated.  As stated, the necessary guardrails were not in place to protect the subnet operation from a single application negatively impacting service availability.  This is serious and rating limits need to be tweaked ASAP. 

How does an application know that there is an issue?  Is an event thrown that is visible to the application to indicate a performance threshold has been hit?  Is there a performance monitoring api exposed to the developer?  Not sure if there is a performance dashboard that is accessible.  The IC is supposed to hide all the complexity and get the developer out of the Operations role.

Educating developers is great but expect malicious actors to do the opposite. 

This is a beta product so glitches are expected.  Perfection is not what is needed.  However, timely corrective action is needed.  The corrective actions should be tracked and put forward as NNS proposals when ready for deployment and made visible.  

Thanks for sharing the report.  Far better than what industry has been providing IMHO.

-------------------------

CryptoKnight | 2021-09-03 03:43:05 UTC | #6

What's dfinity doing to avoid the same in the future? And how long will it take?

-------------------------

alexander | 2021-09-03 06:01:59 UTC | #7

"The IC replicas were not serving real user traffic anywhere close to the theoretical maximum rate." - What is theoretical maximum rate for this number of nodes in current subnet? queries and updates.

Also it is very interesting to know the relation between adding nodes to subnet and throughput for updates, how many updates can handle one submet?

-------------------------

saikatdas0790 | 2021-09-03 06:50:38 UTC | #8

[quote="ililic, post:1, topic:6928"]
Use more threads for Execution (currently we are single threaded using 1 of 64 cores)
[/quote]

Wow, this sounds like such a waste. What's the plan for utilizing the full capability of the hardware?

-------------------------

free | 2021-09-03 07:31:11 UTC | #10

> I was experiencing network request errors on the NNS app and Distrikt I believe around the same time periods. Were other subnets affected? It seemed like almost a general slowdown, is there any evidence for this?

Looking at the dashboard for the NNS subnet, there's a slight increase in traffic with a corresponding increase in "CalledTrap" errors; as in some canister, presumably Internet Identity, explicitly called `trap` / `panic` (and if I had to guess, that would be something like using the wrong YubiKey). But the error rate is consistently well under .5 TPS and mostly under .1 TPS.

![Screenshot from 2021-09-03 09-21-33|690x351](upload://tlnoIXMOM7Wu9VmBYODXPisqCjT.png)

It could have been boundary node throttling or something completely different, but I'm not seeing it.

-------------------------

singularity | 2021-09-03 07:35:35 UTC | #11

I experienced issues logging in and using NNS App at this time too. Was it related? Also, there were cors errors on the browser console, did the throttling cause that?

-------------------------

free | 2021-09-03 07:39:29 UTC | #12

[quote="BHare1985, post:3, topic:6928"]
The rate limiting is interesting. I can understand a rate limit per an IP, but what is the purpose of a general rate limit?
[/quote]

It's not a general rate limit, it's a rate limit per subnet. All replicas on a subnet do the exact same replicated workload (ingress message and canister message execution) and share the same query load (round-robin) so it makes sense that they share the same rate limits.

> Let’s hope this never happens again on the IC because it looks really bad given how the technology was touted and how load balanced static-content competitors could of handled this load.

Yes, there's still a lot of work to do (including optimization) and this the first time we've experienced this level of traffic on a single subnet. That being said, the scalability claims were made for the whole of the IC, not for single canisters. There is only so much a single-threaded canister can scale before it hits a wall. (And canisters need to be single-threaded because they need to be deterministic. There may well be more elaborate solutions to that, but the IC is not there yet.)

-------------------------

free | 2021-09-03 07:46:34 UTC | #13

[quote="northman, post:5, topic:6928"]
How does an application know that there is an issue? Is an event thrown that is visible to the application to indicate a performance threshold has been hit? Is there a performance monitoring api exposed to the developer?
[/quote]

Not yet. What we're using internally to monitor NNS canisters is a hand-rolled Prometheus / OpenMetrics `/metrics` query endpoint where we export timestamped metrics (queries handled, Internet Identities created, etc.). Timestamped because you can't guarantee which replica you will be hitting and some replicas may be behind. Prometheus will drop samples with duplicated or regressing timestamps, so this solves the issue of counters going back.

Of course, a lot more than that can be provided by the IC directly, but there is data privacy to be taken into consideration and we're still working down our priority list.

-------------------------

free | 2021-09-03 07:56:23 UTC | #14

[quote="saikatdas0790, post:8, topic:6928, full:true"]
[quote="ililic, post:1, topic:6928"]
Use more threads for Execution (currently we are single threaded using 1 of 64 cores)
[/quote]

Wow, this sounds like such a waste. What’s the plan for utilizing the full capability of the hardware?
[/quote]

This 1 core limit only applies to this one subnet and (by total coincidence) was introduced earlier this week, due to limitations we hit because of another canister (with updates and queries that read and wrote tens of MB of memory each) and how that interacted with our orthogonal persistence implementation (which uses `mprotect` to track dirty memory pages; and that we're already working on optimizing).

The other thing that we're working on is canister migration (to other subnets), but that's still in the design stages. We will share a proposal when we have something more solid. Canister migration would have allowed us to move ICPunks to a different subnet, which would not have made a huge difference to ICPunks itself, but would have had less of an impact on other canisters on the subnet (due to there being 64 cores available for execution).

-------------------------

flyq | 2021-09-03 08:02:55 UTC | #15

Thank you for being able to provide so much information transparently.

[quote="ililic, post:1, topic:6928"]
communicate best practices to developers.
[/quote]
Looking forward to the documentation about These.

I encountered a problem before, there is a choice, 1. Double the storage, such as adding a hashmap, the advantage is that only O(1) complexity when querying. 2. No double the storage, but it is O(N) when querying, need to traverse the array and make some judgments, I don't know which is better.

-------------------------

free | 2021-09-03 08:07:01 UTC | #16

[quote="alexander, post:7, topic:6928, full:true"]
“The IC replicas were not serving real user traffic anywhere close to the theoretical maximum rate.” - What is theoretical maximum rate for this number of nodes in current subnet? queries and updates.
[/quote]

I don't think the subnet could have served a lot more update traffic for the ICPunks canister. It was already running into the instructions per round limit (which is there to ensure blocks are executed in a timely fashion, so block rate is kept up; and to prevent one canister from hogging a single CPU core for a very long time while all other canisters have finished processing and all other CPU cores are idle). It could have handled more updates in parallel on other canisters though, had we not introduced this 1 CPU core limit earlier this week (to this one subnet).

As for read-only queries, it could have probably handled about 2x the traffic, but currently the rate limiting is static and it would need to be dynamic (based on actual replica load) in order to scale with the capacity instead of needing to be conservative.

> Also it is very interesting to know the relation between adding nodes to subnet and throughput for updates, how many updates can handle one submet?

Adding more nodes to a subnet will not increase update throughput. All replicas execute all updates in the same order, so update throughput would be exactly the same, but latency would be increased (because Gossip and Consensus need to replicate ingress and consensus artifacts to more machines and tail latency increases).

It would increase read-only query capacity though, proportionally to the number of replicas (as each query is handled by a single replica).

-------------------------

flyq | 2021-09-03 08:30:57 UTC | #17

Also
1. Does the current cycles model truly reflect the consumption of physical resources (of course, the cycles model not only needs to consider the consumption of physical resources, but also needs to consider lowering the threshold for developers, etc.), and is there any subsequent adjustment plan? There was an unreasonable gas price setting for an op code in Ethereum, which led to [an attack by DDoS](https://blog.ethereum.org/2016/09/22/ethereum-network-currently-undergoing-dos-attack/).
2. Is there an accurate description of all operations that consume cycles so that developers can practice best practices. 

https://forum.dfinity.org/t/is-there-any-document-about-the-amount-of-memory-occupied-by-the-basic-types-of-motoko-canister/6442

-------------------------

free | 2021-09-03 10:11:11 UTC | #18

[quote="flyq, post:17, topic:6928"]
1. Does the current cycles model truly reflect the consumption of physical resources (of course, the cycles model not only needs to consider the consumption of physical resources, but also needs to consider lowering the threshold for developers, etc.), and is there any subsequent adjustment plan? There was an unreasonable gas price setting for an op code in Ethereum, which led to [an attack by DDoS](https://blog.ethereum.org/2016/09/22/ethereum-network-currently-undergoing-dos-attack/).
[/quote]

The cycles model does reflect the cost of physical resources, but while it did involve careful thinking, it is based on test loads and estimates, not real-world traffic (which didn't exist back when the cycle model was produced). As a result, it is very likely to get adjusted, although no clear plans for that are yet in place.

> 2. Is there an accurate description of all operations that consume cycles so that developers can practice best practices.

[Here](https://github.com/dfinity/ic/blob/35dd8f93dec82662ed4df35664a9c0be6dbf203a/rs/config/src/subnet_config.rs#L167-L184) is everything that is currently being charged for and the cycle costs. But I would optimize for performance in general rather than cycle costs specifically, as cycle costs are quite likely to follow performance at some point in the future (as in, everything that's slow or causes contention or whatever is likely to cost more and the other way around).

-------------------------

alexander | 2021-09-03 10:22:17 UTC | #19

[quote="free, post:16, topic:6928"]
Adding more nodes to a subnet will not increase update throughput. All replicas execute all updates in the same order, so update throughput would be exactly the same, but latency would be increased (because Gossip and Consensus need to replicate ingress and consensus artifacts to more machines and tail latency increases).
[/quote]

This is exactly what I assumed. In that case to achieve write performance, I need to deploy canister to many subnets and do data sharding on the application level. Am I right?

-------------------------

stopak | 2021-09-03 10:32:24 UTC | #20

I belive that this could solve the problem. Now the question is how to make proper sharding :)

-------------------------

free | 2021-09-03 11:12:52 UTC | #21

[quote="alexander, post:19, topic:6928, full:true"]
[quote="free, post:16, topic:6928"]
Adding more nodes to a subnet will not increase update throughput. [...]
[/quote]

This is exactly what I assumed. In that case to achieve write performance, I need to deploy canister to many subnets and do data sharding on the application level. Am I right?
[/quote]

Not necessarily to many subnets, a single subnet might suffice. (Think of subnets as VMs and canisters as single-threaded processes. In this instance.) You can likely scale quite a bit on a single subnet (by adding more canisters) before you need to scale out to other subnets.

Once you start scaling across subnets, if there's any interaction between canisters (and I would assume there would be at least some for most applications) you have to account for XNet (cross-net) latency, which is more or less 2x ingress latency (the "server" subnet needs to induct the request, the "client" subnet the response). Whereas on a single subnet without a backlog of messages to process this roundtrip communication could even happen within a single round.

-------------------------

skilesare | 2021-09-03 11:38:16 UTC | #22

[quote="flyq, post:15, topic:6928"]
I encountered a problem before, there is a choice, 1. Double the storage, such as adding a hashmap, the advantage is that only O(1) complexity when querying. 2. No double the storage, but it is O(N) when querying, need to traverse the array and make some judgments, I don’t know which is better.
[/quote]

The right answer is almost always double the storage. Almost every DB in the world uses some kind of index to speed up queries. If you are relying on iterating through a list(tablescan in DB speak) your app is doomed to crawl sooner or later. Clustered data and an index can get you there for data where the data can be ordered for much less than 2x, but your writes are going to be heavier. Everything has a trade off, but there is usually a trade off to make for your future sanity and your user’s experience.

-------------------------

janeshchhabra | 2021-09-03 17:50:08 UTC | #23

Looking into this, evidence of load shedding on other subnets was discovered. The boundary nodes were running out of file descriptors and thus were unable to open socket connections to serve load. The volume of this was not too high, but it was sufficient to cause the general slowdown you observed. In this case, further investigation is needed to determine what caused this bottleneck (since it could be that threads/connections were piling up on boundary nodes because replicas were being slow to respond).

> Load test and tune the rate limits based on more realistic traffic loads.

The boundary nodes will be included in this action item to better tune their performance and prevent this cross-subnet impact.

-------------------------

giuppo | 2021-09-03 21:17:29 UTC | #24

Thank you for the post!

Could you please give more details on the fact that early warnings (limiting requests) were triggered hours before the actual 'flood'?  More specifically, what was the actual metric and value v* used to limit subnet reqs?

Was v* a configuration mistake? i.e. A 'shot in the dark' value? Of course it is understood everything is 'beta' and probably there was not time to benchmark/stress test a 'staging' subnet, but if this is the case can you please explain why action (*) was not taken hours before the actual hurricane? After all, ICPunks publicly planned the event.

If v* was actually an educated guess and there was no way to increase the limit or increasing the limit would not have helped (as @free suggests), can we get to the conclusion that a single app deployed on a 13 fat-nodes subnet cannot sustain a traffic of ~ 15k reqs/sec (i.e. traffic at 20:00)?

As for the action item "Improve documentation on how to scale decentralized applications on the IC."
Can you please anticipate how to achieve this? How could icpunks have deployed its code in a more scalable way?

OT
The ICP burned over time chart https://ic.rocks/charts does not show any 'previously unseen' pattern. I understand that it is an IC global metric, but it seems strange that such an exceptional event has not left a trace there?! @wang 
/OT

Thank you

(*) increase the limit or remove the limit to protect other canisters, or liaise with icpunks to consensuate a solution to avoid the 'difficult' launch, or...

-------------------------

free | 2021-09-04 11:54:35 UTC | #25

[quote="giuppo, post:24, topic:6928"]
Could you please give more details on the fact that early warnings (limiting requests) were triggered hours before the actual ‘flood’?
[/quote]

Traffic was already considerable before 20:00 UTC. Some people got early access to claim their tokens (before it was open to the public, I believe there were 2 previous rounds) and they (plus some eager regular users) were already putting significant load onto the canister before 20:00 UTC. At some point that went over the configured limit, whatever that was, and requests started being dropped.

[quote="giuppo, post:24, topic:6928"]
Was v* a configuration mistake? i.e. A ‘shot in the dark’ value?
[/quote]

The boundary node rate limit (which I don't actually know, but I suppose is something like 1K QPS per subnet) is static. Whatever static limit you set there is bound to be inaccurate, as not all queries (or updates) are equal. E.g. in this case queries were rather lightweight (under 10K Wasm instructions each). Earlier this week and the week before we were seeing queries that were 4-5 orders of magnitude heavier (as in, up to 1-2B instructions, nearly 1 second of CPU time).

Whatever static limit you put in place, it is not going to cover both those extremes. What's necessary is a feedback loop allowing boundary nodes to limit traffic based on actual replica load.

[quote="giuppo, post:24, topic:6928"]
can you please explain why action (*) was not taken hours before the actual hurricane?
[/quote]

It took us a couple of days to piece together exactly what happened from metrics and logs. There was no action that could be taken ahead of time, particularly with no insight into how the respective dapp was put together (how many canisters, where, etc.).

[quote="giuppo, post:24, topic:6928"]
can we get to the conclusion that a single app deployed on a 13 fat-nodes subnet cannot sustain a traffic of ~ 15k reqs/sec (i.e. traffic at 20:00)?
[/quote]

A subnet is essentially a (replicated) VM. A canister is (for the purposes of this analysis) a single-threaded process as far as transactions are concerned; and a distributed service for read-only queries.
So a canister can execute (almost) as many transactions as can run sequentially on a single CPU core.

And ~1K read-only QPS (which is what the subnet was handling according to the replica HTTP metrics) is entirely reasonable if you consider (a) that there was no HTTP caching in place (every single query needed to execute canister code) and (b) single-threaded execution (including for queries) was in place on this subnet only, as a temporary mitigation for a different issue (high contention in the signal handler-based orthogonal persistence implementation).

And again, both query and transaction throughput depends on how much processing each requires: returning 1KB of static content is very different from scanning 100 MB of heap to compute a huge response. In that context "15k reqs/sec" is somewhat meaningless.

[quote="giuppo, post:24, topic:6928"]
As for the action item “Improve documentation on how to scale decentralized applications on the IC.”
Can you please anticipate how to achieve this? How could icpunks have deployed its code in a more scalable way?
[/quote]

A few of the things I can think of: query response caching (via HTTP expiration headers), spreading load across multiple canisters and/or subnets, single-canister load testing to see what a realistic expected throughput (based on instruction limits) should be. Some of these would benefit from additional tooling or protocol support, most can already be put in practice.

[quote="giuppo, post:24, topic:6928"]
The ICP burned over time chart [Charts | ic.rocks](https://ic.rocks/charts) does not show any ‘previously unseen’ pattern. I understand that it is an IC global metric, but it seems strange that such an exceptional event has not left a trace there?!
[/quote]

Queries and query traffic are (for now) free. A canister executing transactions on a single thread can only burn so many cycles within half an hour (which is how long the whole thing took before there was no more need to execute any transactions) even at full tilt.

I hope this answers your questions.

-------------------------

giuppo | 2021-09-04 23:12:14 UTC | #26

Yes, it helped a lot, especially the technical insights.
The IC is really impressive.

I still cannot understand why nobody warned icpunks at 4pm that pjljw was bound to be killed because of the airdrop, as it was clear from the metrics. Icpunks could have helped in many different ways (maybe even cancelling the airdrop), but I take this was not a purely technical decision.

> A few of the things I can think of: query response caching (via HTTP expiration headers), spreading load across multiple canisters and/or subnets, single-canister load testing to see what a realistic expected throughput (based on instruction limits) should be. Some of these would benefit from additional tooling or protocol support, most can already be put in practice.

I think I understand, but does not that mean that the whole burden then is on the app development? I cannot see how the IC can help my app 'scaling'. Infinite scaling is 'you can deploy infinite canisters, but!'?

HTTP caching, refactor/rewrite my app 'spreading load across multiple canisters and/or subnets' (?microservices?) and local benchmarking are hard.

If I understand correctly, after writing some motoko and dfx deploy, I have a 'thread' on some machine, so I'd better heavily optimize if I expect usage, as I cannot stop/start instance to change its type, or pay for a bigger dyno, or deploy k8s autoscaling, or pay for more lambda concurrency, or ...? 

But maybe dfx can already help?

-------------------------

jzxchiang | 2021-09-06 07:32:34 UTC | #27

It'd also be nice to get some info on these boundary nodes, since they are evidently very important.

Where do they run? Who owns them? How many are there?

-------------------------

free | 2021-09-06 17:48:08 UTC | #28

[quote="giuppo, post:26, topic:6928"]
I still cannot understand why nobody warned icpunks at 4pm that pjljw was bound to be killed because of the airdrop, as it was clear from the metrics. Icpunks could have helped in many different ways (maybe even cancelling the airdrop), but I take this was not a purely technical decision.
[/quote]

A number of reasons: we didn't know anything about the ICPunks architecture or how they planned to handle the load. Just before 20:00 all ingress messages were being processed normally and there was no drop in finalization rate. Some query messages were being dropped, but that's neither here nor there. ICPunks themselves, even in the absence of metrics, could have tried loading their own website and checking if it was working as expected or not. And finally, no one was specifically looking at dashboards with the goal of identifying and quickly addressing any issues related to the ICPunks launch.

The oncall engineer did get paged and a group of us did get together to try and piece together what was happening, but we were more interested in the health of the subnet and the IC as a whole rather than the ICPunks drop specifically.

[quote="giuppo, post:26, topic:6928"]
I think I understand, but does not that mean that the whole burden then is on the app development? I cannot see how the IC can help my app ‘scaling’. Infinite scaling is ‘you can deploy infinite canisters, but!’?
[/quote]

I will point out again that the marketing blurb was about the IC itself being "infinitely scalable" (ignoring the "infinitely" bit), not canisters or even dapps. Certainly not trivially scalable by default (as in, you allocate 1000 CPU cores and you can handle 1000x the traffic). Nothing can do that.

[quote="giuppo, post:26, topic:6928"]
HTTP caching, refactor/rewrite my app ‘spreading load across multiple canisters and/or subnets’ (?microservices?) and local benchmarking are hard.
[/quote]

I wouldn't call them hard, but they do require work, yes. E.g. HTTP caching is simply asking yourself "How often is the value returned by this API going to change? Is it going to be different for different users?". E.g. the ICPunks canister had a "dtop start timestamp" (which returned "2021-09-01 20:00 UTC"), which could have been trivially cached. And the local benchmarking that was necessary was to simply call the "claim" API in a loop from a shell script or whatever and see how many NFTs could be claimed per second. They would have found out that number was ~20 and could have optimized the respective code.

I'm not placing blame on ICPunks, BTW. Just pointing out that with some decent best practices in place (which is our job to come up with) a canister developer can significantly improve their dapp with comparatively little work.

[quote="giuppo, post:26, topic:6928"]
If I understand correctly, after writing some motoko and dfx deploy, I have a ‘thread’ on some machine, so I’d better heavily optimize if I expect usage, as I cannot stop/start instance to change its type, or pay for a bigger dyno, or deploy k8s autoscaling, or pay for more lambda concurrency, or …?
[/quote]

There is no library or dashboard where you can tweak a knob that will result in N instances of your canister being immediately deployed (there should be one; or more). But it's also not impossibly hard to do so. All you need is a wallet canister (per subnet; something else that needs improvement)  that you can then ask to create as many canisters as you need and you can install your Wasm code into each such canister.

Of course you need to figure out how exactly you're going to spread load across said canisters (e.g. load 1K NFTs into each canister and have some Javascript in your FE pick one of them at random; or something more elaborate). But you don't have to configure a replicated data store, integrate with a CDN; or set up firewalls. And then maintaining those.

[quote="jzxchiang, post:27, topic:6928, full:true"]
It’d also be nice to get some info on these boundary nodes, since they are evidently very important.

Where do they run? Who owns them? How many are there?
[/quote]

Boundary nodes are basically proxies. There is only a handful of them, managed by Dfinity, and they run in some of the same independent datacenters where the replicas run. More importantly though, anyone can run their own. I know Fleek does (it's how they manage to have custom domain names for their clients), but I don't know the details.

-------------------------

jzxchiang | 2021-09-06 18:55:04 UTC | #29

If you run your own boundary node, wouldn't you need to set up your own DNS entry to point to that boundary node?

I'm guessing a URL like `https://h5aet-waaaa-aaaab-qaamq-cai.raw.ic0.app/` points to a boundary node, which then forwards the call to the actual IC replica. What I'm unclear about is how the boundary node knows what the domain name (or IP address) of the replica is.

-------------------------

free | 2021-09-06 20:02:30 UTC | #30

[quote="jzxchiang, post:29, topic:6928"]
I’m guessing a URL like `https://h5aet-waaaa-aaaab-qaamq-cai.raw.ic0.app/` points to a boundary node, which then forwards the call to the actual IC replica. What I’m unclear about is how the boundary node knows what the domain name (or IP address) of the replica is.
[/quote]

The boundary node knows the IP addresses of the NNS replicas. These all host identical copies of the Registry canister (among others) and the Registry lists all subnets, all nodes on each subnet and the mapping of canister ID ranges to subnets. `h5aet-waaaa-aaaab-qaamq-cai` is a canister ID (DSCVR, in this case) hosted on the `pae4o-...` subnet. The boundary node then forwards the request to one of the replicas that make up the `pae4o` subnet. That's all there is to it, really.

-------------------------

jzxchiang | 2021-09-06 20:31:00 UTC | #31

Oh interesting, I didn't know there was a registry canister.

Is there any documentation on how to run your own boundary node? In Ethereum, anyone can run their own node (or they can choose to use a centralized node provider like Infura, which in this case is the Dfinity-managed boundary nodes I guess?).

-------------------------

giuppo | 2021-09-07 22:27:40 UTC | #32

Understood. 

>some decent best practices in place (which is our job to come up with) a canister developer can significantly improve their dapp with comparatively little work

To this regard, where one should expect to find this information?
I struggle to divide the different info sources (dfinity library, sdk web, github repos, medium posts, reddit AMAs,...) into 'domains', and I cannot see this forum as a scalable (pun intended) place for 'IC deep engineering' docs @diegop 

Many thanks for your time @free

-------------------------

alexander | 2021-09-10 06:45:51 UTC | #33

[quote="free, post:21, topic:6928"]
Not necessarily to many subnets, a single subnet might suffice. (Think of subnets as VMs and canisters as single-threaded processes. In this instance.) You can likely scale quite a bit on a single subnet (by adding more canisters) before you need to scale out to other subnets.
[/quote]

By the way. Are there any possibility to set which subnet to use during creating/deploying canister? Right now I can create as many canister as I need but I cannot control which subnet will be used for canister. Or I missed something? Who is define which subnet will be used while creating a new canister?
A good post how to build a scalable application on the dfinity whould be great!

-------------------------

lastmjs | 2021-09-09 18:16:07 UTC | #34

Ignoring update calls for now, I would love to see the IC itself provide infinite scaling of query calls for canisters. Maxing out the cores on individual replicas will help, but could the IC manage backup read-only replicas that are ready to be deployed ad hoc into subnets experiencing spikes in query traffic? I imagine this would be relatively simple to do, at least theoretically.

As an example, imagine the ICPunks drop. Assuming there were 7 full replicas in the subnet, as query traffic began to approach certain limits, the subnet would request extra read replicas. These could be relatively quickly added into the subnet, using the catch-up package functionality to quickly get the replica onboard. It wouldn't particpate in consensus, it would be a read-only replica.

As traffic continued to increase, read-only replicas could continue to be added. The IC would have to maintain a pool of these replicas, always ready to be deployed where needed. Once traffic died down, the replicas would be returned to the pool. If the traffic never died down, perhaps the replica would become a permanent part of the subnet.

So subnets might have a fixed number of full consensus replicas, and some number of read-only replicas. This would not slow down consensus, but would scale out query calls infinitely without the developer needing to do anything fancy (even a single canister would automatically scale out queries).

Please consider this, I think it would be a powerful capability.

-------------------------

diegop | 2021-09-10 04:15:15 UTC | #35

@lastmjs fwiw, I passed your suggestions along to R&D for wider visibility, to make sure more people saw it.

-------------------------

PaulLiu | 2021-09-10 04:26:44 UTC | #36

@lastmjs Thanks for the suggestion! Yes it is a very good idea! We had similar thoughts in the same direction, but didn't prioritize because there are other quicker ways of improving query performance. The priority was (and I believe still is) exploring ways to maximize the utility of existing hardware.

Just a quick update on what we have discovered and fixed:

1. Boundary nodes were running out of file descriptors which caused many 500s. This has been fixed.
2. Another bug was identified that caused boundary nodes to still reach out to defective subnet nodes and then return 502s to user (which causes CORS error). The fix will be rolled out tomorrow.
3. We are still working on improving multithread query execution on subnet nodes. Making progress, but not final yet.
4. We are working on caching at system level for query calls. This should drastically reduce loads on query execution especially during hot spot events like this. Whether this should be done at boundary nodes side or subnet nodes side is still being explored.
5. We are still trying to understand a good balance on rate limiting. Parameters have not been adjusted yet.

There are a couple things a canister developer can do to help improve usability:
1. Implement http_request call with custom response header to control cache expiry.
2. Code defensively when using query calls. Catch errors & try again later instead of waiting for user to reload the whole page (which makes congestion situation worse).
3. Understand that a query call may not always execute against the latest state. Some user saw that "remaining punks" number rolled back which gave a bad impression, but this can be avoided by adding a bit of client side logic.

Please watch this space for more updates! Thanks!

-------------------------

PaulLiu | 2021-09-10 04:54:36 UTC | #37

Another idea I have, just to throw it out there:

I think we can improve asset canister to index assets by sha256 hash (or some shorter version). This can be optional, but when coupled with webpack to properly rewrite links in HTML, it can offer immutable assets, or in other words, the asset canister can serve them with a very long expiry timestamp. This may help quite a bit with relatively no effort on developer side.

What do people think?

-------------------------

stopak | 2021-09-10 17:56:10 UTC | #38

Thanks for the great work!!

-------------------------

BenTaylor | 2021-09-19 18:21:52 UTC | #39

I have noticed that the performance of some applications on the Internet Computer has been degraded over the weekend (at times making some of them unusable). The problems seemed to start around the ICPBunnies "testnet" yesterday, but have lingered since. Is this caused by the same issues surrounding the ICPunks launch earlier this month?

-------------------------

diegop | 2021-09-19 18:52:41 UTC | #40

Thanks for the heads up! Im am not aware of anything, so let me ping folks to see if they see something

-------------------------

PaulLiu | 2021-09-19 19:01:20 UTC | #41

I'm not aware of any noticeable traffic or workload over the weekend. One of the subnet `pjljw` (the same one that ICPunks was on) has a long standing problem that required a temporary fix to restrict its query execution to 1 thread. We are aware of the degraded performance, but it shouldn't affect other subnets, unless ICBunnies happens to be on the same subnet, which would be very unforutnate.

That said, we have a fix (that is tested have very good improvement) ready to deploy to this subnet (hopefully tomorrow) via NNS proposals. We also have query caching implemented and currently under going internal testing. So we should expect IC subnets to have significant improvements at handling user traffic very soon.

-------------------------

BenTaylor | 2021-09-19 19:04:06 UTC | #42

Thank you for the quick and detailed response.

-------------------------

MisterSignal | 2021-10-08 16:08:48 UTC | #43

Happened again with NFT sale, and minting is currently paused to deal with the bottleneck:

https://forum.dfinity.org/t/icpuppies-another-nft-event-overwhelmed-the-ic/7773

-------------------------

jplevyak | 2021-10-08 17:59:58 UTC | #44

We have made a number of improvements and more are in progress to address this performance issue.  We have improved the efficiency and parallelism of canister smart contract execution and we have improved our load shedding at the node level.  This will enable us to increase the traffic which we allow through the boundary nodes and ultimately to fully decentralize the boundary nodes without risking the health of individual nodes, subnets and the IC overall.  We in the process of adding caching at the boundary nodes and we will be adding caching support in the service worker and we are investigating adding caching at the replica node layer as well.

Safety and stability are of utmost importance and while we are disappointed this subnet cannot handle all the load being thrown at it at this time, we are confident that we can continually improve performance.

-------------------------

PaulLiu | 2021-10-08 18:15:24 UTC | #45

From the look of it, query calls were less of a bottleneck this time. Rate limiting still kicked in at the peak time (despite the subnet still has capacity due to the conservative measure we took), so we did hit some 503s, but I expect this to be improved very soon.

But your question is pointing to a issue of a different nature, we are still trying to understand what was going on.

-------------------------

MisterSignal | 2021-10-08 18:15:38 UTC | #46

I'm confident in you, too! 

Perhaps a path for projects to directly submit incident reports (if one doesn't exist already) would be valuable for events like this.

-------------------------

MisterSignal | 2021-10-08 18:16:20 UTC | #47

It looks like there were around 1500 transactions out of the first 7k mint attempts that ended up failing -- Bob Bodily from Toniq Labs should have the details.

-------------------------

bob11 | 2021-10-08 18:50:21 UTC | #48

I think we were running into issues with the ICP ledger canister not being able to handle all of the requests this time around (punks didn't hit the ICP ledger canister at all) and were getting the majority of the errors from the ICP transfer requests. Happy to get on a call with Stephen (at Toniq) and you all if you want more info.

-------------------------

PaulLiu | 2021-10-28 02:09:05 UTC | #49

A short update on more recent progress:

1. Concurrent query execution and its memory usage has been fixed. Ulan Degenbaev wrote [an excellent article on this](https://medium.com/dfinity/optimizing-the-internet-computer-memory-systems-performance-c0253e94f60), worth a read!

2. We have also deployed query caching on boundary nodes. All query calls are cached for 1 second by default. Since states are not expected to change within 1 second, this should take off a lot of load when a canister becomes the hot spot.

3. With the caching, query calls are also rate limited separately to protect subnet nodes from being DoS-ed.

With the above fixes, we expect overall performance improvements at handling query loads and emergent traffic. That said, we still have some work in progress:

- Respect Cache-Control response header that is returned from the http_request public method of a canister, so that developers will have greater control over how static assets are managed and transported.

- Query cache in replica because it knows whether a canister state has been updated or not.

As always, please help report any problem you notice to help us improve! Thanks!

-------------------------

nomeata | 2021-10-28 21:01:44 UTC | #50

[quote="PaulLiu, post:49, topic:6928"]
Query cache in replica because it knows whether a canister state has been updated or not.
[/quote]

Hmm, but it would not know whether the canister would behave differently based on time or cycle balance (which can change even if the state didn't). Some caching might be possible, of course.

-------------------------

free | 2021-10-30 07:24:11 UTC | #51

[quote="nomeata, post:50, topic:6928"]
Hmm, but it would not know whether the canister would behave differently based on time or cycle balance (which can change even if the state didn’t).
[/quote]

Don't queries also use block time? (Honest question, I don't know the answer.)

But regardless, due to network latency, queuing, clock drift, hitting a different replica etc. you can't really expect query responses that are accurate WRT sub-second time or cycle balance changes. E.g. you may issue your query at time T and it can execute at T+.5s; or because replica clocks are not in sync, the other way around.

So whether those inaccuracies come from the above limitations; or from getting a cached response; it doesn't really matter They will still happen.

-------------------------

nomeata | 2021-10-30 09:38:31 UTC | #52

[quote="free, post:51, topic:6928"]
Don’t queries also use block time? (Honest question, I don’t know the answer.)
[/quote]

I am also not sure. The public spec doesn’t indicate that. I would expect it to be the block time of the most recently finalized block or something like that, whether the canister had a state change there or not. But one could feasibly say it can be up to, say, 10s behind in order to have a larger window for caching.

> So whether those inaccuracies come from the above limitations; or from getting a cached response; it doesn’t really matter They will still happen.

Absolutely! As long as caching doesn’t inflate that “acceptable delay“ too much. Caching the query response for hours might be bad here.

-------------------------

