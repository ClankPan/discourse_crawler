diegop | 2022-07-26 19:30:31 UTC | #1

# DRAFT PROPOSAL

*Below is an example of a proposal we (the DFINITY R&D team) intend to submit to the NNS in a few days/weeks, depending on community feedback as well as questions and answers. We want to hear what you all think, including any wording changes we need to make*


***

## Summary 

TLDR: As part of the work to further decentralize the infrastructure layer, we want to submit a new motion proposal to introduce a new type of hardware spec (and its corresponding remuneration) for nodes on the IC. We would like to get community feedback on this before proposing. 

## 1. State of the world

Nodes are remunerated based on their location and node type. The [Node Rewards table](https://dashboard.internetcomputer.org/proposal/44877) shows the rates per location and node type.

The current types are are listed here:

https://wiki.internetcomputer.org/wiki/Node_provider_hardware

## 2. What we are proposing

If you vote ACCEPT, you are agreeing on two things:

1. **IC should Introduce a new node type**. The new type has requirements independent of vendors (except for the CPU).

You can see the details for the new proposed type here:

* Dual Socket AMD EPYC 7313 Milan 16C/32T 3 Ghz, 32K/512K/128M
  * optionally 7343, 7373, 73F3
* 16x 32GB RDIMM, 3200MT/s, Dual Rank
* 5 x 6.4TB NVMe Mixed Mode
* Dual Port 10G SFP or BASE-T
* TPM 2.0

2. **DFINITY foundation will determine the expected cost of the new node type based on data from several independent vendors and propose reward rates based on an expected node lifetime of 4 years**. DFINITY R&D will research various ways to construct the vendor-generic node type and propose rates for this node type.

This is a governance proposal, so if this vote passes, there will be subsequent NNS proposals to introduce the new node type and reward rates to the NNS.

## 3. Why we are doing proposing this

This new node type is being introduced for two reasons:

a. The current node specifications are vendor-specific which is an unnecessary centralisation a year after launch. Vendor-specific specs also makes adding future nodes more difficult as it is harder to buy machines with older hardware specs.

b. The current node types do not support VM memory encryption and attestation which will be needed in future features.

## 4. What we are asking the community

* Read proposal
* Ask questions
* Give feedback

-------------------------

diegop | 2022-07-07 14:10:58 UTC | #2

For any questions, @Luis (who runs the node provider efforts at DFINITY) will take any questions!

-------------------------

mparikh | 2022-07-10 02:13:20 UTC | #3

Typically such proposals ideally should come with options that are cost versus performance driven; otherwise there is no particular debate or discussion to be had.

The real work of figuring out what the cost of this hardware is (including hosting) vs what the potential reward would be for the different options SHOULD BE CLARIFIED UP FRONT. 

Otherwise what , exactly, are we discussing on?

-------------------------

Luis | 2022-07-13 20:33:56 UTC | #4

Totally agree. That's why I wanted to explain the additional cost/rewards to performance ratio in the upcoming updates on this thread and subsequent motion proposal. The debate internally was: "How baked does this effort needs to be for community to be looped in?"

In this case, we opted for "Lets let folks know our intent. Post updates as we learn more. Iterate based on feedback. Dont get too far down the project without feedback."

-------------------------

mparikh | 2022-07-13 21:29:55 UTC | #5

I am not exactly sure why there is not more traction on this thread from other potential node-providers. This will impact their bottom lines. 

But i agree with you, @luis. I think some amount of background work is necessary for something of this magnitude. Otherwise we will come back in year scratching our heads as to why only one option (which is really not an option within options) was covered.

For example, why limit ourselves to Milan? Why not Milan-x? Most of our current workloads have likely  low L3 cache miss rates and high L3 cache coherency misses. But is this going to change in the future? I believe so because we should be gravitating towards exploiting the data currently in possession instead of using it just once. i.e. would a  7573X be an option?

-------------------------

Luis | 2022-07-14 09:54:59 UTC | #6

> For example, why limit ourselves to Milan? Why not Milan-x?

The Milan 7373 that we allow optionally is an X model. The reason we chose these models is trivial: We want the node type 3 spec to be as near as possible to the type1 in order to use both node types in the same subnet for a while. If this wouldn't be possible we would need to onboard many new NPs before the first subnet with a sufficient decentralisation can be created. That could mean these nodes would cause costs before the can provide value to the network.  

> would a 7573X be an option?

The 7573X would cost more than the 7373X that we would like to allow. Using a X series would atm increase the node cost by about 30-50%. The reason why we chose to stay with the 7373 is that it has the same core configuration like all other 73' models. 
It's already a lot of work and costs to build the tests and environments to get the confidence that the system is able to handle this additional diversity in the node hardware. That's why we decided to only change as much as necessary to ensure that there is a bigger choice of vendors (decentralisation) and thereby a safer supply chain.  

> I am not exactly sure why there is not more traction on this thread from other potential node-providers. This will impact their bottom lines.

A reason could be that we (DFINITY) didn't yet talk about a network growth strategy in general and in public yet. We are working on preparing community talks about all topics that are connected to the growth of the network like: NNS driven remuneration finding, node operation DAOs and other platform decentralisation related things. As some of you already know we are still working on the platform decentralisation roadmap from end of last year.

-------------------------

garym | 2022-08-16 18:02:09 UTC | #7

DFINITY is working towards the new hardware specification. To ensure these machines work at least as well as IC nodes before asking the community to vote on the HW specification, DFINITY wants to run a battery of micro and macro benchmarks with synthetic and real-world workload.

More precisely, in addition to running single-machine benchmarks, we want to evaluate these machines under as realistic circumstances as possible. Therefore, we propose to add two such machines to mainnet subnets. We will start with two low-traffic subnets, namely [lhg73](https://dashboard.internetcomputer.org/subnet/lhg73-sax6z-2zank-6oer2-575lz-zgbxx-ptudx-5korm-fy7we-kh4hl-pqe) and [shefu](https://dashboard.internetcomputer.org/subnet/shefu-t3kr5-t5q3w-mqmdq-jabyv-vyvtf-cyyey-3kmo4-toyln-emubw-4qe), where weâ€™ll add one such node respectively.

Once they reside on these subnets, we want to run benchmarking experiments against them

to assess the behavior under workloads exercising canister creation, query and update call processing with more and less memory-intensive workloads

We will compare the metrics of the new hardware and observe if we can spot any anomalies compared to the first generation HW.

When these experiments have been successful, weâ€™ll add the nodes to the following high-traffic subnets with at least one DFINITY node: [eq6en](https://dashboard.internetcomputer.org/subnet/eq6en-6jqla-fbu5s-daskr-h6hx2-376n5-iqabl-qgrng-gfqmv-n3yjr-mqe), [mpubz](https://dashboard.internetcomputer.org/subnet/mpubz-g52jc-grhjo-5oze5-qcj74-sex34-omprz-ivnsm-qvvhr-rfzpv-vae).

-------------------------

diegop | 2022-08-30 21:27:41 UTC | #8

For visibility: Gary McElroy (@garym) is a Senior Engineer at DFINITY working on hardware.

-------------------------

Sormarler | 2022-08-30 23:50:52 UTC | #9

When is the new onboarding process starting? When should people expect to be able to submit proposals to the NNS to run nodes from data centers?

-------------------------

diegop | 2022-09-01 19:10:07 UTC | #10

Hi @Sormarler m

Edit: I crossed out below because I was wrong

~~I need to verify, but my understanding is that aspiring NPs can currently use the [wiki instructions](https://wiki.internetcomputer.org/wiki/Internet_Computer_wiki)  but the hardware spec may block certain people that cannot get access to the machines. An aspiring node provider can follow the instructions to onboard themselves.~~

The work to make this much more user-friendly (e.g. by using the NNS Frontend dapp) is likely coming after SNS and hardware spec.~~

-------------------------

ritvick | 2022-08-31 23:11:29 UTC | #11

@diegop I am little confused here.
What I understood initially is that this form (https://internet-computer.typeform.com/to/IWl3iClx?typeform-source=www.dfinitycommunity.com) is closed, and was waiting for https://forum.dfinity.org/t/draft-motion-proposal-new-hardware-specification-and-remuneration-for-ic-nodes/14202/10 proposal to reach a conclusion before new nodes can be onboarded to the network.

Are you suggesting any one can follow instructions in the wiki will be able to become NP and receive rewards ?

-------------------------

Sormarler | 2022-09-01 03:27:38 UTC | #12

Same. I am a little confused as well. I was under the assumption the whole process stopped because a new method was in development.

-------------------------

diegop | 2022-09-01 20:03:07 UTC | #13

@ritvick @Sormarler i am not surprised I confused you all **since it turns out I was wrong** and team corrected me when I checked in with them. The new process is still under development and wiki instructions are still under development (they could work, but have some rough areas the team is still working on) so they are not the experience the IC should have. 

That being saidâ€¦the hardware stuff (which is the main theme of this thread) i  believe is a necessary condition so @garym will post an update on hardware tests.

Apologies for confusion I caused. This is not my area. I should have verified earlier.

-------------------------

garym | 2022-09-01 19:51:15 UTC | #14

We have executed the validation plan described previously on two ASUS machines. These meet the generic Gen 2 hardware specifications as specified earlier in this thread. Some abbreviated specs:

* 2x AMD EPYC 7313 (3,00 GHz, 16-Core, 128 MB)
* 512 GB (16x 32GB) ECC Reg ATP DDR4 3200 RAM
* 32TB (5x 6,4TB) NVMe Kioxia SSD 3D-NAND TLC U.3 (Kioxia CM6-V)
* Swiss price: 21â€™595.75 CHF

## Validation results:

* ### Low Level

  * Stress tests (using `stress-ng`) - increase confidence in the hardware configuration and these specific machine instances - Passed âœ…
  * System benchmarks (using `sysbench`) - Gauge performance against known Gen 1 node performance - Passed âœ…
    * About 2x performance increase for cpu and memory.
    * Disk performance ranges from equally good to better for the majority of tests
  * SEV-SNP capability - Verified BIOS and kernel support working in tandem - Pending ðŸš§
* ### High Level

  * Method: deploy machines into subnets and ensure subnet metrics do not deviate negatively by a meaningful threshold
  * Low usage subnet deployment. Scalability benchmarks ([system baseline](https://github.com/dfinity/ic/blob/master/scalability/experiments/run_system_baseline_experiment.py) and [large memory](https://github.com/dfinity/ic/blob/master/scalability/experiments/run_large_memory_experiment.py))
    * All metrics nominal - Passed âœ…
  * High usage subnet deployment
    * All metrics nominal, except
      * Individual node checkpointing performance discrepancy of <3-6%. 
This has no impact on subnet performance, but weâ€™re still keeping an eye on it.
    * Passed âœ…

We have updated the [node provider hardware wiki](https://wiki.internetcomputer.org/wiki/Node_provider_hardware) to include this ASUS server configuration. An example ASUS quote and bill of materials (BOM) is available for interested community members.

-------------------------

garym | 2022-09-01 19:55:44 UTC | #15

## Whatâ€™s the plan now?

We plan on continuing validation of new Gen 2 hardware configurations and publishing the results. Many factors influence how we proceed, e.g., community input on hardware configurations/manufacturers, price, availability.

## What We Are Asking The Community

Please comment on and prioritize next hardware choices (abbreviated specs):

* Dell PowerEdge
  * 2x AMD EPYC 7343 3.2GHz, 16C/32T, 128M Cache (190W) DDR4- 3200
  * 16x 32GB RDIMM, 3200MT/s, Dual Rank 16Gb (BASE x8)
  * 5x 6.4TB Enterprise NVMe Mixed Use AG Drive U.2 Gen4 with carrier
  * Swiss price: 27'159.09 CHF
  * USA price: $26,460.32 USD
* HPE Proliant
  * 2x AMD EPYC 7343 3.2GHz 16-core 190W Processor for HPE
  * 16x HPE 32GB Dual Rank x4 DDR4-3200 CAS-22-22-22 Registered Smart Memory Kit
  * 5x HPE 6.4TB NVMe Gen4 Mainstream Performance Mixed Use SFF BC U.3 Static Multi Vendor SSD
  * Swiss price: 27â€™031.83 CHF
* Lenovo
  * 2 x ThinkSystem AMD EPYC 7343 16C 190W 3.2GHz Processor
  * 16 x ThinkSystem 32GB TruDDR4 3200MHz (2Rx4 1.2V) RDIMM-A
  * 5 x ThinkSystem U.3 Kioxia CM6-V 6.4TB Mainstream NVMe PCIe4.0 x4 Hot Swap SSD
  * Swiss price: 30'534.27 CHF
  * USA price: $28,525.54 USD

Note: Prices are provided as rough examples and donâ€™t include tax. USA prices are provided for comparison - the hardware will be validated in Switzerland. Example quotes and BOMâ€™s for these hardware configurations are available on request.

Having instances of identical hardware as node providers has an additional benefit: if node providers face problems, the DFINITY engineering team can reproduce and debug independently on an identical environment. This must be done without access to node provider owned machines.

-------------------------

jleni | 2022-09-01 20:21:19 UTC | #16

Are variations like using Kioxia disks in Dell servers acceptable?
or only the specific combinations that have been validated can be used?

-------------------------

garym | 2022-09-01 21:27:07 UTC | #18

> Are variations like using Kioxia disks in Dell servers acceptable?

Yes. Some vendors may not provide components like the configurations above. 
That said, performance characteristics of alternatives should be equivalent.

-------------------------

jleni | 2022-09-01 21:40:39 UTC | #19

I have not seen any connectivity requirements. 
What are the expectations per node in this respect? and how is that compensated?

For instance, if a rack has a 10Gbps dedicated connection shared by N nodes.. what are the requirements and how are rewards calculated in that case?

-------------------------

garym | 2022-09-01 22:30:32 UTC | #20

The aim is for 10Gb connectivity per the second requirement here: https://wiki.internetcomputer.org/wiki/Node_Provider_Onboarding

Regarding node rewards: still under development.

EDIT: I'll check on per-node connectivity expectations

-------------------------

ritvick | 2022-09-01 23:50:26 UTC | #21

[quote="garym, post:15, topic:14202"]
Example quotes and BOMâ€™s for these hardware configurations are available on request.
[/quote]

How and Where do we request this ?

-------------------------

garym | 2022-09-02 20:39:33 UTC | #22

~~DM'ing me here works!~~

Evolving spreadsheet of BOM listings: https://docs.google.com/spreadsheets/d/1YSWyFf5oT1kzeLN0bctpxCzkQhQ2CVR6Jo03u-ju6cw

-------------------------

garym | 2022-09-02 21:02:03 UTC | #23

>What are the expectations per node in this respect?

Consulted with the team. We're aiming for a 10Gb connectivity per 1 or 2 racks. 
A more nuanced answer including info about rewards is coming.

-------------------------

jleni | 2022-09-02 21:24:15 UTC | #24

Thanks! This makes sense.

More information about rewards would be useful, in particular given that DC costs are likely to keep rising due to energy costs. Also if the reward multiplier goes down to 2.5/1.75 (as I read somewhere else) the economic equation changes quite a bit.

-------------------------

garym | 2022-10-15 00:56:27 UTC | #25

We have run the validation procedure on a machine from an additional vendor: Gigabyte.

Gigabyte test machine abbreviated specs:

* Dual AMD EPYC 7413 2.64 GHz, 24 Core, 180W
* 512GB (16x32GB) 3200MHz DDR4 RDIMM
* 10x7.68TB NVMe SSD (exceeds minimum specs)

## Validation Results:

* Stress tests - Passed âœ…
* System benchmarks - Passed âœ…
  * As with the ASUS, about a 2x performance increase for cpu and memory.
  * Disk performance similar or better to previous gen hardware
* SEV-SNP capability - Verified BIOS and kernel support working in tandem - Passed âœ…

We have updated the [node provider hardware wiki](https://wiki.internetcomputer.org/wiki/Node_provider_hardware) to include this configuration.

-------------------------

jleni | 2022-10-15 09:44:32 UTC | #26

Does this mean that it is valid to use 7.68TB drives ( DWPD < 3 ) in gen2?

Adding this additional flexibility would be really useful!

-------------------------

garym | 2022-11-29 00:36:12 UTC | #27

> Does this mean that it is valid to use 7.68TB drives ( DWPD < 3 ) in gen2?

~~Yes~~ No. See update: https://forum.dfinity.org/t/draft-motion-proposal-new-hardware-specification-and-remuneration-for-ic-nodes/14202/31

The intent of the 5x 6.4TB recommendation was to cover minimum storage requirements while balancing cost and reliability. Reducing the number of drives reduces probability of failure of any one drive (the disk layout does not utilize redundancy).

More 'functional' guidance and specifications for SSD's and RAM are in the works.

-------------------------

garym | 2022-10-22 00:43:23 UTC | #28

We have run the validation procedure on a machine from an additional vendor: Supermicro.

Supermicro test machine abbreviated specs:

* Dual AMD EPYC 7543 32-Core Processor 2.8 GHz, 225W
* 1024GB (16x64GB) 3200MHz DDR4 RDIMM
* 10x7.68TB NVMe SSD

Note that the configuration of each of these components exceeds the Gen2 specs.

## Validation Results:

* Stress tests - Passed âœ…
* System benchmarks - Passed âœ…
  * As with other Gen2 configurations, about a 2x performance increase for cpu and memory.
  * Disk performance similar or better to previous gen hardware.
* SEV-SNP capability - Verified BIOS and kernel support working in tandem - Passed âœ…

The `sysbench` tool used in previous validation runs gave us some trouble and odd numbers. We switched to `fio` which provided more stable and predictable performance. The validation procedure is being updated and will be run again on previous machines to maintain a fair comparison.

We have updated the [node provider hardware wiki](https://wiki.internetcomputer.org/wiki/Node_provider_hardware) to include this configuration.

-------------------------

ritvick | 2022-10-24 15:02:10 UTC | #29

When this validated hardware is expected to get final approval ( if there is a thing around this ). 
I am asking this because when is it safe to place an order for hardware like this? I have checked with vendors like dell they have a lead time of 6-8 months ( at least here in Canada).

-------------------------

ritvick | 2022-11-18 20:17:55 UTC | #30

Is there a timeline by which we can define the minimum storage and ram requirements for gen 2 hardware?

-------------------------

garym | 2022-11-29 00:48:44 UTC | #31

DFINITY **strongly** recommends using hardware that meets the exact specification for hardware components.

To simplify:

* System must be one of {Dell, Supermicro[0], Gigabyte[0], ASUS[1], HPE[1]}
* Processors must be **Dual** AMD EPYC Milan. Minimum model number: 7313 (higher end models are OK)
* RAM must be **16x 32GB** 3200 MT/s. All from the same manufacturer. Donâ€™t mix and match.
* SSDâ€™s must be **5x 6.4TB** mixed mode NVMe (DWPD >=3). All from the same manufacturer. Donâ€™t mix and match.
* Must have dual port 10G SFP or BASE-T
* Must have TPM 2.0

Choice of manufacturer of SSD or RAM can be decided by the NP. These choices may be constrained by the vendor.

**Deviate from this specification at your own risk.** DFINITY does not provide hardware support or troubleshooting. Deviating from the spec will cause issues during installation of node software and risks losing NP rewards.

You may have noticed the previous validated configurations for Supermicro and Gigabyte had deviated from the specification by either doubling the size of each RAM stick or using 7.68TB SSDâ€™s (DWPD ~=1). We donâ€™t recommend doing this. Sorry if that caused confusion.

[0] We will re-run validation on the Gigabyte and Supermicro with components which meet the exact specification.

[1] SEV-SNP validation is still pending for the ASUS configuration. Complete validation is still pending for HPE.

-------------------------

ritvick | 2022-11-29 01:15:34 UTC | #32

@garym Is it worth adding minimum read and write sequential speeds, random read and write IOPS and latency requirements for SSDs?

-------------------------

garym | 2022-11-29 01:15:43 UTC | #33

We have run the validation procedure on a machine from an additional vendor: Dell

Dell test machine abbreviated specs:

* Dual AMD EPYC 7343
* 16x 32GB RDIMMs, 3200 MT/s Dual Rank
* 5x 6.4TB Mixed Mode NVMe SSDs

Validation Results:

* Stress tests - Passed âœ…
* System benchmarks - Passed âœ…
* SEV-SNP capability - Verified BIOS and kernel support working in tandem - Passed âœ…

We have updated the [node provider hardware wiki](https://wiki.internetcomputer.org/wiki/Node_provider_hardware) to include this configuration.

-------------------------

garym | 2022-11-29 01:18:19 UTC | #34

@ritvick Great question - I think it is worth it to clarify what's acceptable. We're resolving discussions internally before posting such guidance.

-------------------------

garym | 2022-12-01 20:45:58 UTC | #35

[quote="garym, post:14, topic:14202"]
* * SEV-SNP capability - Verified BIOS and kernel support working in tandem - Pending :construction:
[/quote]

Update on ASUS hardware: SEV-SNP validation has succeeded! The ASUS configuration is now fully validated to run on the IC. 

We had some interaction with our suppliers and ASUS. The issues we saw were solved with the [latest version of the BIOS](https://servers.asus.com/products/servers/rack-servers/rs700a-e11-rs12u/#Resources), released recently. 

Up next: validating HPE, and re-validating Gigabyte and Supermicro.

-------------------------

Hugo | 2023-03-29 03:00:10 UTC | #36

We wanted to become a node provider, so there were asking vendors for information about hardware. They said there is no way to do RAID because it is SSD storage, will this be a problem?

-------------------------

garym | 2023-03-29 04:04:43 UTC | #37

> They said there is no way to do RAID because it is SSD storage, will this be a problem?

No, not a problem. Hardware RAID should not be attempted. The IC-OS installer will verify there are 5x independent 6.4TB NVMe SSD's and prepare them appropriately. IC-OS uses a 'striped' LVM volume across all the disks (technically a software RAID 0).

What about redundancy? Replica nodes currently provide it at a higher level than disk redundancy.

-------------------------

ritvick | 2023-04-11 14:21:34 UTC | #38

I have a question regarding the 3DWPD (3D Write Per Day) drives with varying capacities. Specifically, I wanted to know if it is **necessary to use a 6.4TB capacity SSD**, or if other capacities are acceptable.

I understand that it may not always be possible to obtain an exact 6.4TB capacity SSD, as they may not be readily available in stock. Hence, I am wondering if alternative options such as 10x3.2TB or 5x7.6TB drives from reputable brands like Micron or Kioxia can be used.

-------------------------

andrewbattat | 2023-04-12 17:40:49 UTC | #39

As Gary stated before "**Deviate from this specification at your own risk.** DFINITY does not provide hardware support or troubleshooting. Deviating from the spec will cause issues during installation of node software and risks losing NP rewards."

And we've only validated hardware for the exact specifications discussed above.

That being said, the current requirement is not that every SSD must be exactly 6.4TB, but rather that each SSD must be *at least* 6.4TB, and the total storage capacity must be *at least* 32TB. So 5x7.6TB drives should pass the hardware requirements, but we encourage everyone to use the exact specs or risk causing issues during the installation of node software and risks losing NP rewards.

-------------------------

Shuai | 2023-04-19 08:26:54 UTC | #40

@garym Hi. If NP configures the machine model by themselves, can they conduct testing by themselves? Will the Dfinity team assist in testing? Thanks.

-------------------------

garym | 2023-04-26 00:53:09 UTC | #41

>If NP configures the machine model by themselves, can they conduct testing by themselves? 

For performance validation, yes! The validation scripts are [available on the public GitHub IC project](https://github.com/dfinity/ic/tree/master/hw_validation). It's on our backlog to have it export in CSV for easier comparison but you can run the scripts right now. DM me once you have the results and we can compare numbers. 
Our performance validation results should be made public but they're unorganized at the moment and we have higher priority items in our backlog.

-------------------------

garym | 2023-04-26 01:01:15 UTC | #42

Currently the 5x 6.4TB SSD requirement should be considered a requirement. Please stick with 3 DWPD SSD's. There are some upcoming features which will increase disk usage. This may cause 1 DWPD SSD's (7.6TB, e.g.) to have durability failures before the warranty period is over. 
If there are issues with supply please let us know.

-------------------------

Shuai | 2023-04-27 10:59:28 UTC | #43

Thank you very much. I have another question. 

In the validated configuration, specific chassis and model configurations have already been specified. Can I choose similar chassis or configurations? For example, can I switch from ASUS RA2112 to a similar RS720A chassis? For Kioxia's NVMe SSD, is it only necessary to meet the requirements of 6.4 TB, 3 DWPD and U.3, without being exact to the specific model of CM6-V, for example, Kaixia CM7? 

Does passing the performance test mean that the machine can run IC programs and operate normally on the subnet? We want to chose ASUS. And why doesn't ASUS need a TPM module? I can't find the `1U AMD dual-CPU RA2112-ASEP server`. Can I use another common ASUS chassis as a substitute?

-------------------------

garym | 2023-04-27 19:34:46 UTC | #44

>Can I choose similar chassis or configurations?

Yes! What's important are the [requirements specified above](https://forum.dfinity.org/t/draft-motion-proposal-new-hardware-specification-and-remuneration-for-ic-nodes/14202/31). 
DFINITY has validated a few configurations but we cannot cover all manufacturers and all components. It's expected that new configurations will be used going forward.

>Does passing the performance test mean that the machine can run IC programs and operate normally on the subnet?

Yes. The [validation suite](https://github.com/dfinity/ic/tree/master/hw_validation) is designed to vet the hardware appropriately. We're working on making this easier to use and to contribute results to one place.

> And why doesnâ€™t ASUS need a TPM module?

It does require a TPM 2.0 module. Looks like I didn't list that on the ASUS configuration in the wiki. Will fix. Thanks for bringing it up!

> I canâ€™t find the `1U AMD dual-CPU RA2112-ASEP server` . Can I use another common ASUS chassis as a substitute?

Yes. As long as the requirements listed above are met, you should be good.

-------------------------

Shuai | 2023-04-28 02:23:17 UTC | #45

Thank you very much for your patient guidance. :+1: :clap:

-------------------------

Shuai | 2023-05-05 08:31:49 UTC | #46

Hi. Help.

Does anyone try NVMe with U.2? Does its IO read and write speed meet the requirements?

ASUS RS720A-E11 servers integrate PFR FPGA as the platform Root-of-Trust solution for firmware resiliency to prevent from hackers from gaining access to infrastructure. Do we still need to configure a separate TPM? Or use the built-in FPGA?

-------------------------

garym | 2023-05-05 17:03:18 UTC | #47

Hello Shuai!

> Does anyone try NVMe with U.2? Does its IO read and write speed meet the requirements?

Yes, U.2 meets the performance requirements. The [Dell that was validated by DFINITY](https://docs.google.com/spreadsheets/d/1YSWyFf5oT1kzeLN0bctpxCzkQhQ2CVR6Jo03u-ju6cw/edit#gid=213072892) used U.2 NVMe drives.

>Do we still need to configure a separate TPM? 

No. There's a requirement for the TPM 2.0 in the [generic hardware specs](https://wiki.internetcomputer.org/wiki/Node_Machine_Hardware#Gen_2_Node_Machine) for future use by IC-OS, but it's not used yet. No extra configuration is currently necessary.

-------------------------

Zboi04 | 2023-05-05 18:33:32 UTC | #48

Can Node Providers use AMD Milan models with 24C or 32C?

-------------------------

Zboi04 | 2023-05-05 18:35:35 UTC | #49

Furthermore, do the CPUs have a to have a base clock speed of 3Ghz?

-------------------------

garym | 2023-05-08 20:40:19 UTC | #51

> Can Node Providers use AMD Milan models with 24C or 32C?

Yes. Note that choosing a higher CPU model can increase power requirements on the power supplies. Please consider this and adjust appropriately. See [Wikichip.com](https://en.wikichip.org/wiki/amd/epyc#7003_Series_.28Zen_3.29) for reference. 

> Furthermore, do the CPUs have a to have a base clock speed of 3Ghz?

No. They just need to be 2x EPYC Milan CPU's. I have clarified the wiki to reflect this.

-------------------------

andrewbattat | 2023-07-31 22:09:22 UTC | #53

We have finalized validation on the **Gigabyte** for exact Gen2 specs :tada:

Gigabyte test machine abbreviated specs:

* AMD EPYC 7313 (3,00 GHz, 16-Core, 128 MB)
* 512 GB (16x 32GB) ECC Reg DDR4 3200 RAM
* 5x 6.4 TB NVMe TLC SSD, PCIe 4.0 x4, U.3 2.5", 3 DWPD

Validation Results:

* Stress tests - Passed :white_check_mark:
* System benchmarks - Passed :white_check_mark:
* SEV-SNP capability - Verified BIOS and kernel support working in tandem - Passed :white_check_mark:

We have updated the [node provider hardware wiki ](https://wiki.internetcomputer.org/wiki/Node_Provider_Machine_Hardware_Guide#Gigabyte) to include this configuration.

-------------------------

frezabek | 2023-08-02 15:44:14 UTC | #54

Hello,
We are new to Dfinity and are considering to run a node. When looking at the specs, we got a few questions:

1. What is the motivation to run a 2xCPU instead of 1xCPU with a comparable amount of CPU cores and possibly higher clock speeds? Is due to more amount of RAM slots the CPUs combined can handle?
2. Has anyone experienced running a 1xCPU setup instead of dual CPU setup?
3. When it comes to SSD, you require 5x6.4TB of storage, but would 3x12.8 storage work, too? Was wondering if the target is to achieve a higher throughput with the 5x setup or not.

Thank you for any comments/answers and looking forward to dig deeper to the setup!

-------------------------

garym | 2023-08-02 21:09:13 UTC | #55

>What is the motivation to run a 2xCPU instead of 1xCPU with a comparable amount of CPU cores and possibly higher clock speeds?

I wasn't there for the original design of specs, but dual CPU systems can use more memory lanes and therefore have higher memory throughput.

> Has anyone experienced running a 1xCPU setup instead of dual CPU setup?

No. SetupOS will fail if it doesn't detect 2x (EPYC Milan) CPU's

> When it comes to SSD, you require 5x6.4TB of storage, but would 3x12.8 storage work, too? Was wondering if the target is to achieve a higher throughput with the 5x setup or not.

This can be a complex discussion :slight_smile: 
The goal was a balance of speed, reliability, and cost. The nodes use LVM (effectively software RAID 0) striped over each SSD so writes are distributed. Larger SSD's might give higher throughput per SSD, but this striping allows for parallel writing to each SSD so I/O to the 5x 6.4TB SSD's is very fast. 
Larger SSD's are usually disproportionately more expensive.

Node specs (Gen3?) may evolve in the long run. For now the specifications above should be followed rigidly.

-------------------------

frezabek | 2023-08-03 16:14:00 UTC | #56

Thanks for the explanation. Will then stick with the recommended setup.

-------------------------

mmmzzz | 2023-09-04 14:39:40 UTC | #57

Is TPM 2.0 necessary? What is the main function of this?

-------------------------

garym | 2023-09-05 17:36:57 UTC | #58

> Is TPM 2.0 necessary? What is the main function of this?

Yes. It will support upcoming security features.

-------------------------

andrewbattat | 2023-09-05 20:27:20 UTC | #59

The latest Global R&D recapped the Gen 2 hardware validation status. Here is a summary of the validated hardware:

![Pasted Graphic|690x383](upload://9isd3ytl8v1DwW66QJDWk9AxjFe.jpeg)


See the wiki [Node Provider Machine Hardware Guide](https://wiki.internetcomputer.org/wiki/Node_Provider_Machine_Hardware_Guide) for details on the validated machine configurations

-------------------------

Volgan11 | 2023-10-18 14:52:27 UTC | #60

Hello everyone what is currently the most profitable equipment to buy in terms of reliability and price? If you have purchased equipment recently, please share your experience.

-------------------------

mrpablo | 2023-10-20 09:20:10 UTC | #62

@andrewbattat Hello, you've mentioned the server models for Dell. Could you please inform me about the server models you used for Supermicro and Asus?

-------------------------

andrewbattat | 2023-10-20 17:14:31 UTC | #63

The specs for all the validated servers can be found on the [Node Provider Hardware Guide wiki page](https://wiki.internetcomputer.org/wiki/Node_Provider_Machine_Hardware_Guide)

-------------------------

Rick | 2024-05-09 01:01:12 UTC | #64

How close are we to Gen 3 specs coming out because I'm about to spend a lot of money for a Gen 2 spec'd machine and I don't want to be outdated in a year from now. TIA

-------------------------

ZackDS | 2024-05-09 09:47:37 UTC | #65

Hey Rick that is a very good question that I and probably others are looking to get an answer for. I suggest you join us in the https://forum.dfinity.org/t/technical-working-group-node-providers/30255 and ask any questions there. 
Hope to talk about the Next-Gen AMD Epyc CPUs and the extension off current storage in order to see the New storage layer increased to 2 TB.

-------------------------

