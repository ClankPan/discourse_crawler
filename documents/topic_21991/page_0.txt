icpp | 2023-08-07 20:07:14 UTC | #1

Those of you who are into conversational AI are probably aware of this model that has taken the open source world by storm: [karpathy/llama2.c](https://github.com/karpathy/llama2.c)

Using [icpp-pro](https://docs.icpp.world/), I am able run this LLM in a canister and do inference:

```bash
dfx canister call llama2 inference '(record {"prompt" = "" : text; "steps" = 20 : nat64; "temperature" = 0.9 : float32;})'
(
  variant {
    ok = "Once upon a time, there was a little boat named Bob. Bob loved to float on the water"
  },
)
```

I created this [video](https://www.loom.com/share/a065b678df63462fb2f637d1b550b5d2?sid=bdffa0d8-17d8-4815-a5ba-1ee26d922dc2) that shows the full process of build/deploy/upload/test.

You can find the code in [icppWorld/icpp-llm](https://github.com/icppWorld/icpp-llm)

I believe this is a foundational step in bringing Conversational AI to the IC. Once the infrastructure scales and some limitations are removed, we will be able to scale this up to larger & larger AI models running directly on the IC, without the need for doing the inference on another cloud.

-------------------------

ildefons | 2023-08-07 21:18:01 UTC | #2

What is the average inference time? If it takes longer than the consensus period, what happens?

-------------------------

icpp | 2023-08-08 02:41:14 UTC | #3

@ildefons ,
Right now, the inference call is a query call, so it does not need to go through consensus.

It is very fast to get the response back from this tiny LLM. I plan to activate the timer, and I can give you the tokens/sec.

Good question though about what would happen if we switch to an update call.  I can see this being needed in case you want to save the inference results and also chat histories similar to what ChatGPT does.  For this tiny LLM there will be no issue,  but I wonder what will happen once the Orthogonal Persistence memory becomes larger and we can deploy really big models.

-------------------------

yangzijiang | 2023-08-08 04:48:15 UTC | #4

That's awesome, a feat that could empower ic if it came to fruition

-------------------------

MillionMiles | 2023-08-08 05:14:47 UTC | #5

Very nice! Can't wait to see more your progress :+1: :+1:

-------------------------

bob11 | 2023-08-08 16:04:01 UTC | #6

This is VERY cool. Can we chat about it? You can DM me @BobBodily on Twitter, Discord, or Telegram. Would love to learn more.

-------------------------

icpp | 2023-08-11 19:59:36 UTC | #7

# Try it

Deployment to main-net went smooth.

`icpp_llama2` with the stories15M.bin is now running on-chain in canister `4c4bn-daaaa-aaaag-abvcq-cai`.

You can call it's inference endpoint with:

```
dfx canister call --network ic 4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = "" : text; steps = 20 : nat64; temperature
 = 0.8 : float32; topp = 1.0 : float32;})'
(
  variant {
    ok = "Once upon a time, there was a little boat named Bob. Bob loved to float on the water"
  },
)
```

If you play with the parameters, you will quickly run into the instructions limit. That is an area of investigation right now.

-------------------------

icpp | 2023-08-30 01:47:50 UTC | #8

I like to use this thread to share the high level roadmap I have in mind for icpp-llm, and keep you posted on the progress each time I reach a milestone or encounter a blocker.

Milestone 1: remove the limitations of the current tinystories canister, so it can generate stories longer than 20 words.
This means I need to find a way to work around the max instructions per message limit.

Milestone 2: run inference with memory and matrix calculations distributed across multiple canisters.
For this, I plan to use an HPC type appoach, kind of treating the IC as a massively parallel compute cluster. 

Milestone 3: Run inference with the [llama2_7b_chat](https://github.com/karpathy/llama2.c#metas-llama-2-models) model. Not worrying about speed, just the ability to load it and talk to the LLM.

Milestone 4: Optimize and scale.

This is going be a fun challenge.

-------------------------

icpp | 2023-09-06 10:19:07 UTC | #10

**COMPLETED Milestone 1**: remove the limitations of the current tinystories canister, so it can generate stories longer than 20 words.

I fixed it by:
- Switching from using a single query call to a sequence of update calls
- Save the LLM's runState in orthogonal persistence
- Implement a new endpoint `new_chat`, to reset the runState when you're starting a new chat.

This screenshot shows how the call sequence works with dfx:
![icpp_llama2_without_limits|690x342](upload://IIFn7NipZzNaykm7XAlJAPtHfa.png)
- You start with a call to the `new_chat` endpoint
- You then generate a starting prompt by a sequence of update calls. 
- You then ask the LLM to write the rest of the story, using more update calls.

So, for this small LLM:
-  there is NO limit anymore on the length of the prompt or the chat. 
   *(It is only limited by the seq_length used during the model training, but that value is typically huge.)*
- The 10 token increments come back in ~3 seconds in my local network.
- The remaining limits are the size of the transformer we can store in a canister, and as we use bigger transformers, the inference time. 


NOTE: I did not yet update the canister on the main-net, because I first need to build in a multi-user approach for the inference endpoint. That is to  be done.

-------------------------

domwoe | 2023-09-06 11:26:33 UTC | #11

Thanks for sharing the progress! Looking forward to trying on mainnet!

-------------------------

icpp | 2023-09-18 13:30:36 UTC | #12

Multi-user is now supported by [icpp-llm](https://github.com/icppWorld/icpp-llm) and it is again deployed to main-net canister  4c4bn-daaaa-aaaag-abvcq-cai !

It is deployed to main-net, and to see it in action, check out this short [loom video](https://www.loom.com/share/2772c23399814ce09545daa937e40a99?sid=b3e8229f-99e6-427c-b402-40fd05cfecef), where I run two concurrent shell scripts, and two stories are being generated by the LLM for two different principals. 

If you want to try it out yourself, you have to still use `dfx`, because I did not yet build a frontend:
(Note, on Windows, use `wsl --% dfx ....`)

```bash
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai new_chat '()'
# You can build the prompt in multiple calls
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = "Lilly went to"           : text; steps = 0  : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = "the beach this morning." : text; steps = 0  : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = "She saw a little boat"   : text; steps = 0  : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = "with her friend Billy"   : text; steps = 0  : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
# Followed by building out the story
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call  --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = ""                        : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  4c4bn-daaaa-aaaag-abvcq-cai inference '(record {prompt = "" 
```

@domwoe has pointed out that doing client side orchestration like this is good & flexible, but there are benefits of moving the orchestration calls on-chain as well, for speed & cost reasons. We're going to look into that, but I wanted to push this new capability out asap.

-------------------------

domwoe | 2023-09-08 08:52:46 UTC | #13

Hey @icpp,

how do you handle randomness at the moment? Is the same prompt giving the same answer or is icpp taking care of this and uses the randomness API of the Internet Computer?

-------------------------

icpp | 2023-09-08 11:27:35 UTC | #14

That is a great question! The way randomness is introduced right now, and this is just the original code from llama2.c, is by starting with a random seed, based on the time in ns. For that I am using the canisters system api for time.

Then a pseudo random number is generated from this, by using some bit twiddling. I did not study that aspect much yet.

I will check out the IC randomness API. That sounds like a really interesting improvement over the current approach.


...btw... the LLM has two settings. If you set the `temperature` parameter to zero, you will get the same story each time. If you put it to non-zero, the randomness will be introduced as explained above, and the randomness is used by the `sampler` who picks the next token from the list of most probable tokens predicted by the LLM.

-------------------------

icpp | 2023-09-25 23:08:59 UTC | #15

Unfortunately I had to make the icpp-llm repository private for now.

The hosted version of the on-chain LLM is still active and will continue to improve rapidly.

UPDATE on Sep 25, 2023:
- The repo is public again, although renamed: [icpp_llm](https://github.com/icppWorld/icpp_llm)

-------------------------

icpp | 2023-09-19 18:28:26 UTC | #16

Hi All,

Some of you reached out asking why I made icpp-llm repository private.

The main reason is that I wanted to bring some sanity to the topic of LLMs running in canisters of the IC.

One has to understand that it is a very interesting and promising **field of research**. But that really is what the state-of-the-art is. **It is R&D**, and a lot of hard work still has to be done to make it a reality.

To enable sharing of my research in LLMs running in canisters, I created a dApp that allows collaborators and partners to exercise the available & deployed LLMs.

The name of my dApp is **ICGPT Labs**.
- ICGPT = **I**nternet **C**omputer **G**enerative **P**re-trained **T**ransformers
- Labs = it is research!  We are making amazing strides, but the models today are still small

The Beta Preview of the frontend has now been deployed. It is still a little rough around the edges, but I think it is already very cool to see a live token stream coming back from the LLM canister.

Feel free to try it out and give me feedback. You must login with your Internet Identity, and then you can build stories. **All real, all live on main-net !**.

You can find here: https://icgpt.icpp.world/


A few screenshots to wet your appetite:

**The login screen** :

![image|600x324](upload://hvNAM3csW50zUzXkxvIpNAQTD3O.png)

---

**The New Chat page** allowing you to select the LLM you want to try and an input area to provide the prompt. Right now, we only have one LLM deployed, the Tiny Stories model with 15M parameters which has been the workhorse of my R&D so far:

![image|525x500](upload://iMGCNokUnT9KTn8Hr771575zwpm.png)


---

Once you click Submit, the frontend connects to the selected LLM and starts the inference.

![image|525x500](upload://8R1c3akATNKu5qIctl8igPXpbBM.png)


---
**The tokens stream in progress!** 

![image|525x500](upload://nZv3VyH9dFmKmzW2rLMyA7XQvb1.png)


Once the initial part of the story has been build, you can continue to build it out, or start over.

-------------------------

icpp | 2023-09-19 18:29:34 UTC | #17

As I mentioned, it is **Beta Preview...**:
- I will update the dApp without notifications. If it is not active, please try again after some time

If you want to provide feedback, please do it in this thread, or in the C++ community on OpenChat.

-------------------------

ajismyid | 2023-09-18 01:55:37 UTC | #18

![image|386x500](upload://adVymPwONOF1qcPYagNMPDW5lfT.png)
:heart_eyes:

-------------------------

icpp | 2023-09-19 18:19:14 UTC | #20

I updated the styling for [ICGPT Labs ](https://icgpt.icpp.world/) and it has a custom domain.

It now works a lot smoother on mobile, and the logic to create longer stories in multiple steps is also fixed.

Unless I get requests for further styling improvements, I am going to focus next on building out the LLMs, in this order:

1. **Tiny Stories, 15M, fine tuned for Chat**. That will allow you to describe to the LLM what kind of story you want, and is a more natural experience.
2. **Tiny Stories, 42M & 110M, fine tuned for Chat**. See if I can get these larger models on a canister. The 15M model already produces pretty decent stories, but these larger models do a much better job at producing comprehensive stories with proper grammar and a sensible plot.
3. **Llama2, 7B, fine tuned for Chat**. This will be the milestone we need to reach, for real applications. It will be a huge challenge, but I am very encouraged with the results so far.

I had a great zoom call today with @LowFreeKey , to brainstorm how a canister based LLM could fit in [kawak](https://3ysab-rqaaa-aaaan-qaewq-cai.ic0.app/#/) and it is felt that once we reach the Llama2-7B level and response time is reasonable, it would have potential. The kawak dApp is focused on privacy and having to send data out to an off-chain LLM at eg. OpenAI would break that.

-------------------------

qwertytrewq | 2023-09-25 18:08:22 UTC | #21

Who will pay for the traffic?

If you generously present LLMs for free, where do you obtain funds?

I started to think using Llama 2 to check my upcoming social network posts for spam, like

```
Is the following a spam (answer "yes" or "no")?

XXX
```

and then check `response.lowerCase().startsWith('yes')` (pseudocode).

So, I am going to call Llama 2 on _every_ of my posts. That may amount to a significant amount of funds to be paid for cycles. Is it OK for you?

-------------------------

icpp | 2023-09-25 20:01:38 UTC | #22

@qwertytrewq ,
thank you for sharing that use case.

Let me answer it with a couple of question :slightly_smiling_face:

- Would you be willing to pay such a service?
- If no, would you be interested in an ad supported offering?
- If yes, would you prefer fixed subscription fee or usage based?

-------------------------

qwertytrewq | 2023-09-25 20:14:52 UTC | #23

Would you be willing to pay such a service?
Yes, but only if the price is minor.

If no, would you be interested in an ad supported offering?
It is usage through API. I don't understand where the ads can be put in this relationship.

If yes, would you prefer fixed subscription fee or usage based?
Usage-based is more fair for both parties.

I also can deploy my own canister, but I doubt that it has a value to duplicate functionality.

BTW, what are obstacles due which Llama 2 is not yet deployed, but only TinyStories are deployed?

-------------------------

icpp | 2023-09-25 23:04:18 UTC | #24

Thank you for those answers!

Sending ads through an API is indeed not very easy to do. It was a bit of a dumb question :slightly_smiling_face:

Usage based indeed would make most sense.

The main obstacles to deploying the meta-llama2 models are available resources in a canister, and a limit on the number of instructions per message. 

I am working on the llama2-7B model, and hope to get that deployed reasonably soon, so we can run some more experiments. 

Definitely interested in digging deeper into enabling your example use case.

-------------------------

icpp | 2023-09-25 23:09:17 UTC | #25

FYI.

The repo is public again, although renamed: [icpp_llm](https://github.com/icppWorld/icpp_llm)

-------------------------

Gamris | 2023-09-27 02:17:50 UTC | #26

Great work on this!

Llama2-7B is definitely a milestone goal for canister LLMs. But in terms of "usable" LLMs for applications (ex. NPCs), there are more compact LLMs that are useful enough for generative text/conversational tasks.

[TinyLlama 1.1B project](https://github.com/jzhang38/TinyLlama) follows Llama2's architecture and tokenizer formats (easier for fine-tuning/Q-LoRA) and is being trained for 3 Trillion tokens with all the speed optimizations and performance hacks for inferencing. Using the Llama.cpp framework, a Mac M2 16GB RAM generates 71.8 tokens/second. That's really fast for non-GPU inferencing.

Due to its performance for its parameter size, I'd imagine this is to be the model to test canister LLMs.

-------------------------

icpp | 2023-09-27 10:13:24 UTC | #27

Hi @Gamris ,

Thank you for pointing out the TinyLlama reference. I will definitely research it potential and perhaps port it into the dApp as an alternative.

How do you think the [karpathy/llama2.c](https://github.com/karpathy/llama2.c) LLM that I am using in the canister compares to that one?

It is also based on the identical Llama2 architecture & tokenizer.

-------------------------

icpp | 2023-09-28 02:25:20 UTC | #28

The `110M` llama2 model with TinyStories data set is up & running on main-net !!

This is quite a jump from the previous 15M model.

The performance is still remarkably good. Each call comes back after ~10 seconds. 
Try it out:

```bash
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" new_chat '()'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
dfx canister call --network ic  "obk3p-xiaaa-aaaag-ab2oa-cai" inference '(record {prompt = "" : text; steps = 10 : nat64; temperature = 0.0 : float32; topp = 0.9 : float32; rng_seed = 0 : nat64;})'
```

This size LLM generates a very coherent story and doesn't suffer from some of the deficiencies of the 15M size model:

```bash
--------------------------------------------------
Generate a new story using llama2_110M, 10 tokens at a time, starting with an empty prompt.
(variant { ok = 200 : nat16 })
(variant { ok = "Once upon a time, there was a little girl" })
(variant { ok = " named Lily. She loved to play outside in the" })
(variant { ok = " sunshine. One day, she saw a big" })
(variant { ok = ", red apple on a tree. She wanted to eat" })
(variant { ok = " it, but it was too high up.\nL" })
(variant { ok = "ily asked her friend, a little bird, \"Can" })
(variant { ok = " you help me get the apple?\"\nThe bird said" })
(variant { ok = ", \"Sure, I can fly up and get" })
(variant { ok = " it for you.\"\nThe bird flew up to" })
(variant { ok = " the apple and pecked it off the tree." })
(variant { ok = " Lily was so happy and took a big bite" })
(variant { ok = ". But then, she saw a bug on the apple" })
(variant { ok = ". She didn\'t like bugs, so she threw" })
(variant { ok = " the apple away.\nLater that day, L" })
(variant { ok = "ily\'s mom asked her to help with the la" })
(variant { ok = "undry. Lily saw a shirt that was" })
(variant { ok = " too big for her. She asked her mom, \"" })
(variant { ok = "Can you make it fit me?\"\nHer mom said" })
(variant { ok = ", \"Yes, I can make it fit you.\"" })
(variant { ok = "\nLily was happy that her shirt would fit" })
(variant { ok = " her. She learned that sometimes things don\'t fit" })
(variant { ok = ", but there is always a way to make them fit" })
(variant { ok = "." })
(variant { ok = "" })
```

Will expose it in the front end shortly, after more testing. Something is not working yet when you use a non-empty prompt.

-------------------------

cyberowl | 2023-09-28 07:19:02 UTC | #29

You are making good progress.

-------------------------

icpp | 2023-09-29 19:56:37 UTC | #30

The frontend at https://icgpt.icpp.world now uses the 42M model as the default:

![image|477x295](upload://ULpVpxtfNjqcYZd2lTb2IVaYjw.png)

This size model is a lot better than the 15M model, and it determines by itself when it has completed a story. For example, the prompt `Bobby wanted to catch a big fish` results in a generative story that ends itself:

![image|444x336](upload://kHEuHKG83h3Z1sHCOD4NHkWIKc5.png)

Response time of this model is just as good almost as the 15M model, and the words stream onto the screen very naturally.

However, even though the stories are a lot better than the 15M model, this 42M model still throws some nonsensical stuff in there, as in this example. The prompt `Bobby was playing a card game with his friend` results in a pretty good story, but some of the comprehension is not right and there is a repeat in the middle. This should go away with the 110M model :crossed_fingers:
![image|561x436](upload://v8omfd3wFjZGPLwazmafuOhvMcq.png)

-------------------------

icpp | 2023-09-29 21:21:54 UTC | #31

A real fun one to use is the prompt:
`Billy had a goat.`

-------------------------

Artemi5 | 2023-09-30 06:00:14 UTC | #32


Did you just put a LLM inside a canister :exploding_head:

-------------------------

icpp | 2023-09-30 10:13:55 UTC | #33

@Artemi5 ,
yes, the canister is very capable in running an LLM !
The goal is to scale it up more & more now.

-------------------------

evanmcfarland | 2023-10-02 15:40:23 UTC | #34

This is really impressive stuff, especially the streaming effect. I had no idea that was possible. Thanks for open-sourcing this!

Have you done any cost analysis, e.g., cost/token in-canister?

-------------------------

hope888 | 2023-10-02 22:15:23 UTC | #35

Is there a simple way to donate cycles to the canister?thank you!

-------------------------

paulous | 2023-10-02 23:15:41 UTC | #36

Curious to know if anyone has used the new Mistral 7B open-source offering?

https://mistral.ai/news/announcing-mistral-7b/

thanks!

-------------------------

Severin | 2023-10-03 06:39:29 UTC | #37

[quote="hope888, post:35, topic:21991"]
Is there a simple way to donate cycles to the canister?
[/quote]

Here's a few ways to do it: https://wiki.internetcomputer.org/wiki/Topping_up_canisters

-------------------------

hope888 | 2023-10-03 11:26:40 UTC | #38

thank you very much :kissing_heart:

-------------------------

hope888 | 2023-10-03 11:40:57 UTC | #39

*obk3p-xiaaa-aaaag-ab2oa-cai*  is  the Canister ID ?

-------------------------

hope888 | 2023-10-03 11:49:55 UTC | #40

![Screenshot_20231003_194658|227x500](upload://8ML5WSkIXRWXfw7rxGDJx64W0uz.jpeg)
![Screenshot_20231003_194553|227x500](upload://cnkn7CSFLRPvipbfMoLzPQM9pvO.jpeg)

There was an error :sweat_smile:

-------------------------

Severin | 2023-10-03 12:08:46 UTC | #41

Nono, it's all fine. The error means that you are not a controller, but you can still use 'Add Cycles'. I added a note to the guide that this is fine

-------------------------

hope888 | 2023-10-03 12:59:07 UTC | #42

You are right，after closing the pop -up window, the button *Add cycles*  still there.Thanks for your help
![Screenshot_20231003_205403|227x500](upload://vxefbiGzKhBjJPt5Ux6hcfptgUv.jpeg)
!
![Screenshot_20231003_205328|227x500](upload://ciIYmELKIkoS23S8pGTGPhImAGJ.jpeg)

-------------------------

icpp | 2023-10-04 15:35:48 UTC | #43

@hope888 ,
THANK YOU!!

-------------------------

icpp | 2023-10-04 23:06:07 UTC | #44

@evanmcfarland 
I have not done a cost/token comparison yet. I plan to do that though.

-------------------------

icpp | 2023-10-04 23:18:38 UTC | #45

@paulous ,
Thanks for that reference. It looks really interesting, but I am afraid porting it to the IC will be a big task. I think it will require to run pytorch in a canister and that is still far away. 

It will be interesting to see though if some of their ideas will make it into the pure C/C++ LLMs we can run in a canister today

-------------------------

paulous | 2023-10-04 23:28:09 UTC | #46

no worries, just saw it on a blog and looked interesting so thought I would pass it by the experts. Thanks for the feedback.

-------------------------

Mercury | 2023-10-06 04:42:50 UTC | #47

https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF

Btw, TheBloke is funded by a16z, and bakes some of the best GGUF(GGML) out there

-------------------------

patnorris | 2023-10-21 19:10:33 UTC | #48

Hi @paulous , if you like you can give Mistral 7B a try here: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io 
Note that the model is not running on-chain but is downloaded onto your device and runs there (you thus need a laptop with 16GB RAM to run it smoothly). To use Mistral 7B, you can log in and select it under User Settings. Then, on the DeVinci AI Assistant tab, click on Initialize to download it (will take a moment on first download). Once downloaded, you can chat with it. Please let me know if you have any feedback :) Cheers

-------------------------

icpp | 2023-10-22 02:20:18 UTC | #49

@patnorris 
This is very cool.

I am a big proponent of solutions like this, in browser or in canister, to avoid having to call out to web2.

I am planning to buy a big machine now🙂

-------------------------

ZackDS | 2023-10-22 08:08:33 UTC | #50

Any specific specs you looking at ? for the big machine I mean .

-------------------------

patnorris | 2023-10-22 10:14:32 UTC | #51

@icpp thank you and agreed, there should be good alternatives to the common Web2 players. And I'm looking forward to the ones we'll build :muscle:

Yes, running the LLM in browser is currently limited by which device the user has. Having it in a canister would be more accessible. I could see great hybrid solutions as well :slight_smile: 

Same here. The high-end machines can even run Llama2 70B. So if you go for one of those, let me know and I'll add the 70B version for you to try (same if anyone else wants to give that a shot, just let me know).

-------------------------

icpp | 2023-10-23 01:02:11 UTC | #52

@patnorris ,
How much RAM is needed for the 70B model?
Any other specs to keep in mind?

-------------------------

patnorris | 2023-10-23 08:13:36 UTC | #53

@icpp I'm reading that the 70B model requires 64GB RAM. I haven't found any exact specs for the GPU but as far as I understand it's needed to run the model smoothly (even though CPU only might work), so best to have that.

-------------------------

qwertytrewq | 2023-11-15 17:33:44 UTC | #54

@icpp At https://icgpt.icpp.world/ Llama2 isn't available, only TinyStories.

Is it already possible to compile (and self-host) Llama2 using https://github.com/icppWorld/icpp_llm or is it not yet ready, too?

-------------------------

icpp | 2023-11-15 20:26:03 UTC | #55

Hi @qwertytrewq ,
you can indeed self host using the icpp_llm repo. 

That is actually the code for Llama2:
- It is the backend of ICGPT, but it is loaded with models trained on the TinyStories dataset
- You can deploy yourself and load it with other trained models. 
- The pre-trained Llama2 models from meta can be loaded, but do not yet fit. I am working on it and also keeping a very close eye on the underlying core code at https://github.com/karpathy/llama2.c

-------------------------

qwertytrewq | 2023-11-15 20:27:27 UTC | #56

What do you mean by "fit"?

If it can be loaded, then it can be used, can't it?

-------------------------

icpp | 2023-11-16 00:46:27 UTC | #57

When installing an LLM in a canister, there are two steps:
1. Deploy the wasm
2. Upload the trained model (the weights)

Step 1 is always the same, and that fits without any problem in a canister. 

Step 2 however is a different story. Uploading the model weights is done by running a python script that is provided in the icpp_llm repo. This script must be pointed at a `model.bin` file on your disk. It will read it in chunks, and send it to an endpoint of the canister where the llm is running.  When the canister endpoint receives those bytes,  they are stored in an std::vector, which is orthogonally persisted. That vector grows dynamically, and there is where the `fit` comes into play.

Note that this upload is done only once. If the upload succeeds, then the model `fits`!

Small models, with eg. Millions of parameters fit just fine, but models with several billion of parameters will hit the canister limit.

If you want to dig deeper to see where the memory goes once all fully loaded, [this](https://github.com/icppWorld/icpp_llm/blob/c29712f4d65e8c9ffea62e37b45a7c0f1cd23492/icpp_llama2/README_icpp_llama2_resource_requirements.md#canister-resource-requirements-for-icpp_llama2) is a study I did. I find it very interesting and believe there is still a lot of room for improvement.

-------------------------

icpp | 2023-11-24 15:43:53 UTC | #58

Hi All,

I like to announce that the frontend is now also open source under MIT License

You can find it at https://github.com/icppWorld/icgpt 

And it is deployed on mainnet as [ICGPT](https://icgpt.icpp.world)


I hope some of you will try out to deploy this as we continue deAI efforts. Let me know if you run into any issues.

-------------------------

ildefons | 2023-12-14 22:49:19 UTC | #59

(First, excuseme for xposting). I would like to introduce Motokolearn, a Motoko package meant to facilitate on-chain training and inference of machine learning models where having a GPU is not a requirement: https://forum.dfinity.org/t/introduction-of-motokolearn-v0-1-on-chain-training-and-inference-of-machine-learning-models/25312?u=ildefons

-------------------------

Samer | 2024-04-02 13:07:10 UTC | #60

@icpp any updates?

Starting to get really excited about on-chain deAI as I learn more

-------------------------

icpp | 2024-04-03 02:19:25 UTC | #61

@Samer ,

Thanks for your interest!
I share your enthusiasm about AI on chain, and we're working on several items.

- Experimental demonstration apps working within current limits of the IC.  Target release during April and May. 
- Received a grant to port llama.cpp. Target release is in September. 
- A pipeline to easily deploy and configure your AI components, and be ready to scale once the IC capabilities like query charging, 64bit WASM, and ultimately gpu come available.

I have teamed up with @patnorris on some of these items.

-------------------------

CodingFu | 2024-05-25 12:16:57 UTC | #62

This is so cool! I can't believe you can run such a model on IC.

I wonder what is the cycle cost per 1M tokens?
Also, maybe someone knows when we will be able to deploy beefier canisters?

-------------------------

icpp | 2024-05-25 22:13:46 UTC | #63

The costs right now are pretty high. A story that generates about 100 tokens consumes 0.076 TCycles = $0.1.

That will come down drastically though once we can do the inference as a query call and when the compute/AI improvements of the DFINITY Roadmap become available. 

I expect to already deploy a real chat LLM in a few months, but latency will be poor. Implementing it now though, so we are ready when the IC scales up.

-------------------------

laska189345938458347 | 2024-07-01 03:59:56 UTC | #64

[quote="icpp, post:63, topic:21991"]
DFINITY Roadmap become available
[/quote]

When DFINITY Roadmap for this become available?

-------------------------

Severin | 2024-07-01 08:18:50 UTC | #65

It's here: https://internetcomputer.org/roadmap

-------------------------

