patnorris | 2023-08-19 12:32:15 UTC | #1

Hi everyone, I'm excited to share DeVinci, the browser-based AI chatbot app served from the Internet Computer. You can chat with the AI model loaded into your browser so your chats remain fully on your device. If you choose to log in, you can also store your chats on the IC and reload them later.

It's based on Web LLM (https://github.com/mlc-ai/web-llm) which enables LLMs to run in the browser. Currently, only Chrome and Edge on desktop are supported (with increasing WebGPU support this should change in the future though).

If you like, you can find the code here (https://github.com/patnorris/DecentralizedAIonIC) and give the chat app a try here: https://github.com/patnorris/DecentralizedAIonIC :) Do you have any feedback? I'd love to hear it! Thank you.

-------------------------

yangzijiang | 2023-08-19 14:20:45 UTC | #2

Great, do you have a product that I can try

-------------------------

patnorris | 2023-08-21 13:32:16 UTC | #3

Sure thing, you can give it a try here: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io/ Please let me know if you have any feedback, thanks :)

-------------------------

ZackDS | 2023-08-21 14:11:36 UTC | #4

Great stuff btw, for me it gets stuck @ Fetching param cache[49/51]: 1566MB fetched. 94% completed even after few refreshes 

![image|689x310](upload://cDsfbiPm6DyZtjQv1IlkfaDAnXC.png)

index-37d6f91f.js:934 Uncaught (in promise) DOMException: The operation failed for an operation-specific reason

-------------------------

patnorris | 2023-08-21 16:27:03 UTC | #5

Thank you for giving it a try and sharing this!
I'm actually facing the same issue currently when I'm trying to integrate bigger AI models into DeVinci and running them on my device (btw, currently DeVinci uses RedPajama-INCITE-Chat-3B-v1-q4f32_0, so a model with ca 3 billion parameters). My best guess at this point is that the browser/device cannot allocate enough memory to the model (which actually requires quite a lot).

Do you happen to have many other programs/processes or other tabs in the browser open? They could block needed memory.
Do you know how much RAM the device you're running this on has? And does it have a GPU?

-------------------------

ZackDS | 2023-08-21 16:45:17 UTC | #6

Yes it is a Ryzen 5 5600 6c/12t with 32 GB RAM and an intel arc 750 with 8 GB of ram, tried latest Chrome and Canary as well as Brave and Nightly. Also the same on an intel i7 -10700 with iGPU only 16 GB ram still stuck at the same step. Will try on Linux after I get home and dig a little deeper.

Most certainly is a "out of memory" issue https://developer.mozilla.org/en-US/docs/Web/API/DOMException

-------------------------

ZackDS | 2023-08-21 17:32:47 UTC | #7

Sorry my bad, not sure yet if WSL2 and or Docker or other Windows11 bloat. But on a fresh Win11 with only updates nothing else, it works like a charm. Doesn't even eat that much ram as I expected.  Works on chrome and edge very smooth.

![DeVinci|690x289](upload://4xZgGyxFlZoeJyGf12OwMhUTnyf.png)

-------------------------

patnorris | 2023-08-21 18:11:01 UTC | #8

Great, happy to hear :) And thank you for giving it several tries!

Your machine is actually more powerful than mine, so potentially even the bigger models would run. I'll see that I integrate the Llama2 model soon and allow users to choose between the models, so you and others with a similar device can actually try the state-of-the-art models :)

-------------------------

ZackDS | 2023-08-21 18:20:40 UTC | #9

I am actually also looking at running Llama 2 locally before tinkering with it and try running it in a canister since https://forum.dfinity.org/t/llama2-c-llm-running-in-a-canister/21991 it has been done already. But will do another approach.

-------------------------

patnorris | 2023-08-24 09:17:33 UTC | #10

Just pushed changes which make the Llama2 model available to power the chat. Under Settings, you can now choose which model you'd like to use (the default is RedPajama 3 billion parameters).

This is the Llama2 7 billion parameters model, so quite a bit bigger than the default and thus also requires a pretty powerful device to run. Anyone who thinks their device is up to the challenge is invited to give it a try :)

Yes, getting models to run in a canister is amazing. I hope we can make some exponential steps to soon be able to run models like Llama2 7b and beyond in a canister. I'm not sure which improvements we'd need on the protocol/network level to achieve this.

-------------------------

patnorris | 2023-10-21 19:15:06 UTC | #11

Hi everyone, if you like you can give the new Mistral 7B model a try on DeVinci now: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io/
To use Mistral 7B, you need to log in and select it under User Settings. Then, on the DeVinci AI Assistant tab, click on Initialize to download it (will take a moment on first download) and once downloaded, you can chat with it as usual. Please let me know if you have any feedback :) Cheers

-------------------------

patnorris | 2023-12-11 01:14:08 UTC | #12

Hi everyone, I just added a bunch of new models:
Llama-2-13b
WizardCoder-15B
WizardMath-7B
OpenHermes 2.5
NeuralHermes 2.5

You can give them a try by logging in and then selecting them under User Settings: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io/#/settings

Please note that the bigger the model, the more RAM needed :slight_smile: 

Enjoy and please let me know if you have any feedback!

-------------------------

patnorris | 2023-12-14 01:40:30 UTC | #13

Happy to announce that DeVinci now also works on Android :slight_smile: 

If you want to give it a try, you'll need the latest Chrome Canary browser on your Android device. You can then download the RedPajama 3B LLM (default, so you just need to click on Initialize) and then chat with it fully on your device as usual: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io

If you've got limited mobile data, please wait until you've got a Wifi connection as the LLM is over 1 GB big :)

-------------------------

patnorris | 2024-03-01 12:43:18 UTC | #14

Hi all, you can now also use
Gemma 2B
TinyLlama 1.1B

in addition to previous models (e.g. Mistral 7B, RedPajama 3B, several LLama2 versions).

Happy to hear any feedback you might have! Thanks and best :)

-------------------------

patnorris | 2024-03-03 20:02:19 UTC | #15

and Phi-2 is now available if you're on Android (needs Chrome Canary currently)  :+1:

-------------------------

patnorris | 2024-04-20 21:17:29 UTC | #16

You can now try out Llama3 on DeVinci :rocket:

Under Settings, choose Llama3 8B or 70B on laptop and 8B on an Android phone. Best to be on WiFi to not use up your mobile data :)

Enjoy and let me know how it performs for you :+1:

-------------------------

patnorris | 2024-05-14 17:38:50 UTC | #17

Hi everyone, just released a new version with the following updates:
* First vector database functionality: you can upload a PDF document which is turned into an in-browser vector database (so stays fully private) and can then be used by the LLM as a knowledge base (RAG)
* Markdown for messages works now
* Pressing the enter key will send the prompt in the message input field

Please also note that DeVinci is a Progressive Web App and you can thus install it (including using the chat functionality offline).

I'm happy to hear about any feedback or ideas you have :)

-------------------------

patnorris | 2024-05-16 09:07:31 UTC | #18

Thrilled to announce that @Nuno and I will redesign the DeVinci user experience and add some great features to it! Our goal is to improve DeVinci such that it can truly be your end-to-end decentralized and fully private AI Chat app as an alternative to common Web2 services :muscle: The app will stay a hybrid with on-device (easily accessible through the browser) and on-chain components built with and on the Internet Computer.

With @Nuno joining this mission, we've got great expertise in UX/UI, design and branding on the team now, and I'm very excited to present you our redesigned app soon. It'll be on a whole other level :rocket:

We're grateful that DFINITY's supports this work via a dev grant :pray: :tada: As such, please let us know how DeVinci can become max valuable to you and to the ecosystem :ear: 

DeVinci is also part of the shared vision with @icpp for decentralized AI where a hybrid network of on-chain and on-device AI services work together seamlessly for the user and everyone contributes (instead of only a few centralized big services providing all AI) --> DeAI for the Win

Are you open to providing us feedback as a DeVinci user along the way or even interested in becoming an official tester? Please let me know, would be great to have you :)

-------------------------

Agnostic | 2024-05-21 05:45:37 UTC | #19

[quote="patnorris, post:18, topic:22263"]
Are you open to providing us feedback as a DeVinci user along the way or even interested in becoming an official tester? Please let me know, would be great to have you
[/quote]
On Internet Computer main site, there's now an Ai category in the Dapps section.  There are like 16 Ai dapps.  This must have recently been added because I don't see any content creators on social media covering it.

I tried your Ai chat a few weeks ago.  I would say your Ai dapp is the best on ICP right now if you're looking to chat about different topics without jumping to different Ai agents.  The responses are also fast compared to one other ai chat I used where the responses were slow and limited.

I've also tried Elna, ICgpt, and DeAi Chat (DeAI chat not working yet).

-------------------------

patnorris | 2024-05-21 09:17:27 UTC | #20

Thank you so much for the feedback! If you're open to sharing it (here or as a DM); which aspects about DeVinci are most important to improve from your perspective? And what would you like to do with the dapp that it currently doesn't support? Would love to hear more about your ideas and experience, as we're working to lift DeVinci to another level :)

-------------------------

Agnostic | 2024-05-21 17:27:36 UTC | #21

My views would be from an average user perspective since i have no programming/developer skills/experience.

I think it would be good to add some of the same features that we find on Open Ai chat gpt, like text-to-image and even text-to-video.  The Uix has a basic appearance that can be improved, but I'm guessing that you're still developing the site. I know the important thing is the performance of the Ai.

I think we have to find more ways for ai to be decentralized and make it something that the user can take advantage of.  One feature can involve having an easy process (one that doesn't require technical skills or needing to be a developer) for users to contribute/improve on the knowledge base.  For instance, Elna has an option to create an Ai companion.  Maybe make an Ai bot that is managed by a DAO.

-------------------------

patnorris | 2024-05-29 20:00:54 UTC | #22

Thank you so much for your feedback and ideas! That's great as we actually want to make DeVinci very accessible and there are quite a few other tools for programmers already. 
Fully agreed, the goal is to make decentralized AI valuable to the users. For this, we'll level up the UI and UX and some of the features we'll add over the next weeks will allow users more customizations and personalization, so I'll be excited to hear if the features work well and if we're making progress :)

-------------------------

patnorris | 2024-06-08 13:40:21 UTC | #23

DeVinci got an update üöÄ You can now chat offline with your AI, choose from a bunch of new AI models and chat with a PDF of your choice!

You can easily install the DeVinci app on your device. Please note the popup which appears when you enter the app, click on Install and it'll just take a short moment.
Once installed, Initialize the AI model and after it's downloaded, you can use the app and AI chat even offline!
Please note: on desktop, you'll also see the install icon in the url bar and on Android, you'll find the "Add to Home screen" option in the top right menu.

We also added multiple new AI models for you to choose from ü§ùü§ñ
On desktop, these are Phi3-mini, Hermes-2-Pro, Qwen1.5 (great for Chinese :cn:), Stable LM 2, and WizardMath-7B üßÆ. 
On Android, Phi3 is now available for you.
To choose the model you like, please log in and go to the menu in the top right corner, under Settings you can find all available AI modelsü¶æOr go there directly via this link: https://x6occ-biaaa-aaaai-acqzq-cai.icp0.io/#/settings

You may now give one of your PDFs to DeVinci to include its content in the responses. The PDF is loaded into a local knowledge base and the AI will use the most relevant parts of the PDF to provide you specific answers. This way you can chat about the PDF while keeping it private :lock:
Please note that the PDF doesn't leave your device, it remains in your browser session and fully private :+1:

-------------------------

patnorris | 2024-06-08 13:44:26 UTC | #24

To see the current DeVinci dApp in action, please take a look at these demo videos:
Part 1 - Optional Install and AI Selection: https://x.com/thepatnorris/status/1798360550935789925
Part 2 - Private chat with AI and option to include local PDF file content: https://x.com/thepatnorris/status/1798360964494189054
Part 3 - Offline AI chat usage: https://x.com/thepatnorris/status/1798361152214434030

And if you're interested to see what we're working on, please have a look here:
Demo video of the new UI (work in progress): https://x.com/2n1u0/status/1798365281808449694

Thread with new features we're cooking up: https://x.com/thepatnorris/status/1799429384383684797

Do you have any feedback, ideas or requests? We're happy to hear it :slight_smile:

-------------------------

patnorris | 2024-07-14 19:58:26 UTC | #25

Hi everyone, Qwen 2 is now available in DeVinci (laptop and Android) :rocket:
Let me know how it behaves for you :slight_smile:
Have a great day

-------------------------

