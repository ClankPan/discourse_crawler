lastmjs | 2021-02-11 01:19:59 UTC | #1

I've seen the videos on BigMap and BigSearch, and seen some examples and a lot of hearsay. One major set of functionality might be missing here, and I would love to be corrected if I'm wrong.

Is there going to be any native (built into the data structure) way of querying a BigMap, with search queries similar to what you'd find with a relational database or something like MongoDB? As far as I know, BigSearch is for full text search. But we need per field searching capabilities on BigMap, so that we can truly emulate a database.

The LinkedUp GitHub example shows a simple Motoko HashMap being used as the database, and the search functionality is implemented with a for loop that searches through all records to find the correct record. I'm really hoping BigMap will have this type of functionality built into it, otherwise that's a major project that someone is going to have to do.

So, will BigMap/BigSearch have these features? If not, am I correct in that a library would need to be made that essentially reimplements the common search functionality of popular database like Postgres, MySQL, or MongoDB? And that in the mean-time rudimentary searches through all records is the best we have (and that library would essentially provide abstracts over full-iterative searches)?

-------------------------

enzo | 2021-02-11 03:29:45 UTC | #2

The purpose of BigMap is to provide a key-value store that is distributed across multiple canisters. In contrast to relational databases, which define a data structure made up of tables of rows and columns with predefined data types, key-value stores store data as a single collection without any structure or relation. I do not expect BigMap to provide relational features since it is not relational in nature. If you want something that resembles a relational database, then I would recommend you look into compiling SQLite down to WebAssembly and deploying that. I'll let others speak for the capabilities of the newer Rust implementation of BigMap. My Motoko implementation from last Summer supported few features beyond `get` and `put`, but was also written in just a few hundreds lines of code, which I think is rather remarkable. If I had more time, then I would have supported ordered iteration through keys and thus searches on partial keys. I believe the Rust implementation has been more focused on autoscaling via dynamic canister creation and automatic rebalancing of data.

-------------------------

lastmjs | 2021-02-11 04:58:11 UTC | #3

So it sounds like there is going to still be a significant effort to have something like the databases that we are used to developing with...compiling something like SQLite to Wasm could work within one canister, but we are going to need a database that automatically scales across canisters, and can query/search across canisters.

We were told we wouldn't need databases anymore. It looks like we still need a database, but one specifically designed for the Internet Computer. A simple key value store is insufficient for many applications...in fact, I'm not sure what web scale CRUD-based application if any is going to succeed without a scalable queryable data store.

Am I missing something?

-------------------------

Gabriel | 2021-02-11 09:09:08 UTC | #4

Hi @lastmjs 

I've been researching this for a bit but it seems BigMap won't be the solution for a relational db. 

As @enzo said and AFAIK bigmap will behave like an auto-scalable redis. 

Check Matt's answer here(scroll down a bit): https://forum.dfinity.org/t/idea-for-crud-types-feedback-appreciated/1054/4

I've been working on simple structure for a graph as well and hopefully I'll be able to share that soon but it's going slow because of my different background (go and ts).

For a better understanding I would suggest to have a look at: 

https://en.wikipedia.org/wiki/Graph_database

https://neo4j.com/developer/guide-importing-data-and-etl/

https://www.slideshare.net/neo4j/designing-and-building-a-graph-database-application-architectural-choices-data-modeling-and-testing-ian-robinson-graphconnect-ny-2013-28056565

https://www.dataversity.net/property-graphs-swiss-army-knife-data-modeling/

Most of the online docs are pointing towards neo4j and it will be a bit hard to get your head around it but the core implementantion could start with this: 

https://github.com/matthewhammer/motoko-graph/blob/px/src/Persistent.mo#L46

-------------------------

cryptoschindler | 2021-02-11 09:57:17 UTC | #5

If one could access the canisters underlying Memory based on addresses it should be possible to implement neo4j‘s approach and scale it out following their ‚Fabric‘ approach. Is this what you have in mind @Gabriel ?

-------------------------

Gabriel | 2021-02-11 10:18:51 UTC | #6

Hi @cryptoschindler 

Not sure what you mean with `access the canisters underlying Memory based on addresses` but basically yes. In the end their approach is based on labeled property graph model but they've been developing this for years so my approach is the least rudimentary. Also in terms of scaling I don't know how this is going to work. Big map auto scale would give me an idea how to map things in order to scale when the data structure gets too big. 

For now I have a simple structure just to get things going and on top of that I'm trying to add a simple orm for crud operations, validation etc.

-------------------------

Gabriel | 2021-02-11 10:30:47 UTC | #7

@cryptoschindler 

Ah, now I see what you meant yeah basically if we can access each canister memory like they use fabric infrastructure eg: data sharding, and the query language will retrieve results from multiple federated graphs

That should solve the scaling issue quite nicely. 

I don't know exactly how this will work with dfinity as I don't have a clear understanding how canisters clone and split memory between them and how you keep track of each one of them but this could become the go-to graph database.

-------------------------

cryptoschindler | 2021-02-11 11:08:29 UTC | #8

Basically one of the reasons why neo4j is so fast is because they mostly store fixed size records and use index-free adjacency. This means each node acts as a micro-index of nearby nodes. They also divide their storage into Node storage, Label storage, Property  storage and Relationship storage which contain the respective information. 
If we have fixed size records and want to look up a certain node id, we can calculate the node records location directly with cost O(1). One could probably implement this on top of a higher level data structure but it would be nice to get rid of the overhead and directly write to the canisters memory at a certain address. 
As a canister only has access to 4 GB of memory you have to shard this out to multiple canisters anyways if your DB is bigger than 4GB.

I'm interested in working on this, let me know if you have an accessible repository :slight_smile:

-------------------------

lastmjs | 2021-02-11 14:59:56 UTC | #9

Ah, thanks for all of the info.

So we definitely will need to create a general-purpose CRUD database that automatically scales and allows complex querying, BigMap and BigSearch seem out for that then.

Now my question is, what is the best architecture to choose for this database? I know you've mentioned a lot about graph databases, I just want to make sure that's the right choice. Please reason through this with me if you don't mind.

Taking an existing relational database like SQLite or Postgres is probably out of the question for multiple reasons. SQLite would probably work well compiled to Wasm, but it will be stuck within one canister, so no automatic scaling. And as far as I understand, relational databases don't do horizontal scaling too well (I could be wrong, I believe you can scale them horizontally, but I believe their underlying mathematical properties don't lend to this well, and I don't seem to ever hear about horizontally sharded Postgres or other SQL databases).

So if a traditional SQL/relational database is out of the question, which architecture is best? For vertical scaling, we've got about 4gb of storage and that's it. As for processing power within a canister, I'm not sure what we're dealing with. But anyway, we'll need an architecture that can be sharded and scale out horizontally, extremely well.

So a graph database architecture seems like a good choice. But is it the best choice? What other architectures have you considered? What led you to the graph database?

This is going to be a foundational and indespensible project, I feel most apps are going to need a scalable database, as in it will be crucial for web-scale applications.

You mentioned creating an ORM. Personally, I'm more interested in just having the query language or the low-level API. It might make more sense to focus efforts there, and create a solid foundation first that ORMs and other projects can build off of. ORMs are notoriously bad, and getting them right I believe is a very difficult task.

To that end, that's one of the major motivations for the project I'm about to start working on in a couple weeks. I'm not sure the exact name I'll use yet, probably something like GraphIC, GraphICP, or ic-graphql. The idea is to create the ultimate GraphQL generator. You define a schema, and it will automatically generate resolvers for all basic CRUD operations, and eventually more. Essentially it will be an ORM, and IMO using GraphQL as your ORM is the best way to do an ORM that I've ever seen. The standardization and world of tooling will be unmatched compared to any custom ORM, especially one being built now from scratch.

But for my project to succeed and reach its full potential, it needs an infinitely scalable database API to implement the resolvers with. I was hoping BigMap/BigSearch were going to provide that implementation, but I guess not.

So my motivation here is creating the ultimate declarative backend, the simplest way to develop a CRUD application that has ever existed. I'm about to start dedicating most of my time to the GraphQL project, but I need this database.

-------------------------

bengo | 2021-02-11 18:28:29 UTC | #10

[quote="lastmjs, post:9, topic:1977"]
I need this database.
[/quote]

1. Option 1: Build the database you need. Maybe others will even pay to use it if it's so necessary. There will be nothing remotely graphql-related useful until this ambitious effort is done.
2. Option 2: Limit the domain of GraphQL queries the tools you build can handle to be ones that you know how to program a resolver for. It's totally fine to only be able to resolve simple queries in a first prototype milestone. Most of the time, rather than taking on a big ambitious set of work in one batch, you'll be delivering better return on investment by setting goals to [limit your batch size](https://www.scaledagileframework.com/visualize-and-limit-wip-reduce-batch-sizes-and-manage-queue-lengths/), ship something useful for for a small set of use cases, and then release regularly with progressively increased use case goals.

[![mvp|669x500](upload://y6N3u7aFBJK7mtkhleqkbMUuNyQ.jpeg)](https://blog.crisp.se/2016/01/25/henrikkniberg/making-sense-of-mvp)

-------------------------

Gabriel | 2021-02-11 20:15:55 UTC | #11

> Now my question is, what is the best architecture to choose for this database? 

Mostly because you don't have any other options. Unless you get TCP/HTTP the only option for now is in memory db's. sqllite option could work as well but I'm fairly sure it won't scale. (eg: augur core).

> So a graph database architecture seems like a good choice. But is it the best choice? What other architectures have you considered? What led you to the graph database?

For scaling if @cryptoschindler suggestion can be implemented then we're gold. Also most of the benchmarks shows that graph databases are way faster. Check :::https://www.researchgate.net/publication/220996559_A_comparison_of_a_graph_database_and_a_relational_database_A_data_provenance_perspective

> You mentioned creating an ORM. Personally, I’m more interested in just having the query language or the low-level API. 

The ORM in question is not the magic ORM you're thinking of. 

Basically I want the graph structure be a simple structure and the ORM part to be added on top of the graph. For example for our own API we need custom validations per field per action, sanitisation and a lot of custom logic. See https://forum.dfinity.org/t/idea-for-crud-types-feedback-appreciated/1054

> The idea is to create the ultimate GraphQL generator.

For a graph database a GraphQL layer is esentially useless. Because you don't need custom resolvers for complex queries, parsers etc. See Cypher is the best example as why you don't need it. https://medium.com/neo4j/connecting-to-react-app-to-neo4j-148881d838b8

Baby steps. 

PS: @cryptoschindler I'll be in touch over a message once I clean my stuff.

-------------------------

lastmjs | 2021-02-11 21:20:28 UTC | #12

[quote="Gabriel, post:11, topic:1977"]
For a graph database a GraphQL layer is esentially useless. Because you don’t need custom resolvers for complex queries, parsers etc. See Cypher is the best example as why you don’t need it. [Connecting your React app to Neo4j with React Hooks | Neo4j Developer Blog](https://medium.com/neo4j/connecting-to-react-app-to-neo4j-148881d838b8)
[/quote]

I wouldn't agree with that. Taking a look at Cypher, GraphQL is far more declarative. Cypher I'm sure is far more powerful and flexible, but I think GraphQL could provide a very nice declarative abstraction over resolvers written with Cypher, for example.

For the database itself, I'm not sure GraphQL would be the right choice (but it could be, see https://dgraph.io/), but for an application that might be using a graph database along with other data sources, using GraphQL could be a great choice to abstract over those underlying details.

Also, authentication, authorization, and per field rules can be provided through GraphQL schema directives, not that the graph database shouldn't have that functionality natively as well, perhaps.

-------------------------

lastmjs | 2021-02-11 21:24:47 UTC | #13

This database is such an important building block, I'm hoping a robust community project evolves or the foundation takes a deep interest in incentivizing its creation.

One of my biggest problems here is the marketing we have received. We were told specifically that there would be no need for databases, and though skeptical I hoped it would be true. It seems we will have the primitives to build a database on the Internet Computer, but it must be built and many applications will require a specialized database built for the Internet Computer.

I wish for the DFINITY team to be more transparent with the vision and the limitations of the Internet Computer. Right now I have to shuffle through everything that's said to get to what's real and practical and what's not.

-------------------------

enzo | 2021-02-12 03:05:10 UTC | #14

Few would dispute that there is much work to be done before the vision for the IC is fully achieved. There are many challenges still to overcome, not least better search functionality. Though perhaps the greatest challenge of all is to think differently on how existing use cases can be better satisfied without resorting to the legacy stack, especially when messaging is reliable, persistence is orthogonal, and computation is trusted. All the same, I will share your feedback with marketing to ensure we deliver a consistent message across all communication channels.

-------------------------

lastmjs | 2021-02-12 03:23:22 UTC | #15

Thanks, I appreciate it!

And thanks for pointing out these new capabilities, I'm excited for the challenge ahead

-------------------------

Dunning | 2021-02-12 10:34:42 UTC | #16

It should be possible to build a Balanced-tree or [General-balanced tree](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.7244) library, in which case you would be well on the way to enabling indexes that can be searched in O(Log n) time. Finding a good method to partition the tree across multiple canisters might have some challenges but perhaps just try it naively and see what the performance is like.

-------------------------

lastmjs | 2021-02-12 14:08:24 UTC | #17

What about joins with a B-tree? Any advice there? Do you think those would perform well?

-------------------------

Steve | 2021-02-12 15:47:00 UTC | #18

It should work just fine:

Consider - https://dev.mysql.com/doc/refman/5.7/en/nested-loop-joins.html

The tricky part would be how you handle a join with data split between two canisters.

I haven't put much thought into it just yet, but I wonder if the IC lends itself more towards a system like Hadoop / MapReduce instead of relational DBs.

-------------------------

lastmjs | 2021-02-12 15:53:34 UTC | #19

Yep, the multi-canister issues seem to be the major source of complexity when discussing these primitives that need to be built on top of the platform.

It would have been really nice for the multi-canister/multi-Wasm-module abstraction to have been handled at the canister level, instead of moving it up above the canister level to user-space.

-------------------------

bengo | 2021-02-12 21:24:34 UTC | #20

+1

It's a good practice to present an internet service as an API that isn't tightly coupled to one database vendor like Neo4J. GraphQL is clearly becoming the de-facto standard for that (IMHO) and (I believe) has much more widespread adoption in HTTP APIs that Neo4J query language.

-------------------------

bengo | 2021-02-12 21:27:43 UTC | #21

> I haven’t put much thought into it just yet, but I wonder if the IC lends itself more towards a system like Hadoop / MapReduce instead of relational DBs.

Totally. In fact, those tools you mention are good any time you are employing a lambda architecture (not to be confused with AWS Lambda) https://en.wikipedia.org/wiki/Lambda_architecture

Any time you have way more reads that writes, it can be helpful to take your most common reads/queries, precompute them on write, store that somewhere, then reads/queries are just key-value lookups and your service can spend time calculating expensive queries instead of common/easy ones.

Some programmers don't like that it's not 'DRY', but it's simply a matter of data structure choice based on the nature of your reads/writes in the wild.

-------------------------

PaulLiu | 2021-02-13 17:41:25 UTC | #22

I agree with what Enzo said. 

IC removes the need to use databases or files for persistence purpose, but it does not, nor was it intended to, change any data relationships in your application. If your data naturally fits a key/value model, bigmap can be a candidate solution to scaling. If your data fits a traditional relational model, something else needs to foot the bill. We (those who work at DFINITY) don't have a ready solution to offer to developers at this moment, but I do imagine building a relational data model will also be less complex when persistence is transparent, and atomicity (at message level) is already provided at system level.

It makes a lot of sense that a conventional application will want to use relational DB both for its data model and for persistence. But when it comes to scaling out, it can also become very complex and application specific. I'm afraid there is no silver bullet in this area (unlike key/value stores, where every other NoSQL solution claims they are the silver bullet).

-------------------------

PaulLiu | 2021-02-13 00:17:35 UTC | #23

Ping @matthewhammer as I recall he has built a relational data model with joins in Motoko, but I don't think it supports multi-canister, nor was it intended as a scaling solution.

-------------------------

lastmjs | 2021-02-13 18:11:20 UTC | #24

Thanks for all of the thoughts in this thread so far, they have been very helpful.

As I've been reasoning through this more based on all of the discussion here, I'm starting to come to the conclusion that an efficient and scalable data structure, which allows relational modeling, will be possible by composing together multiple basic data structures customized somewhat for the IC (the customization mostly dealing with scaling across canisters).

It really seems that with a combination of tree/map structures, like b-trees and b+-trees, we can get the basic functionality that we want. For example, these two data structures are what SQLite is built out of, under the hood (if my sources aren't wrong). SQLite uses b+-trees to store the tables, and b-trees for indexes.

Given the orthogonal persistence of the IC, a lot of the underlying complexity of getting a database to function properly in a web/network app context will just melt away. That is exciting.

I'm thinking of starting using simple Rust data structures that already exist, such as a BTreeMap. With some binary search functionality, either built-in or custom-made, I think I can get a good prototype to work well within one canister. If that can work, I think we'll be well on our way to a scalable solution.

As long as the data structures are built to compose together well, I'm thinking we should be able to create Big versions of these data structures, and they should work remarkably similar to the non-Big versions that only work within a single canister.

So if we can get a BTreeMap and a B+TreeMap and a binary search to work well within one canister, then if we build a BigBTreeMap and a BigB+TreeMap, with a BigBinarySearch, then we might be able to simply swap out the data structures in the single canister version and viola get a multi-canister version.

Thoughts?

-------------------------

hassen | 2021-02-16 08:22:34 UTC | #25

@lastmjs Here is a port of SQLite to the Internet Computer [IC_sqlite](https://github.com/HassenSaidi/IC_sqlite). It's a simple canister that allows you to create a database and to run SQL updates and queries. SQLite is probably the easier port of a legacy application to the IC you can find. I think it is nearly impossible to do this for the most popular database software like MYSQL or PostgreSQL. Running a SQLite on the IC has its advantages compared to running it on an OS. SQLite does not have any specific user management functionality and hence is not suitable for multiple user access. SQLite does not have an inbuilt authentication mechanism. However, your can add to your canister an authentication layer, and the ability of having multiple users access multiple databases hosted on the same canister.

Note that the wasm is not optimized at all. 2/3 of the wasm code is probably dead code. Many settings that make sense on a traditional OS are not needed on the IC. So there are a lot of things to like about it despite its limitations. It's probably not too difficult to extract the core BTreeMap logic and throw away most of the rest of the code. This way you can take advantage of years of optimizations put into SQLite.

I'd like to echo what @enzo and others have said though. Building applications and services on the IC is as much about **simplification** as it is about taking advantage of the fact that messaging is reliable, persistence is orthogonal, and computation is trusted.  So I think we might be threatened by a great opportunity to innovate here.

-------------------------

ovi | 2021-02-18 14:51:38 UTC | #26

thank you for sharing that! can you also estimate how many days/hours it took you to make this port?

-------------------------

zafaransari | 2021-03-06 12:34:21 UTC | #27

I would recommend an RDF triple store like Terminusdb  https://terminusdb.com/ or Flureedb  https://flur.ee/ with datalog as query language.

Please also check this article https://www.linkedin.com/pulse/rethinking-database-lars-r%C3%B6nnb%C3%A4ck/  and the implementation of this idea in rust https://github.com/Roenbaeck/bareclad

-------------------------

Dylan | 2021-03-10 10:34:16 UTC | #28

For use cases like the one in this thread, would it be possible to implement a memory manager that abstracts away the canister memory/storage limit? If you were porting C code like SQLite, you could override malloc(), free(), etc. to call into your memory manager. The memory manager would scale the heap by using multiple canisters, effectively extending the heap memory address space from 32-bit to 64-bit (16-exabyte)

That's assuming you can compile the SQLite C code as 64-bit and run it in a canister. I haven't compiled C code to Wasm, so it's quite possible that what I just suggested cannot be done, at least not without some tricks in the C code to implement pseudo-64-bit addressing. Also, it seems like it would be inefficient, since I would think that inter-canister calls to retrieve large blocks of data are very inefficient compared with accessing memory within a canister.

I agree that the best option in most cases is to try to think exclusively in a way that breaks large, complex systems into components that live within multiple IC canisters, at least while the 4GB Wasm/canister limit exists. For those of us coming from 64-bit programming though, this is a limitation we're not used to, at least not in the past decade.

-------------------------

justinmchase | 2021-05-17 19:08:56 UTC | #29

This seems really relevant and interesting:
http://www.actordb.com/docs-querymodel.html#h_412

It seems like it would not work perfectly with ICP out of the box but it appears to have done a lot of relevant research that may apply here as well.

-------------------------

justinmchase | 2021-05-17 19:17:41 UTC | #30

A very interesting conversation here!

I just wanted to add that it seems like if you have a BigMap and a BigTree, then any database in an actor based system could be expressed as a set of actors which each contain a different index over the same data.

It seems likely that at the maxiumum scales were considering here that such a database won't be able to support arbitrary queries without limits and that aggregates will likely need to be expressed as actor indexes themselves.

-------------------------

joshbenaron | 2021-05-26 16:45:51 UTC | #31

I think this would be a huge step forward. I'd be happy to work on this pretty much full-time. Has anyone worked on this yet?

-------------------------

ehsan6sha | 2021-07-13 17:04:23 UTC | #32

Hi. Has there been a progress or any codes published since this conversation?

-------------------------

lastmjs | 2021-07-15 18:53:53 UTC | #33

I've made a lot of progress on Sudograph, which you can view here: https://github.com/sudograph/sudograph

Sudograph has an underlying relational database, called Sudodb: https://crates.io/crates/sudodb

Currently Sudodb is implemented with a combination of Rust BTreeMaps, and works within a single canister. Based on multiple conversations with DFINITY engineers, though it's not a certainty, it seems very likely that with memory64 and some improvements to the IC, that canisters will be able to theoretically scale to the capacity of a subnet without too many complications. A single canister being able to operate on terabytes of data doesn't seem out of the question.

So for now, Sudograph will be moving forward with its single-canister design, and will take advantage of hopefully soon-to-come improvements to the canister storage limit. And I am starting to believe that this will be sufficient for an enormous amount of use-cases.

-------------------------

