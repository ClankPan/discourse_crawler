diegop | 2023-12-01 18:31:14 UTC | #1

## Summary

Currently, smart contract canisters can send *update calls* to any canister. These calls can even form "chains" of calls such as the following:

Imagine you have three canisters: A, B, and C.

A.updateCall() -> B.updateCall() -> C
**Result**: A gets a response in around 4 seconds* 

*(*if it's within the same subnet, it could be much faster or as slow as 5 seconds per call if its cross-subnet)*

This is helpful in making an application composed of multiple canisters calling each other. The "catch" is that update calls are replicated using consensus so they take around 2 seconds each. This means that if A calls B which calls C, then it may take 2 seconds (A -> B) + 2 seconds (B -> C) or 4 seconds for A to get a response. If one had a system where 10 canisters had to linearly call each other with update calls, it may take up to 20 seconds for the initial canister to get a response as each canister *update* call takes 2 seconds. Not great user experience.

Fortunately, we have *query* calls that are very very fast (around 200 ms) and can also be sent to any canister, but currently, they cannot be chained in the same way as "inter-canister query calls." This project is about adding inter-canister query calls as a native feature to the IC. 

This would allow the following chain to happen:

A.queryCall() -> B.queryCall() -> C 
**Result**: A gets a response in around 400 milliseconds.

## Rationale

This feature has been asked about many times and is the [most popular in the community megathread](https://forum.dfinity.org/t/megathread-community-submissions-for-dfinity-foundation-s-roadmap/6175/5?u=diegop). Here are some examples of the community asking for it:

1. https://forum.dfinity.org/t/megathread-community-submissions-for-dfinity-foundation-s-roadmap/6175/5?u=diegop
2. https://forum.dfinity.org/t/megathread-community-submissions-for-dfinity-foundation-s-roadmap/6175/34?u=diegop
3. https://forum.dfinity.org/t/fast-inter-canister-query-calls/1953/3 
4. https://forum.dfinity.org/t/consensus-and-inter-canister-calls/1424/7


## What you can do to help

* Ask questions on intent, design, trade-offs
* Propose design ideas
* Propose a narrower or different scope for the project
* Say whether you like or dislike prioritizing this project
* Vote on proposals

Want to see what to expect? Check out project to get a sense of the emerging process: https://forum.dfinity.org/t/increased-canister-storage/6148?u=diegop

## Documentation

This project is just starting, but there has been significant design thought put into this in the past (but not enough time to prioritize its implementation). More documentation will be posted here, but there are some helpful links to add context: https://sdk.dfinity.org/docs/developers-guide/tutorials/intercanister-calls.html


## Key people involved
Ulan (@ulan ), Akhilesh Singhania (@akhilesh.singhania), bogdan warinschi (@bogwar )

-------------------------

nomeata | 2021-08-27 20:48:40 UTC | #2

Some questions to get started (nothing too surprising).

* Will this work across subnets?
* What happens when a query _method_ is doing such a call during replicated execution (e.g. when itself called from another canister like now, or when itself called via update calls)?
* Will adding this feature make it harder to add “certified queries” (i.e. queries that don't go through ordering consensus, but are still signed by the subnet) in the future? Or maybe it makes that feature actually _more_ likely (because with certified queries, we can actually implement inter-canister query-from-update calls that way, instead of falling back to crossnet messaging)?

-------------------------

Hazel | 2021-08-28 16:09:12 UTC | #3

Voicing a :+1: :pray: for this. Ideally after canisters can move ICP. 

+1 on Nomeata's questions.

-------------------------

Maxfinity | 2021-08-29 18:08:11 UTC | #4

For inter-canister calls, I think it would good if in addition to caller(), we have a command like original_caller() that gets the id  of the first caller in a chain of calls. This  would be like the meta data in the transaction for an Ethereum transaction.

-------------------------

nomeata | 2021-08-29 21:33:32 UTC | #5

That has been discussed a long time ago internally, and while it seems at first like an innocent feature, if you are coming from Ethereum, some of us deemed it possibly dangerous. But we should take this to a dedicated thread, as it isn't really about inter-canister query calls, so if you want to hear my reasons, just start one :-)

-------------------------

Maxfinity | 2021-08-29 21:34:39 UTC | #6

I have created a new thread for your (as well as others') thoughts and comments. :)

-------------------------

Trevor | 2021-08-30 03:46:43 UTC | #7

Just voicing my support for this. I think creating a better experience within IC is more important than the integrations with BTC/ETH currently. 

I think understanding resources requirements to do this would be helpful, what do we give up by having people work on making this possible, or is this a fairly easy thing to change? Are there any risks to this?

-------------------------

flyq | 2021-09-03 09:49:20 UTC | #8

What is the reason for the previous restriction?

-------------------------

Daniel | 2021-09-16 09:07:44 UTC | #9

could you please link the thread?

-------------------------

alexander | 2021-09-17 06:40:53 UTC | #10

This feature has highest priority in our project. Our project has large amount of data distributed acros many canisters (possibly subnets). 
For calculation we are planning implement MapReduce and execute it across those canisters and "eventually consistent" data just fine for us.

-------------------------

alexeychirkov | 2021-09-17 06:45:52 UTC | #11

Cant wait to see this implemented!
Top priority for us!

-------------------------

diegop | 2021-09-21 03:00:33 UTC | #12

Just an update for readers: the folks who own this project are a bit underwater at the moment, specially Ulan (you may have seen his video on [memory management](https://youtu.be/NWSeM8YgGv4?t=471)) but I hope to get this back up soon.

-------------------------

jzxchiang | 2021-10-18 00:17:18 UTC | #13

Is there any update on this?

-------------------------

diegop | 2021-10-20 03:59:17 UTC | #14

Thank you for asking. I have been aiming to update but I keep getting pulled away. 

Here is the status:

The team who can work on this has been pulled away on three major things so they have not spent much time on this. 

For transparency, those big things are:
* Sandboxing processes (which affect security, BTC integration, Threshold ECDSA, among others)
* Increased canister smart contract memory: 
* Performance and stability: https://www.youtube.com/watch?v=NWSeM8YgGv4&t=471s

-------------------------

jzxchiang | 2021-10-20 06:56:00 UTC | #15

Those do sound important as well.

I'm curious if you know whether Rust canisters are somehow able to make inter-canister queries? I've heard that it's possible in Rust but not Motoko—but it's not documented anywhere.

-------------------------

diegop | 2021-10-20 18:46:41 UTC | #16

Good question. I’m not aware so let me ask.

-------------------------

rossberg | 2021-10-21 07:11:43 UTC | #17

@jzxchiang, curious where you heard that, but that information is incorrect. Inter-canister queries are not currently supported by the IC, so not available in any language.

-------------------------

jzxchiang | 2021-10-21 20:34:06 UTC | #18

I heard it here: https://twitter.com/lastmjs/status/1441637144024403974?t=0fWyxOndErg37zbxiX3YSQ&s=19

-------------------------

nomeata | 2021-10-21 21:19:17 UTC | #19

The replica that shipped with `dfx` had an incomplete “tech preview” of that feature for a while. It was never a feature on “the” IC, but it might explain the confusion.

-------------------------

jzxchiang | 2021-10-22 07:09:23 UTC | #20

Interesting, didn't know.

Well, quick plug for why inter-canister queries are important... I think it's especially critical for multi-canister apps. For example, if there's an auth canister that other canisters call to authorize requests they receive from end users, that would need to be a quick query and not a slow update.

Another situation is a BigMap-like setup, where a frontend canister queries a set of backend canisters to get the requested data. It'd be much simpler to have the client just interact with that one frontend canister, instead of having to get an index from the canister and then make the call directly to the appropriate backend canister.

I'm sure there are other use cases, but I do think this feature would make a big impact on developer productivity and the ability to scale apps to multiple canisters.

Personally, I think it's important to make existing things work better—in addition to building new things (i.e. the BTC <> ICP integration).

-------------------------

nomeata | 2021-10-22 08:00:04 UTC | #21

[quote="jzxchiang, post:20, topic:6754"]
For example, if there’s an auth canister that other canisters call to authorize requests they receive from end users, that would need to be a quick query and not a slow update.
[/quote]

Great example!

For an auth canister, I assume that you _also_ want the response to be secure, right? But query calls are inherently insecure, as they are executed by a single, possibly malicious node. And this problem does not disappear just because the call originates in a canister.

Sometimes query calls can be made more secure using certified variables, but it requires some serious effort from the application developer on both the server and the client side (see [the second half of my explainer video](https://dfinity.org/howitworks/response-certification) on that, but note that the shown slides are somewhat wrong and off. A fixed video is in the making by DFINITY and should be uploaded any time now).

Of course, one could say “give us inter-canister query calls, even if they are less secure” – after all, it’s only consistent with what we get with normal query calls. But then the question is: What should happen when a canister running in the replicated environment, meaning it was invoked via a regular (a.k.a. update) call or a regular inter-canister call, wants to do a query calls? Just doing a normal query call from the replicated execution engine could easily break determinism and thus the stability of the system.

This just as a glimpse why inter-canister calls, done correctly, are harder than it looks like.

-------------------------

ulan | 2021-10-22 11:14:22 UTC | #22

Thanks @nomeata for clarifying the confusion and also giving an example why inter-canister query calls (ICQC) are hard. 

For many people the feature sounds like a simple extension of what we already have for update calls. The original post in this thread also gives that impression. I think supporting ICQC is a much harder problem (probably by orders of magnitude). There are many roadblocks in specification, performance, memory usage for which we currently don't have good solutions.

As @diegop mentioned above, the team is busy with higher priority projects. For example, I am working 100% on canister sandboxing and don't have enough cycles to think through all these issues. 

To set the right expectations: please don't expect much progress here in the near future (several months) unless there are volunteers from the community to drive this forward.

-------------------------

dostro | 2021-12-27 17:58:29 UTC | #23

@ulan @diegop any update here on progress?

Our team is also blocked by this

-------------------------

ulan | 2021-12-27 19:14:17 UTC | #24

Hi Dan,

The status of the feature is the same as before. There was no progress due to other higher-priority projects. 

That's said, thank you for the input! I'll bring this up in our next team meeting after the holidays.

Would you mind sharing more details about your use case? One design that we are considering is executing the response callback of an inter-canister query call against the latest state of the canister (which differs from the state when the call was performed). That would resolve the main performance/memory blockers of ICQC but would make the programming model more complicated because the response callback would not see the memory changes done by the original query.

Cheers,
Ulan.

-------------------------

dostro | 2021-12-28 16:05:16 UTC | #25

Thank you for the response!

We're working on making updates to Internet Identity and would like to store registered usernames from all of an anchor's logins so that a user could select which username to "continue as" when authenticating to dapps.

Among other things, we need to communicate between the target canister and ours, likely across subnets.

Do you anticipate us running into any issues running cross-canister, cross-subnet calls like this in the future?

-------------------------

jzxchiang | 2021-12-28 22:27:05 UTC | #26

[quote="ulan, post:24, topic:6754, full:true"]
One design that we are considering is executing the response callback of an inter-canister query call against the latest state of the canister (which differs from the state when the call was performed). That would resolve the main performance/memory blockers of ICQC but would make the programming model more complicated because the response callback would not see the memory changes done by the original query.
[/quote]

Perhaps that would be fine if the main use case is ingress queries making inter-canister query calls, since there are no state changes involved.

If ingress updates make inter-canister query calls, that would indeed be more complicated.

Although ingress updates can probably tolerate additional latency from inter-canister updates (i.e. the client already is doing optimistic updates), so maybe that's OK?

-------------------------

ulan | 2022-01-03 12:23:04 UTC | #27

Sorry for the delayed response. I was on vacation. 

[quote="dostro, post:25, topic:6754"]
Among other things, we need to communicate between the target canister and ours, likely across subnets.

Do you anticipate us running into any issues running cross-canister, cross-subnet calls like this in the future?
[/quote]

The latency of a cross-subnet query will be higher compared to the same-subnet query. The main issue I see is that it may take a long time until cross-subnet queries are supported. We have a prototype for same-subnet queries, but there is no clear solution for cross-subnet queries yet. It will likely take several months to come up with the solution and implement it.

I wonder if there is a way to unblock you in the meantime. Did you consider moving the cross-canister communication to the client-side JavaScript in the browser? So that JavaScript calls your canister and another canister and combines the results? Alternatively, can you use update calls instead of query calls. The latency will be higher, but cross-subnet/cross-canister update calls are already supported now.

[quote="jzxchiang, post:26, topic:6754"]
Although ingress updates can probably tolerate additional latency from inter-canister updates (i.e. the client already is doing optimistic updates), so maybe that’s OK?
[/quote]

Yes, I think that should be OK. The called query would be a replicated query and would need to use the latest state anyway.

-------------------------

jzxchiang | 2022-01-04 19:13:25 UTC | #28

> We have a prototype for same-subnet queries, but there is no clear solution for cross-subnet queries yet.

If I understand correctly and you already have a prototype for same-subnet, inter-canister queries, then when will that prototype be productionized and fully launched on the mainnet?

I thought inter-canister queries were too difficult to implement. I'm surprised you already have a prototype for it.

-------------------------

ulan | 2022-01-05 08:11:51 UTC | #29

[quote="jzxchiang, post:28, topic:6754"]
If I understand correctly and you already have a prototype for same-subnet, inter-canister queries, then when will that prototype be productionized and fully launched on the mainnet?

I thought inter-canister queries were too difficult to implement. I’m surprised you already have a prototype for it.
[/quote]
The prototype works only for same-subnet queries. To make it production ready we need to support cross-subnet queries. Unfortunately, the prototype does not generalize to cross-subnet queries, so we need a completely new approach. In addition to that we need to solve the spec issues raised in this thread and the state explosion problem due to callers holding on to the old states. All this make ICQC a very difficult problem.

-------------------------

jzxchiang | 2022-01-05 23:52:19 UTC | #30

> One design that we are considering is executing the response callback of an inter-canister query call against the latest state of the canister (which differs from the state when the call was performed). That would resolve the main performance/memory blockers of ICQC but would make the programming model more complicated because the response callback would not see the memory changes done by the original query.

Does this design not work for cross-subnet queries? I'm curious what makes cross-subnet queries that much more challenging than same-subnet queries.

I think having some limited, working version of ICQC―even if not complete―could still be potentially very useful.

-------------------------

ulan | 2022-01-06 07:40:21 UTC | #31

[quote="jzxchiang, post:30, topic:6754"]
Does this design not work for cross-subnet queries?
[/quote]
Yes, from the implementation and performance point of view that design is the most promising. However, it is more difficult to use for developers. For example, it is not compatible with [async Rust API](https://github.com/dfinity/ic/blob/master/rs/rust_canisters/dfn_core/src/api/futures.rs) because the response closure is stored on the heap, but the heap changes would not be preserved in the proposed design when the response arrives. This means that only static functions can be used as response callbacks. I am also worried that the developers will miss the subtle point that all memory changes are discarded by the time when the response callback runs, so it will be a major footgun.

-------------------------

nomeata | 2022-01-06 12:48:06 UTC | #32

Interestingly, if canisters program closer to the actor model, e.g. [to be always upgradeable, even with outstanding calls](https://www.joachim-breitner.de/blog/789-Zero-downtime_upgrades_of_Internet_Computer_canisters), then such a model becomes a _bit_ more plausible.

-------------------------

ulan | 2022-01-06 13:52:03 UTC | #33

Great insight and write up, @nomeata! Indeed, the design looks more reasonable with the actor model.

-------------------------

akhilesh.singhania | 2022-01-07 13:33:29 UTC | #34

I chatted with Ulan about one-shot messaging a bit more.  I really like the idea of using them for update calls but for ICQC they can be tricky.  If user queries canister A; A sends a one shot message to B; and is waiting for a response; the system doesn't know that it is waiting for a response and that it should wait.  

I suppose we will need some mechanism for A to tell the system that it may still produce a response in the above case.

-------------------------

jzxchiang | 2022-01-22 19:55:49 UTC | #35

> For example, it is not compatible with [async Rust API ](https://github.com/dfinity/ic/blob/master/rs/rust_canisters/dfn_core/src/api/futures.rs) because the response closure is stored on the heap, but the heap changes would not be preserved in the proposed design when the response arrives.

Can you explain why this is a problem with cross-subnet queries but not same-subnet queries? I'm not sure I understand why the heap is cleared for cross-subnet but not same-subnet.

-------------------------

akhilesh.singhania | 2022-01-24 08:28:59 UTC | #36

I don't think there will be any difference in the programming model / semantics depending on whether or not you are going cross-subnet or same-subnet.  The async rust API difficulties will present in both cases.

-------------------------

ulan | 2022-01-24 09:02:44 UTC | #37

> Can you explain why this is a problem with cross-subnet queries but not same-subnet queries? I’m not sure I understand why the heap is cleared for cross-subnet but not same-subnet.


What we have currently is a prototype implementation of same-subnet ICQC that *keeps* all state changes in memory until all called inter-canister queries return. The main blocker for this prototype implementation is theoretically unbounded memory consumption (every call kind of forks the chain of state changes). The problem becomes worse with cross-subnet queries that have much higher latencies.

One way to fix the memory consumption problem is to change the semantics of the calls to discard the state changes. But that makes the programming model difficult to use (async rust API problem). The problem applies to same- and cross-subnet calls equally as @akhilesh.singhania mentioned.

-------------------------

jzxchiang | 2022-01-24 22:51:02 UTC | #38

Let's say you have an ICQC that's made in the context of a query.

Queries already discard state updates to canisters.

Are you saying that even local variables will be discarded within the context of a query that makes an ICQC?

-------------------------

ulan | 2022-01-25 09:34:10 UTC | #39

> Queries already discard state updates to canisters.

A query discards the state updates after its execution is fully complete. What to do with the state updates when the query has a pending call (i.e. the query performed the call, but its response callback did not run yet) is an open question. Discarding the state changes leads to a confusing programming model.

> Are you saying that even local variables will be discarded within the context of a query that makes an ICQC?

I guess you mean the local variables in async code while awaiting for the result of a call. The local variables are stored in memory across the await points, so discarding the state changes would also discard the local variables. Even worse: the implementation of async/await in Rust stores internal information in memory, so discarding the state changes would make awaiting impossible (that's what I meant by "not compatible with async Rust API" earlier).

-------------------------

jzxchiang | 2022-01-25 23:45:42 UTC | #40

Thanks, this makes sense.

When I make an inter-canister update call, the IC runtime is able to save (i.e. fork) the current state of the canister at the time the call is made, and then resume from that state when the call returns.

Is that difficult to apply that same code to inter-canister query calls? You mention unbounded memory consumption as a blocker. But what I don't quite understand is how the IC's implementation of inter-canister update calls was able to avoid that problem? It seems to work fine for both same-subnet and cross-subnet updates.

-------------------------

akhilesh.singhania | 2022-01-26 10:19:22 UTC | #41

The key insight here is that the update calls are happen "on-chain" whereas query calls are happening off-chain.  

If you think about the blockchain as a git repo, each time we execute an update call on a canister, we are adding a new commit to the main branch of the repo.  

Each time we are executing a query (or a ICQC), we are creating a branch in the repo.  We know that we will eventually discard the whole branch (when the ICQC is done) but while the ICQC is ongoing, we have to maintain it so that we can serve the calls properly.  And then if we imagine that there are a bunch of ICQCs happening in parallel and the call graphs are quite complex or deep, then that is a lot of state that the node has to maintain.  

That's why the approach of not maintain intermediate state during the ICQC will help in significantly reducing the amount of resources required to serve ICQCs.

-------------------------

jzxchiang | 2022-02-08 08:03:14 UTC | #42

Somewhat tangential but perhaps related, I noticed that "F+1 query calls" is on the [roadmap](https://medium.com/dfinity/the-internet-computers-post-genesis-r-d-roadmap-dce2938adcde) for a Q4 2022 release.

IIUC, F := upper bound on the # of corrupt replicas.

My understanding is that ingress update calls (and inter-canister calls) currently go through N - F consensus, where N := # of replicas in a subnet, whereas ingress query calls don't go through any consensus.

Since N - F consensus is a higher bar than F + 1 consensus, is it accurate to say that this feature will enable query calls that take longer to execute than current ingress queries but faster to execute than full-blown updates? If so, are inter-canister queries the intended use case for this? If not, what's the purpose of this feature?

(Just throwing out guesses, please let me know if I'm completely off-base.)

-------------------------

jzxchiang | 2022-02-10 06:43:41 UTC | #43

@akhilesh.singhania @ulan

I'm trying to understand the ICQC "state explosion" problem better.

1. Does your same-subnet ICQC prototype use the same input and output queues that regular ingress and inter-canister calls use? In this case, ICQCs would "compete" with the update calls for canister execution. Or do they bypass this queue mechanism?

2. Currently, does a query call spawn a new thread on a replica? (By the way, I'm curious if these threads all still run on a [single](https://forum.dfinity.org/t/canister-load-balancing-community-consideration/6980/3?u=jzxchiang) core, or can they now run on multiple cores?) If so, maybe another approach is to maintain a thread pool that processes query requests from a shared queue? This could limit the impact of a "state explosion" to the size of the thread pool.
a. Then, we could also add a restriction that ICQCs can only be initiated from either an ingress query call or another ICQC. This would reduce the likelihood that a single ICQC monopolizes a thread (and all of the threads depending on it).
b. Another idea is to put a hard cap on query execution time. If that cap is exceeded, that query—and all its child ICQCs—fail, and an error is returned to the client.
c. One more idea: while a query is waiting on an ICQC to return a response, that thread could execute another query in the meantime. Just like how the JavaScript event loop deals with async function calls.

WDYT? Or am I misunderstanding the problem?

-------------------------

akhilesh.singhania | 2022-02-10 14:18:42 UTC | #44

Queries and updates use complete different mechanisms.  The problem is about holding on to dirty state of canisters while they wait for replies.

First let's consider update calls.  If canister executes an update call at state A and modifies its state to A', the new state is committed and the state of the canister now becomes A'.  All future update calls will run again A'.  The system is free to drop A whenever it is convenient for it.

And when you consider the current situation without ICQC, we see that queries will either run against state A or state A'.  So when state A' is available, the system can let new queries run against A' and when the last queries running against A finish, the system can safely drop it.  

Note that currently without ICQC and with upper limits on how long queries can run for, we have a simple upper bound on how long we need to hold onto older states of canisters.

# Problem 1 with ICQC

Let's say the user queries canister 1 at state A.  Canister 1 sends ICQC to canister 2 and so on.  The system has to hold on to state A of canister 1 till the entire ICQC call graph is finished.  If we allow for call graphs with unlimited depth (which is currently the case with update call graphs), there would be no upper limit on how long the system has to hold onto state A for canister 1.

# Problem 2 with ICQC

The problem above is actually much worse.  Note that when canister 1 executed the query at state A, on top of sending queries to other canisters, it also modified its state to A'.  The system has to hold on to not state A but actually A' because when the replies come back, they are expecting to run against A'.  

This might not seem like a big problem immediately as the number of states that the system is maintaining is not actually changing (A' instead of A).  However, if you consider how the states are maintained in the system, this is a huge problem i.e. we maintain different states of the canister by maintaining deltas from previous versions of the states.  So A' is actually maintained as A + delta.  So what we are seeing here is that each time a canister makes an ICQC, we have to maintain a separate additional delta for that canister.  And the more canisters are involved in a ICQC call graph, the more deltas that system has to maintain.  And without bounding the call graph, we cannot bound the number of deltas we have to maintain.  

Further, note that all the above deltas that are maintained for queries, need to be maintained only temporarily.  Once a ICQC finishes, all the deltas can be dropped.  They also need to have high availability as generally we expect queries to run fairly quickly.  This means that we need to have access to a large amount of storage that also has low latency access behaviour and can be recycled quickly.  All of these are very challenging problems.

I hope the above gives some appreciation of how many different versions of a canister's state (i.e. deltas) we will have to maintain if we implement ICQC naively i.e. with the same guarantees and semantics as we have for update messages.  

This is why @ulan and I are proposing a ICQC design where the replies run against a "clean" state of the canister.  With that design, we are not maintaining additional canister states that we have to maintain today.  This will make programming with ICQC more difficult as has already been discussed above.

-------------------------

rossberg | 2022-02-10 16:36:39 UTC | #45

[quote="akhilesh.singhania, post:44, topic:6754"]
This is why @ulan and I are proposing a ICQC design where the replies run against a “clean” state of the canister. With that design, we are not maintaining additional canister states that we have to maintain today. This will make programming with ICQC more difficult as has already been discussed above.
[/quote]

Hm, "more difficult" sounds a bit like an understatement. Do we have a story for how such a semantics could be reconciled with async in any form or shape?

-------------------------

jzxchiang | 2022-02-11 07:30:05 UTC | #46

> Note that currently without ICQC and with upper limits on how long queries can run for, we have a simple upper bound on how long we need to hold onto older states of canisters.

I wasn't aware there currently was an upper limit on how long a query can run for. Is that limit measured in time or cycles?

> If we allow for call graphs with unlimited depth (which is currently the case with update call graphs), there would be no upper limit on how long the system has to hold onto state A for canister 1.

Maybe we should add an upper limit then? For example, a query and any ICQCs that it kicks off can only run for a grand total of X seconds (or cycles).

> And the more canisters are involved in a ICQC call graph, the more deltas that system has to maintain. And without bounding the call graph, we cannot bound the number of deltas we have to maintain.

Why don't we just impose a limit on the # of concurrent queries a single replica can process (and add pending queries to a queue)? For example, executing queries on a per-replica thread pool will naturally impose a limit. The call graph doesn't need to be bounded then. Every canister's replicas in the chain of ICQCs are responsible for limiting their own query concurrencies. Together with a total time/cycle limit on a query, this should prevent the state explosion problem, I think?

-------------------------

akhilesh.singhania | 2022-02-11 08:24:10 UTC | #47

[quote="rossberg, post:45, topic:6754"]
Hm, “more difficult” sounds a bit like an understatement. Do we have a story for how such a semantics could be reconciled with async in any form or shape?
[/quote]

We don't think it will be possible.  If you do not see a mechanism either, then this will indeed be very tricky.  Maybe canisters cannot use the async paradigm for ICQC.

-------------------------

akhilesh.singhania | 2022-02-11 08:29:03 UTC | #48

[quote="jzxchiang, post:46, topic:6754"]
I wasn’t aware there currently was an upper limit on how long a query can run for. Is that limit measured in time or cycles?
[/quote]

The limit is in instructions just like there is an upper limit on how long update messages can run for.  Otherwise, a canister can trigger an infinite query and consume resources forever.

[quote="jzxchiang, post:46, topic:6754"]
Maybe we should add an upper limit then? For example, a query and any ICQCs that it kicks off can only run for a grand total of X seconds (or cycles).
[/quote]

We have upper limits on how long a single query can run for.  Putting upper bounds on ICQCs breaks composibility.  Essentially it can mean that a call graph like A -> B -> C -> D may fail but C -> D may succeed.  So you are unable to compose functionality by combining smaller functionalities together.  

[quote="jzxchiang, post:46, topic:6754"]
Why don’t we just impose a limit on the # of concurrent queries a single replica can process (and add pending queries to a queue)?
[/quote]

We precisely have this right now.  Currently we can process at most 4 queries in tandem.  We explicitly do not have a queue because having a queue does not help.  What do you do when it is full.

The problem with ICQCs is that when a canister on subnet A sends a ICQC to a canister on subnet B, what do you then on subnet A?  Do block the query thread on subnet A?  If you do, then you create a DoS vector.  If you do not, you need to store the state of A as discussed above.

-------------------------

rossberg | 2022-02-11 10:04:52 UTC | #49

[quote="akhilesh.singhania, post:47, topic:6754"]
We don’t think it will be possible. If you do not see a mechanism either, then this will indeed be very tricky. Maybe canisters cannot use the async paradigm for ICQC.
[/quote]

At an absolute minimum, the API would need to enable attaching temporary out-of-band state (of arbitrary size) to each ICQC, which the canister can query upon receiving the reply.

Because if there was no way for temporary state to survive the ICQC (other than just the i32 env value supplied), then for most practical purposes, ICQC's would be a completely useless feature – a canister would not be able to put the reply it receives into any useful context.

-------------------------

akhilesh.singhania | 2022-02-11 10:14:05 UTC | #50

That makes sense.  Maybe there is a way for a canister to hand off some state to the replica to hold on to till the reply comes back.

-------------------------

jzxchiang | 2022-02-25 02:26:19 UTC | #51

> We precisely have this right now. Currently we can process at most 4 queries in tandem. We explicitly do not have a queue because having a queue does not help. What do you do when it is full.

I'm curious if 4 is temporary and will increase, or is that there to stay?

> The problem with ICQCs is that when a canister on subnet A sends a ICQC to a canister on subnet B, what do you then on subnet A? Do block the query thread on subnet A? If you do, then you create a DoS vector. If you do not, you need to store the state of A as discussed above.

Good question, although I do wonder why that's not an issue with same-subnet ICQCs? In both cases there is a latency (although it's exacerbated with cross-subnet).

I'd assume you would, as you said, hand off some state to the replica to hold onto in some kind of "heap". And if that heap is full on that replica, then the canister can respond to the query call with a 50x error. (Or if the user is fine with throwing away state *or* does not use ICQCs, then the canister can still process the query.)

-------------------------

ulan | 2022-03-17 12:45:42 UTC | #52

I summarized the current state of ICQC and the trade-off space in [this document](https://docs.google.com/document/d/1866YDk9oQOujyFFcjDT9q2zRO25brPFK_xg3Yp6e2mo/edit?usp=sharing).

Feedback and comments are very much appreciated!

-------------------------

ulan | 2022-03-18 11:34:27 UTC | #53

Pasting the contents of [the document](https://docs.google.com/document/d/1866YDk9oQOujyFFcjDT9q2zRO25brPFK_xg3Yp6e2mo/edit#) here for better readability.

## Background

The Internet Computer (IC) has two types of messages: updates and queries. As shown in the table below, queries are fast because they are read-only and don’t have to go through consensus.

||Update|Query|
| --- | --- | --- |
|State changes persisted|✔|✘|
|Goes through consensus|✔|✘*|
|Low latency, high throughput|✘|✔|
|Inter-canister calls|✔|✘**|

(*) A user may choose to run a query in an update context, where queries are executed on all replicas along with update messages. The state changes of queries are discarded, but the results go through consensus. We will refer to queries as replicated and non-replicated depending on whether they run in an update context or not. Replicated queries should not be confused with certified queries that also go through consensus but not in an update context. Note that certified queries currently exist only as an idea and are not implemented.

An update message can call methods of other canisters even if they are located on different subnets. Such inter-canister calls are essential for composing canisters into scalable applications.

(**) Queries do not support inter-canister calls. There is an incomplete implementation of ICQC that is enabled on verified subnets, but it is not generally available for the reasons explained below.

## Requirements

The main requirement for ICQC is consistency with the existing inter-canister calls in update messages. Developers already know how inter-canister calls work and have certain expectations. Specifically:

A. [async/await] A call in a query should have the same semantics as a call in an update message. More concretely, it should be possible to support async/await for query calls.
B. [caller-id] The callee of an inter-canister call should be able to read and trust the identity of the caller.
C. [replicated mode] A call in a query should work both in replicated and non-replicated modes.
D. [cross-subnet] It should be possible to call a canister in another subnet.

In addition to the four consistency requirement above, we want queries to remain fast and to not consume a lot of memory:

E. [fast execution] Queries are faster than update messages because they don’t need to keep track of modified memory pages. Ideally, ICQC does not regress this.
F. [low memory usage] Ideally, ICQC does not introduce additional memory overhead.

Note that requirements E and F are important for stability and availability of IC.

## Prototype Implementation

Verified application subnets have an incomplete prototype implementation of ICQC. The prototype was implemented under time pressure before the launch of IC. There was not enough time to think through the design and to write a specification. The prototype satisfies only two requirements: [async/await] and [caller-id]. Other requirements are not satisfied:

* it works only in non-replicated mode,
* it works only for same-subnet calls,
* it is up 2x slower because of the need to keep track of modified pages.
* it has to hold on to the state until all calls complete, so in some cases it may double the memory usage of the replica.

## Trade-off Space

The bad news is that the requirements are in conflict with each other. We identified two trade-off pairs:

1. [async/await] vs [replicated mode, fast execution, low memory usage].
2. [caller-id] vs [cross-subnet].

In each trade-off pair we can choose only one alternative. For example, the prototype implementation corresponds to [async/await] + [caller-id]. It seems that [async/await] is non-negotiable. Sacrificing it would result in a strange, non-intuitive programming model. Given that, the only other viable combination is [async/await] + [cross-subnet], where all inter-canister query calls are effectively anonymous/public.

## Explanation of Trade-off 1

Consider the following generic async query function that calls another query:
```
async fn query_foo(input: Input) -> Output {
    let data = pre_process(input);
    let result = call(canister_bar, "query_bar", data).await;
    post_process(result)
}
```

It prepares some data, issues a call to another query, awaits the result, and finally returns a processed result. IC executes the functions as two separate messages. The first message runs from the start of the function to the await point. At the await point, the runtime of the language (Rust/Motoko) is going to save all necessary information in the WebAssembly memory (future, task, environment) such that when the reply message of the call comes back, the execution can continue from the await point to the end of the function.

The crucial part here is “save all necessary information in memory”, which means that the state changes are important and can no longer be discarded. Thus, a query becomes similar to an update message until the call completes. Figure 1 visualizes the effect of ICQC on canister states. Normally, the state of a canister evolves linearly, changing only from one round to the next. A query that supports ICQC introduces a branch in the chain of states. This doesn’t work in replicated mode because the replicated state supports only one state per canister. The need to keep track of state changes makes execution slower and increases memory usage.

![\ 610x291](upload://hhc2SHGGblMSLY2eTBA2Rg6husE.png)

Figure 1. A query call creates a branch in the linear chain of canister states.

## Explanation of Trade-off 2

All messages in IC are signed by the key of the sender, which can be either a user or a subnet. Figure 2 shows the scenario where a user sends a non-replicated query to a single replica. If the query calls another canister on another subnet, then that message cannot not be signed because the replica does not have the key of the subnet (by design). This means that the callee cannot trust the id of the caller and has to treat the incoming query as anonymous or public.

![\ 610x224](upload://cZGqjeSAlnz7tiifNqE7e4azPFF.png)

Figure 2. The user sends a non-replicated query to one replica in the first subnet, which in turn sends an inter-canister query to a replica in another subnet.

## Conclusion

Based on the trade-off space, we have the following options:

1. ICQC in non-replicated mode and only on the same subnet. This corresponds to the existing prototype implementation. If we go with this option, then we would enable the prototype everywhere.
2. ICQC in non-replicated mode without caller-id. In this case the callee canister has to assume that the request is anonymous and respond only with publicly visible data, which greatly reduces the utility of ICQC.
3. Do not support ICQC. Developers would have to move logic that calls other canisters to the client side.

-------------------------

jzxchiang | 2022-03-17 22:31:54 UTC | #54

Thanks for the detailed post―very insightful. I wasn't aware of the [caller-id] vs [cross-subnet] trade-off until reading this.

> Note that certified queries currently exist only as an idea and are not implemented.

I was under the impression that [certified](https://smartcontracts.org/docs/base-libraries/CertifiedData.html) queries have been available this entire time. If so, then I don't see the benefit of replicated ingress queries, as users could instead choose to make certified queries, which are faster than but still as secure as replicated queries.

> Note that requirements D and E are important for stability and availability of IC.

Did you mean requirements 5 and 6? I don't see D or E anywhere.

> It seems that [async/await] is non-negotiable. Sacrificing it would result in a strange, non-intuitive programming model.

In that case, how will you solve the unbounded memory problem that you've identified as a major concern? You mentioned it may double the memory usage, but is that due to a hard cap that you will implement as part of this proposal or due to other reasons?

It's also interesting that ICQC will be 2x slower...

> Figure 1. A query call creates a branch in the linear chain of canister states.

In this figure, isn't 2' equal to 2? Because until Canister B responds, the forked query has suspended execution.

-----

Based on your analysis, I think option 1 is the most sensible. Since developers cannot currently choose which subnet to deploy their canister onto AFAIK, I think implementing that feature would be a blocker for this.

-------------------------

ulan | 2022-03-18 10:38:49 UTC | #55

[quote="jzxchiang, post:54, topic:6754"]
I was under the impression that [certified](https://smartcontracts.org/docs/base-libraries/CertifiedData.html) queries have been available this entire time.
[/quote]
These are certified variable. There is also an idea of certified or secure queries where queries would run on some nodes that then reach consensus among themselves without that being part of the blocks. This would ensure that the result of the query can be trusted even without certified variables. 

> Did you mean requirements 5 and 6? I don’t see D or E anywhere.

Thanks fixed. Letters were lost when copying the document contents.

> In that case, how will you solve the unbounded memory problem that you’ve identified as a major concern? You mentioned it may double the memory usage, but is that due to a hard cap that you will implement as part of this proposal or due to other reasons?

Yeah, we have to introduce some hard cap and abort very long-running queries. If we only support same-subnet calls then this is less of a problem because we don't have network latency.

> In this figure, isn’t 2’ equal to 2? Because until Canister B responds, the forked query has suspended execution.

2' contains changes in the memory (futures/tasks/environment) necessary to continue execution of the async query method after the await point.

> Based on your analysis, I think option 1 is the most sensible. Since developers cannot currently choose which subnet to deploy their canister onto AFAIK, I think implementing that feature would be a blocker for this.

I am leaning towards option 1 as well.

-------------------------

ulan | 2022-03-18 10:46:29 UTC | #56

[quote="jzxchiang, post:54, topic:6754"]
If so, then I don’t see the benefit of replicated ingress queries
[/quote]

Currently a user can run a query in replicated mode to ensure trustworthy result even without certified variables. If ICQC is only available in non-replicated mode, then we lose that nice property.

-------------------------

rossberg | 2022-03-18 11:13:43 UTC | #57

Thanks for the summary, @ulan!

It's not overly surprising that caller id's are in the way. Essentially, caller id and access checking by looking at caller id's are a form of security by call stack inspection, which has long been a known source of problems, e.g., in the JVM or JavaScript. Needless to say that this could be retired if the IC adopted a modern, capability-based access model. :)

-------------------------

jzxchiang | 2022-03-19 01:05:20 UTC | #58

If option 1 precludes the possibility of replicated queries, does it still allow for the possibility of certified queries, if and when that gets implemented?

Also, I hate to ask this, but assuming option 1 is favored by the community (perhaps in a proposal vote), when do you expect ICQC to be generally available? Would it be in a matter of 1-2 months or much much longer?

-------------------------

icme | 2022-03-19 18:05:41 UTC | #59

[quote="ulan, post:53, topic:6754"]
1. ICQC in non-replicated mode and only on the same subnet. This corresponds to the existing prototype implementation. If we go with this option, then we would enable the prototype everywhere.
[/quote]

Then what happens when a multi-canister application making these non-replicated queries scales beyond a subnet? Do the queries break, or pursue a different mechanism if on different subnets (i.e. a fallback)?

[quote="ulan, post:53, topic:6754"]
2. ICQC in non-replicated mode without caller-id. In this case the callee canister has to assume that the request is anonymous and respond only with publicly visible data, which greatly reduces the utility of ICQC.
[/quote]

As a “hack” to get around this “publicly visible data access” issue, would this developer defined solution work instead of relying on the IC? 

For example, what if the developer has the caller canister contract code generate a key that can be sent to the callee, and then the caller and callee canister can both store this key and agree upon it/update it as needed. Then anytime the caller queries the callee it would send this key as a function parameter, and the callee would verify that all incoming queries have this key before executing.

An improved version of this same idea would be instead of just a single key to use a public/private key pair that is used, where the callee has the public key and a transformation function, and the caller sends the private key as a function parameter.

-------------------------

nomeata | 2022-03-20 22:12:05 UTC | #60

About the second trade off: isn't it good enough to have the _replica_ of the calling canister sign with it's node key, and the called replica check that this is indeed a signature of _one_ replica of the subnet responsible for the calling canister? The called replica should have the necessary information in the registry.

It seem that the worst that can happen is that a malicious replica on subnet of canister X can obtain data that is confidential and should only be visible to X. But we are _already_ trusting that replica to keep the state of X confidential, so this doesn't seem to weaken the IC's guarantees substantially, does it?

-------------------------

ulan | 2022-03-21 11:27:15 UTC | #61

[quote="jzxchiang, post:58, topic:6754"]
If option 1 precludes the possibility of replicated queries, does it still allow for the possibility of certified queries, if and when that gets implemented?
[/quote]

There is no concrete design for certified queries, so I am going to speculate here. If we disallow cross-subnet calls, then I don't see any blockers. I think the existing prototype would work also for certified queries. If we support cross-subnet calls, then we probably need multiple rounds of consensus, which would increase the complexity of certified queries but at the same time would allow signing of the cross-subnet messages (as @nomeata noted in the very first comment of this thread).

> Also, I hate to ask this, but assuming option 1 is favored by the community (perhaps in a proposal vote), when do you expect ICQC to be generally available? Would it be in a matter of 1-2 months or much much longer?

In terms of engineering work 1-3 SWE-months sounds like the right ballpark. Please note that this doesn't directly translate to real months because there are other high-priority projects queued up and the teams are overloaded.

**Some of the remaining work**:

Since the calls are limited to the same subnet, we would need some mechanism to create canisters on the same subnet and also keep them together in case of [subnet splitting](https://forum.dfinity.org/t/long-term-r-d-subnet-splitting-proposal/9402/4). I think this is the main blocker. 

We need to consider whether we want to introduce some way of charging for queries because the execution cost of _all_ queries is going to increase significantly. That's because we don't know beforehand whether a query is going to call another query or not, so we would have to keep track of modified memory for all queries. (We could also introduce some annotation for developers to indicate that a query is going to use ICQC, but that seems a bit ad-hoc to me).

We need to introduce limits to guarantee that all queries terminate within a reasonable deadline.

-------------------------

ulan | 2022-03-21 12:00:56 UTC | #62

[quote="icme, post:59, topic:6754"]
Then what happens when a multi-canister application making these non-replicated queries scales beyond a subnet?
[/quote]

The cross-subnet query calls would indeed fail if we go with option 1. I don't see a working fallback here.

> For example, what if the developer has the caller canister contract code generate
  a key that can be sent to the callee, and then the caller and callee canister can
  both store this key and agree upon it/update it as needed. 
  An improved version of this same idea would be instead of just a single key to use
  a public/private key pair that is used, where the callee has the public key and a
  transformation function, and the caller sends the private key as a function parameter.

Thanks for sharing the idea! I am assuming you meant that the caller signs the message using the private key and sends the signature instead of sending the private key. I think it would work and seems to have somewhat similar security properties as the scheme proposed by @nomeata (for the case when a malicious replica obtains the private key):

[quote="nomeata, post:60, topic:6754"]
It seem that the worst that can happen is that a malicious replica on subnet of canister X can obtain data that is confidential and should only be visible to X. But we are *already* trusting that replica to keep the state of X confidential, so this doesn’t seem to weaken the IC’s guarantees substantially, does it?
[/quote]

Good point! I haven't considered it from this angle. I think the answer depends on the security/threat model. In theory we trust the replica to keep the state of X confidential, but we can never exclude bugs in the implementation. In the very worst case the replica exploits such bugs and manages to leak the confidential data. Do we want to limit the damage to the subnet of the malicious replica or allow it to spread to other subnets?

I like having the subnet boundary as an additional security barrier, but may be I am too paranoid here. I wonder if the security folks have an opinion here. @robin-kunzler

-------------------------

nomeata | 2022-03-21 22:04:47 UTC | #63

[quote="ulan, post:61, topic:6754"]
That’s because we don’t know beforehand whether a query is going to call another query or not, so we would have to keep track of modified memory for all queries
[/quote]

… or run the opportunistically in the fast, non-tracking mode, and if they do make calls, abort and re-run in the memory-tracking mode. If the fraction of non-calling queries is low enough (as it likely will), determinism allows all kind of neat optimizations :-)

-------------------------

nomeata | 2022-03-21 22:06:05 UTC | #64

[quote="ulan, post:62, topic:6754"]
Do we want to limit the damage to the subnet of the malicious replica or allow it to spread to other subnets?
[/quote]

It doesn’t really “spread“ in the sense that it is still only data accessible to X that is under threat – data owned by unrelated canister Y is unaffected. The data may be living somewhere else, but conceptually, not a huge big deal – or at least small enough to not sacrifice the programming model for it :-)

-------------------------

ulan | 2022-03-22 09:12:59 UTC | #65

[quote="nomeata, post:63, topic:6754"]
or run the opportunistically in the fast, non-tracking mode, and if they do make calls, abort and re-run in the memory-tracking mode.
[/quote]

Yep, that's a neat optimization and I am implemented that a while ago :D https://github.com/dfinity/ic/blob/master/rs/execution_environment/src/query_handler/query_context.rs#L180
The tradeoff is that queries using ICQC become even slower, so we might have to disable the optimization if popular canisters start relying on ICQC (not sure how likely it is). 

[quote="nomeata, post:64, topic:6754"]
It doesn’t really “spread“ in the sense that it is still only data accessible to X that is under threat – data owned by unrelated canister Y is unaffected.
[/quote]
@manu made a good point in our offline discussion yesterday: since a malicious replica can forge any query, it could be the case that the query would be impossible in a valid execution. For example, if the caller performs the access check and the callee trusts `caller_id`, then the malicious replica can get data that shouldn't be accessible to X. I know it is a bad idea to have access checks on the caller side, but this is just to highlight that the issue is subtle and trusting `caller_id` may be a security footgun.

-------------------------

nomeata | 2022-03-24 11:44:09 UTC | #66

Hmm, good example. Although it's still within “only data accessible to canister X is at risk ”, even if an uncompromised canister wouldn't usually query that data.

And note that the leak you describe is comparable in impact (actually less) to the callee's subnet having a single compromised node. 

I'm still not convinced that this is worth breaking (or at least worsening) the programming model over a small strengthening of data privacy guarantees in a corner case. It's odd if a normal call has the correct sender, but a call to a query function doesn't.

-------------------------

jzxchiang | 2022-03-25 03:26:43 UTC | #67

> We need to consider whether we want to introduce some way of charging for queries because the execution cost of *all* queries is going to increase significantly. That’s because we don’t know beforehand whether a query is going to call another query or not, so we would have to keep track of modified memory for all queries. (We could also introduce some annotation for developers to indicate that a query is going to use ICQC, but that seems a bit ad-hoc to me).

My personal opinion is that having an annotation wouldn't be too bad in terms of developer experience if it can save them some cycles.

I think charging for queries may change the decision calculus for voters. Do you plan on submitting a proposal for Option 1 before implementation? I think it'd be nice to get community feedback on whether this tradeoff is worth it. (A proposal with a timer also makes it more likely for people to read and comment.)

-------------------------

ulan | 2022-03-28 08:18:07 UTC | #68

> Do you plan on submitting a proposal for Option 1 before implementation? 

Absolutely! We are currently looking into the first blocker: a way to group canisters. We will propose that as a separate feature because it is useful for subnet splitting.

-------------------------

icme | 2022-04-02 17:20:18 UTC | #69

[quote="ulan, post:62, topic:6754"]
The cross-subnet query calls would indeed fail if we go with option 1. I don’t see a working fallback here.
[/quote]

Because of this limitation, I’m pretty firmly against Option 1. If an app on the IC is massively successful or runs into a crowded subnet, it makes for a difficult breaking point in the future, where an successful app now needs to plan huge data migrations on the IC - that sounds rough!

-------------------------

Zane | 2022-04-06 15:59:11 UTC | #70

>this could be retired if the IC adopted a modern, capability-based access model

What's blocking you from achieving that? Technological constraints or ideological ones?

-------------------------

jzxchiang | 2022-04-17 06:10:02 UTC | #71

@ulan Do you plan on submitting a proposal for this, or is this still lower priority than other projects?

-------------------------

ulan | 2022-04-17 08:03:34 UTC | #72

We are working on a proposal to group canisters first. The proposal of ICQC will come after that.

-------------------------

Zane | 2022-04-25 13:37:15 UTC | #73

I personally don't like option 1 and would rather have what @nomeata proposed, one of the selling points of IC's subnets is they are transparent to devs, if we go with option 1 that would no longer be the case.

-------------------------

ulan | 2022-05-01 09:12:00 UTC | #74

Thanks for the input @Zane! Since each option has its own disadvantages, I think the community will settle this by voting.

-------------------------

jzxchiang | 2022-07-01 05:56:40 UTC | #75

Did we ever get a chance to vote on an ICQC proposal?

-------------------------

ulan | 2022-07-01 07:15:17 UTC | #76

> Did we ever get a chance to vote on an ICQC proposal?

Sorry not much progress on ICQC itself. @bogwar is working on the canister groups proposal, which turned out to be much more difficult than we anticipated. Option 1 depends on the outcome of that. 

I am currently working on the deterministic time slicing. Other people who could help with ICQC are busy with the BTC integration. Once both projects ship, there will be faster progress on ICQC.

-------------------------

ulan | 2022-09-28 14:16:27 UTC | #77

I will give an update in the public Global R&D today: https://internetcomputer.org/live-sessions/ and a more detailed technical presentation in Scalability & Performance WG on October 20: https://forum.dfinity.org/t/technical-working-group-scalability-performance/14265/19

The plan is to release ICQC incrementally:

1. Get the existing prototype into a production ready state with two main limitations: no support for cross-subnet calls and no support for replicated execution.
2. Work on adding cross-subnet support. The main challenge here as mentioned before is the `caller_id` problem. We either need to find a solution to ensure its trustworthiness or accept the reduced trustworthiness. Another challenge is to rewrite the prototype to use more complex async/distributed algorithm.
3. Work on replicated execution. This step may be infeasible. To be on the safe side, we need to introduce a new query type for ICQC that doesn't allow replicated execution before we release the prototype.

After gathering feedback here and in the Scalability & Performance WG session, I'll prepare and submit a motion proposal with the ICQC roadmap.

-------------------------

Zane | 2022-09-28 14:57:15 UTC | #78

Thanks for the update

[quote="ulan, post:77, topic:6754"]
no support for replicated execution
[/quote]

I might be missing something, aren't queries already not replicated? Does it mean we can only use ICQC to serve data to users and not to get data from another canister and then run replicated logic on it?

-------------------------

JaMarco | 2022-09-28 15:15:30 UTC | #79

"(*) A user may choose to run a query in an update context, where queries are executed on all replicas along with update messages. The state changes of queries are discarded, but the results go through consensus. We will refer to queries as replicated and non-replicated depending on whether they run in an update context or not. Replicated queries should not be confused with certified queries that also go through consensus but not in an update context. Note that certified queries currently exist only as an idea and are not implemented."

-------------------------

cryptoschindler | 2022-09-29 06:37:39 UTC | #80

[quote="JaMarco, post:79, topic:6754"]
certified queries that also go through consensus but not in an update context
[/quote]

I thought they only pass the certificate through along with the data, at what place do they go through consensus? Maybe it's the "but not in an update context" part  that explains that, although I'm not sure what it means.

-------------------------

JaMarco | 2022-09-29 13:04:06 UTC | #81

Your're talking about certified variables

-------------------------

cryptoschindler | 2022-09-29 14:40:59 UTC | #82

True, I mixed that up. 

What's a certified query though? Is there any writeup on the idea or is it the concept discussed here

https://forum.dfinity.org/t/discussion-data-certification-on-ic/15379?u=cryptoschindler


Where are you quoting from?

-------------------------

JaMarco | 2022-09-29 15:10:21 UTC | #83

https://forum.dfinity.org/t/inter-canister-query-calls-community-consideration/6754/53?u=jamarco

-------------------------

ulan | 2022-10-18 16:13:29 UTC | #84

Hi folks. In preparation for the NNS proposal I would like to share our current understanding of the problem. The main change since [my previous post](https://forum.dfinity.org/t/inter-canister-query-calls-community-consideration/6754/53) about the trade-offs is that we no longer think that `[async/await]` conflicts with `[replicated mode]` because supporting replicated execution for ICQC seems feasible (even though very difficult).

-----

## Background & Concepts

### Execution mode

A message on the IC can be executed in two different modes: replicated and non-replicated. The following table summarizes the differences between them.

|Replicated execution|Non-replicated execution|
| --- | --- |
|High-latency|Low-latency|
|Runs on all nodes|Runs on a single node|
|Goes through consensus|Doesn’t go through consensus|
|Result is signed by the subnet|Result is signed by the node|

Sidenote: there is a third mode that currently exists as only an idea: run non-replicated execution on at least n/3+1 nodes. This mode is known as certified/secure/repeated execution.

### Canister method types

Canisters have two types of methods: updates and queries. The following table summarizes the differences between them.

|Query|Update|
| --- | --- |
|Read-only|Modifies state|
|Isolated from other queries|Sees changes of other updates|
|Supports all execution modes|Replicated execution only|
|No calls|Cross- and same-subnet calls|

The second property - query isolation - is crucial for reasoning about ICQC. It means that state changes made by one query are not visible to the other queries. The property currently holds trivially for queries because they are read-only and discard all state changes after the execution.

## Objective & Requirements

Our goal is to allow a query method to call other query methods of the same or other canisters. Let’s refer to the new queries that have this ability to make calls as “ICQC queries” and to the existing queries without calls as “regular queries”.

What makes ICQC queries really challenging to implement is that they combine the most difficult properties of regular queries and updates as shown in the following table:

|ICQC query|Regular query|Update|
| --- | --- | --- |
|Modifies state|Read-only|Modifies state|
|Isolated from other queries|Isolated from other queries|Sees changes of other updates|
|Supports all execution modes|Supports all execution modes|Replicated execution only|
|Cross- and same-subnet calls|No calls|Cross- and same-subnet calls|

In the following sections we discuss each of these properties in detail.

## State modification

In order to support async/await, an ICQC query has to keep canister state changes until all pending calls return. In other words, an ICQC query behaves like an update while the calls are pending. Once all calls return, then the state changes are discarded and the ICQC query behaves like a query.

![\ 610x300](upload://vcMIm2GBpwvO4stlHaznQcnMqVV.png)

## Query isolation

Query isolation means that effects of one ICQC query execution are not visible to other ICQC and regular queries. To see why this property is important, consider the following valid ICQC query that calls another query and then destroys the state of the canister.

```

async fn query_foo(input: Input) -> Output {
  let result = call(bar, "query", data).await;
  destroy_all_global_state(); // No problem for other queries.
  return result;
}
```
Other queries should work without any problems after this ICQC query finishes.

What follows from the query isolation and state modification properties is that we need to clone or copy the canister state in order to execute an ICQC query. If the ICQC query calls another ICQC query, then we need to clone the other canister state as well.

In general, if we have a call graph where nodes are canisters and edges are queries, then canisters will be cloned as many times as the number of ICQC query edges in the graph:
**![|610x381](upload://6B5TqDfLNcx1qi3KpnVVcBfRbZj.png)**

## Replicated execution

Currently the replicated state contains each canister exactly once. Conceptually we can think of it as a mapping from the canister id to canister state: `[CanisterId → CanisterState]`.

In order to support replicated execution of ICQC queries, we need a way to keep multiple versions of the same canister in the replicated state. This means that our mapping becomes something like `[CanisterId → CallContextId → CanisterState]` where `CallContextId` corresponds to an ICQC query with pending calls.

To implement this, we would need to change the core components of the IC such as the state manager, message routing, and execution. That would be a large engineering effort (1 - 2 years) with a lot of complexity and unknowns.

## Cross-subnet calls

In order to support cross-subnet calls, we would need to rewrite the existing prototype implementation to use an asynchronous/distributed algorithm to traverse the call graph. That is a medium to large size engineering problem.

The question of the [caller_id](https://forum.dfinity.org/t/inter-canister-query-calls-community-consideration/6754/53#explanation-of-trade-off-2-6) remains open. We either need to find a solution to ensure its trustworthiness or accept the reduced trustworthiness.

## Conclusion & Next steps

Our recommendation is to release the existing prototype implementation as a new ICQC query type without the support for cross-subnet calls and replicated execution and then work on those two features separately. The new query type ensures that we don’t break the existing queries that may be used in replicated mode.

A draft of a motion proposal describing this plan will be shared soon.

-------------------------

cryptoschindler | 2022-10-18 19:01:29 UTC | #85

Why can we even call a queries in replicated mode? Shouldn't it be up to the canister controller to decide be specifying either a query or "normal" function?

-------------------------

ulan | 2022-10-19 08:49:38 UTC | #86

> Shouldn’t it be up to the canister controller to decide be specifying either a query or “normal” function?

Absolutely! The owner of the canister decides whether a method is a query or an update by annotating it correspondingly in the Wasm source code: queries are exported as `canister_query <query_name>` and updates as `canister_update <update_name>`.

That's said, it is possible and sometimes useful to run a query in replicated mode. The query still keeps its query semantics in the sense that it is still read-only and discards state changes, but the execution happens on all nodes of the subnet. That means that the result of execution goes through consensus and is signed by the subnet key, so the result is more trustworthy compared to non-replicated execution.

Another case when a query runs in replicated mode is when an update method calls a query method.

-------------------------

cryptoschindler | 2022-10-19 08:55:19 UTC | #87

But can the caller trigger running a query in replicated mode? If so, doesn't that open the gates for some cycle draining attacks? I know that there are plans to charge for queries in the future, but still I'd assume running a query in replicated mode is more expensive than non replicated mode.

-------------------------

ulan | 2022-10-19 09:02:18 UTC | #88

> But can the caller trigger running a query in replicated mode?

If the caller is running as an update method, then yes. It has been always possible to call a query from an update. If the query performs some very expensive computation, then there is a potential for a cycle draining attack. One way to protect against it would be to inspect the `caller_id` and refuse to do the expensive computation. Ideally, queries are fast and do not perform expensive computations.

-------------------------

ulan | 2022-10-19 11:05:37 UTC | #89

I posted the draft of the motion proposal about the new query type and releasing the prototype implementation here:
https://forum.dfinity.org/t/proposal-composite-queries/15979

-------------------------

jzxchiang | 2022-10-23 16:27:23 UTC | #90

Thank you for the excellent write-up!

> What follows from the query isolation and state modification properties is that we need to clone or copy the canister state in order to execute an ICQC query. If the ICQC query calls another ICQC query, then we need to clone the other canister state as well.

Once this canister cloning feature is built, does that mean it will unlock the possibility for a broader change in async/await semantics for update calls? For example, update calls [currently](https://internetcomputer.org/docs/current/developer-docs/build/cdks/motoko-dfinity/actors-async#traps-and-commit-points) commit canister state whenever they initiate an inter-canister call (to either a query or update method). That limitation can be quite troublesome for canister developers, as they need to worry about state rollbacks and non-atomic update methods.

If this feature lands, am I correct in assuming that limitation could (technically) be lifted?

-------------------------

lastmjs | 2023-02-16 21:54:16 UTC | #91

[quote="ulan, post:84, topic:6754"]
## Background & Concepts

### Execution mode

A message on the IC can be executed in two different modes: replicated and non-replicated. The following table summarizes the differences between them.

|Replicated execution|Non-replicated execution|
| --- | --- |
|High-latency|Low-latency|
|Runs on all nodes|Runs on a single node|
|Goes through consensus|Doesn’t go through consensus|
|Result is signed by the subnet|Result is signed by the node|

Sidenote: there is a third mode that currently exists as only an idea: run non-replicated execution on at least n/3+1 nodes. This mode is known as certified/secure/repeated execution.
[/quote]

I'm looking through the interface spec trying to find a canonical location for this information. Where is this documented?

-------------------------

lastmjs | 2023-02-16 21:58:17 UTC | #92

Also how do you call a query in replicated execution mode? Do you just submit a `call` HTTPS API request to a query method?

-------------------------

dsarlis | 2023-02-17 09:17:44 UTC | #93

I actually don't know of a canonical location of this information. It might be a good idea to incorporate it into the interface spec. Or maybe in this [page](https://wiki.internetcomputer.org/wiki/Query_and_update_call_latency) which already describes the differences between update and query calls and we can also include this table comparison.

> Also how do you call a query in replicated execution mode? Do you just submit a call HTTPS API request to a query method?

Correct. The other way is if you make an update call to a canister which then in turn calls another canister's query method -- in that case the query is also executed in replicated mode.

-------------------------

yvonneanne | 2023-02-17 09:30:02 UTC | #94

It’s documented [here](https://internetcomputer.org/docs/current/references/ic-interface-spec#http-call) in the interface spec under Request: Call and Request: Query call. Do you have suggestions how to improve these sections?

> Also how do you call a query in replicated execution mode? Do you just submit a `call` HTTPS API request to a query method?

Correct, that’s all you need to do.

-------------------------

lastmjs | 2023-02-17 17:03:28 UTC | #95

I tried searching through the interface spec for replicated mode, execution modes, or other language that @ulan had shared with his tables and I didn't find anything useful. I think a section dedicated to these modes and discussing how to invoke them would be helpful.

-------------------------

bytesun | 2023-12-01 15:47:10 UTC | #96

this topic is "consideration" status,  https://dx.internetcomputer.org/topic/179  here is done

are they same?

what's the real condition now?

-------------------------

ulan | 2023-12-01 16:55:52 UTC | #97

@bytesun, thanks for raising this point. The feature is done.  More info is available here: https://medium.com/dfinity/composite-queries-horizontal-scaling-for-multi-canister-dapps-e766e62bdea9

@diegop: is it possible to mark this forum thread as "done" somehow?

-------------------------

bytesun | 2023-12-01 17:43:42 UTC | #98

thanks @ulan for the quick response. I feel it's different with what I want.  Please correct me if I am wrong.  

To use this new feature, developer need create a new structure (frontend -> backends), and it's only for partitioned storage scenario, right?

In my case, I have NFT metadata in one canister(A), and assets in another canister(B), I would like one query to get all information(metadata and assets), which means query call A -> query call B, it doesn't look like can be done by this implement, right?

-------------------------

diegop | 2023-12-01 18:31:02 UTC | #99

[quote="ulan, post:97, topic:6754"]
@diegop: is it possible to mark this forum thread as “done” somehow?
[/quote]

I marked it as solved as well

-------------------------

diegop | 2023-12-01 18:31:34 UTC | #100

[quote="bytesun, post:98, topic:6754"]
thanks @ulan for the quick response. I feel it’s different with what I want. Please correct me if I am wrong.
[/quote]

I will let @ulan answer

-------------------------

