diegop | 2021-09-23 04:24:53 UTC | #1

## Summary

The shuffling of node membership in subnets has been brought up a few times by the community as a way to address security risks within the IC. Truth be told, the researchers at the DFINITY foundation have engaged this topic sometimes within other topics, but I do not think we have had a deep, focused conversation with the community on this. We want to correct this.

That is why we decided to create this thread on /roadmap path to understand and discuss with folks as an exploratory conversation and assign a researcher to poll folks and focus on it.

For this conversation, **Manu Drijvers** (@Manu) will be the lead representing the understandings from DFINITY R&D and the owner of this conversation. Manu is a cryptographer and the engineering manager of the DFINITY Consensus team. 

He will both take back to the R&D team the community's ideas as well as help communicate. He will help bridge the gap if you will. In the past, this conversation has been too scattered across many mediums and people so it did not have time to breathe.

### Related links:
- https://www.reddit.com/r/dfinity/comments/nerppg/comment/gyo89qz/?utm_source=share&utm_medium=web2x&context=3
- https://forum.dfinity.org/t/amd-sev-virtual-machine-support/6156/9?u=diegop

## How you can help
- Propose your ideas
- Ask questions
- Like comments

-------------------------

skilesare | 2021-09-22 18:52:59 UTC | #2

I guess one of my first question is why not across subnets?

-------------------------

Manu | 2021-09-22 19:07:29 UTC | #3

Thanks for kicking this off @diegop! 

As Diego mentioned, I would be super curious to hear the community's ideas and opinions on this topic. Currently, the internet computer does not regularly change subnet membership, while we've heard some people suggest that regularly swapping nodes in and out of a subnet. I'd love to hear what people prefer (more static vs more dynamic subnet membership) and how frequently they think nodes should be rotated. I'd also be curious to hear what type of attacks they are most concerned about and how they could be avoided using such mechanisms. 

> I guess one of my first question is why not across subnets?

@skilesare, I think Diego meant switching up subnet membership, so essentially "across subnets": a node could be taken out of one subnet, and added to another subnet.

-------------------------

diegop | 2021-09-22 19:23:51 UTC | #4

[quote="Manu, post:3, topic:7478"]
meant switching up subnet membership,
[/quote]

This is exactly right. I have edited the post and title to better reflect the intent. 

Thank you @skilesare !

-------------------------

Maxfinity | 2021-09-22 20:27:26 UTC | #5

Ethereums fast shuffling requires a stateless client model, where all the state stored in a shard/subnet is authenticated with merkle  trees and then witnesses are used when carrying out new computations. 

A question I have is how long would it take for a node to download all the state of a subnet in Dfinity. Thus will impose practical limits on how fast shuffling can be.

Will you guys consider a hybrid model?

-------------------------

lastmjs | 2021-09-22 22:44:02 UTC | #6

Yes, please offer us more information on the bandwidth and computational requirements of spinning down a node from one subnet and spinning that same node up in another subnet. That will help inform us on the resource hit node shuffling would provide.

-------------------------

diegop | 2021-09-23 14:29:09 UTC | #7

[quote="Manu, post:3, topic:7478"]
I’d also be curious to hear what type of attacks they are most concerned about and how they could be avoided using such mechanisms.
[/quote]

## A slight nudge

If I may be so bold as to nudge the conversation a bit in this direction...

I think the intent of the shuffling memberships is very important to drill down and get clarity & consensus on. This sounds like a tautology so allow me to explain.

At DFINITY, I have been very fortunate to work with world-class cryptographers (not my field of expertise)... and one of the results of this interaction is that for a while we were talking with a different mental model. The main mental model is that cryptographers speak 
in "attacks" and "breaking X." 

To satirize, a common conversation with cryptographer can go like this: 

**Example: 1**
Diego: "*Is protocol X secure?*

Cryptographer: "*No, there is a paper that shows that if someone does {{describes the world's most complicated attack}} then the protocol is broken. So it is considered Broken.*"

Diego: "Oh"

**Example: 2**

Diego: "*Is protocol Y secure?*"

Cryptographer: "*All I know is that there is no known attack to break it that is easier than brute force over 20 years*"

Diego: "Is that the crypto version of YES?"

## Why it matters in this case

It matters because one motivation Jan, @Manu , and I had for creating this thread is that we realized that (being intellectually honest) we were not sure what the attack node shuffling was addressing. This could very well be we have not been paying close enough attention to the community (which we want to recifty) and it's obvious to other folks, but it was not to us. 

Allow me to present some possible candidates of what we thought we heard:

1. **The attack is that someone wants to tamper with canister X**. 

- The idea that node shuffling will make it harder to tamper with canister X by having nodes collude. In this case, if canister X llves in a subnet Y with 7 node providers (but the IC has 100 node providers), then making a conspiracy among the 7 node providers is harder than making a conspiracy among 100 node providers. This is a reasonable take. The next follow-up would be: if this is the goal, why don't we move the canister around instead of the nodes? Not saying this is easier than node membership, but it shows how we were not sure of the attack being mitigated. If the attack were clear, different creative solutions could be presented.

**2. The attack vector is to corrupt subnet Y.**

- In this case, shuffling canisters does not help, it is about corrupting a particular subnet. Then the question I had was this (I admit my thinking is primitive here): If we assume that 1/3 of the nodes in a subnet are malicious... and node memberships are being shuffled constantly, then won't subnet Y eventually end up with a supermajority of malicious nodes? Could shuffling produce a situation where some subnets have 0 malicious nodes and some subnets have 2/3 malicious... and by the law of probability, eventually end up corrupting subnet Y. 

**3. The concern is less "an attack" and more a concern about the number of nodes in most subnets. The goal is to increase resilience.**

- In this case, node shuffling is seen as a way to boost the nodes that need to agree. If this is the case, is node membership a quicker solution to increasing the node count by tapping all of the node providers in the IC?

**4. All of the above!**

- Very possible it is all of the above. This is why we started the thread: Honest recognition we did not know the attacks to address. If we knew, then we could see if there are easier ways to address them... or inversely, it could be there may already be *easier attack vectors* that need to be prioritized higher. We were not sure, to be honest.

Hope that makes sense!

-------------------------

diegop | 2021-09-23 05:12:07 UTC | #8

Curious if @Manu agrees with what I wrote above. 

Always very possible I took away something different than what he intended when he and I chatted. :upside_down_face:

-------------------------

skilesare | 2021-09-23 11:33:59 UTC | #9

I think it is likely that the community member all have to go through a process of rectifying confusion around what a node is vs what a canister is.  I myself had a bit of a brain shift reading your post.  I wasn't thinking clearly about it and I had specific issues I was most concerned with that were clouding my thinking.  Great post!

I'm concerned about one malicious actor having infinite time to rip open a canister and look at the data in my canister.  This is a problem until we have(and with side-channel attacks, maybe even after) we have secure enclaves.  This is #1 in your post.

#2 seems like maybe it could just be fixed by some transparency and some ability for the user to pick their subnet?  Maybe not...and I probably don't understand the node/subnet distinction well enough to comment.

#3.  Resilience is great and should be a long-term goal.  I remember seeing some early designs where users were going to get to pick the number of nodes that would run their canister(with adjusting costs). I understand that was removed for now to reduce complexity, but I can see where that would be a valuable user choice to have.

-------------------------

Manu | 2021-09-23 12:51:56 UTC | #10

[quote="diegop, post:8, topic:7478"]
Curious if @Manu agrees with what I wrote above.
[/quote]

I do! I'm hoping to get a discussion around what type of attacks people are afraid of and what type of attacks shuffling membership could help avoid.

> I think it is likely that the community member all have to go through a process of rectifying confusion around what a node is vs what a canister is.

Good point, let's start with clear terminology. 
*Canister smart contracts* (or just "*canisters*") are the pieces of software that run on the internet computer. The internet computer consists of many *subnets*, and the canisters are distributed over the different subnets. That means that every canister on the internet computer in fact runs on one of the subnets of the IC. 

There are many physical computers powering the internet computer. I call these *nodes* or *replicas*. Nodes/replicas are grouped together to form a subnet. So if nodes 1, 2, 3 and 4 together form subnet A, then we say nodes 1, 2, 3, and 4 are the *members* or subnet A. The members of a subnet together build a blockchain as a means of reaching consensus on which messages for the canisters on this subnet should be executed. The *replicated state* of a subnet is the state that every member node of the subnet should have, which contains the state of every canister on this subnet.

-------------------------

Manu | 2021-09-23 14:33:52 UTC | #11

[quote="Maxfinity, post:5, topic:7478"]
Ethereums fast shuffling requires a stateless client model, where all the state stored in a shard/subnet is authenticated with merkle trees and then witnesses are used when carrying out new computations.

A question I have is how long would it take for a node to download all the state of a subnet in Dfinity. Thus will impose practical limits on how fast shuffling can be.

[/quote]

[quote="lastmjs, post:6, topic:7478"]
Yes, please offer us more information on the bandwidth and computational requirements of spinning down a node from one subnet and spinning that same node up in another subnet. That will help inform us on the resource hit node shuffling would provide.
[/quote]

Great questions. A new node can always catch up to the latest state of a subnet, this is already present in internet computer today. The "catch-up package" (or "CUP") signs the full replicated state, so a node newly joining a subnet can securely learn the hash of the replicated state. I did a talk on CUPs a while back, you can find it [here](https://dfinity.org/howitworks/resumption). From that, there is a state-sync protocol, allowing a node to securely obtain the full replicated state from other nodes on the subnet.

I think the biggest performance cost is that a node that joins a subnet needs to obtain the full replicated state of a subnet. Currently, there is one subnet with a replicated state of 50GB, another one with 30GB, one with 10GB, and many subnets with a replicated state of a couple of GBs. Obtaining 50GB of data from other nodes consumes quite some bandwidth, and that's bandwidth we could otherwise use for larger consensus blocks (= more update message throughput) or to answer more query calls. The replicated state is currently allowed to grow to 300GB, so things can get even larger. 

To calculate an example: if we assume a 300GB replicated state, and nodes have 10Gb/s bandwidth, then a node could download that state in 4 minutes. It can pull different parts of the state from different nodes simultaneously, so if it's pulling data from 15 other subnet members, they would all only spend ~7% of their bandwidth on helping the new node catch up for the duration of 4 minutes. 

So in summary: I would say that we can relatively quickly add new nodes to a subnet and take another one out. If we're willing to pay a bit of a bandwidth penalty we can change many members of a subnet every day, and overall the approach of shuffling members seems feasible. 

Now my question to everybody here: what do you see as the goal of shuffling subnet membership? How exactly is it more secure than not shuffling subnet members?

-------------------------

diegop | 2021-09-23 14:32:21 UTC | #12

[quote="skilesare, post:9, topic:7478"]
I’m concerned about one malicious actor having infinite time to rip open a canister and look at the data in my canister
[/quote]

Ahhh I had completely missed this particular intent. Has nothing to do with consensus but about data privacy while Secure Enclaves are not online. 

Thank you @skilesare

-------------------------

lastmjs | 2021-09-23 19:15:33 UTC | #13

I have been working on a little post for the past couple days, but there seems to be far too much to discuss, and I don't want to try and put it all into one perfectly long blog post here. I'll just ad hoc add comments

-------------------------

diegop | 2021-09-23 19:16:11 UTC | #14

Welcome to my life, Jordan :upside_down_face:

-------------------------

lastmjs | 2021-09-23 19:29:57 UTC | #15

# Attack vector: Node operator collusion

One major attack vector is the collusion that is possible between node providers. Node providers are currently publicly known, and even if they weren't publicly known, it is feasible that the node providers could use other means through personal networks, google searches, relationships with data centers, etc that would allow them to find each other. 7 colluding nodes could delete every canister on the subnet.

The fact that each subnet has a relatively low replication factor (compared with other blockchains) makes it relatively easy for node providers to find each other and prepare for an attack. For example, on a 7 node subnet 3 colluding nodes can halt a subnet, and 5 colluding nodes could perform a potentially undetected attack and have full access to state changes and possibly more (if I am wrong on the math or the capabilities let me know, I think I am generally correct here).

If I am a node provider, I only need to find two other node providers to cause havoc to a 7 node subnet. Obviously increasing the replication factor would help, but shuffling may help us achieve higher levels of security with lower levels of replication, which is ideal for cost and other reasons. And I think it would be wise to add as many feasible mitigations as possible to the Internet Computer so that it can be incredibly secure.

Since subnet membership is basically created once at subnet inception, node providers have an indefinite amount of time to start colluding and preparing for an attack. The fact that canisters are currently running in plain text on the nodes and other information is known about them (I believe you can easily find out which subnet a canister belongs to?), it is relatively easy for node providers to even target specific attacks against canisters.

Hiding the canisters from the node operators I think is a separate problem that can be mitigated with secure enclaves and possibly other technologies or techniques. But even if node operators don't know what canisters they are running, they can perform an indiscriminate attack with regard to canisters and just attack the subnet. If they're lucky, they'll be able to get a juicy reward from canisters within their subnet, and maybe even affect other subnets that are depending on canisters within their subnet.

Though subnets are islands of consensus, it seems very unlikely that one subnet shutting down would not affect other subnets, since canisters will start to depend on other canisters in other subnets.

Shuffling the nodes would help to destroy node operator relationships within subnets. As soon as a relationship were formed, it may just as soon be destroyed.

Now to make this worth it, the network would need many many node operators, the more the better I would think. I will discuss this in further comments.

-------------------------

lastmjs | 2021-09-23 19:40:25 UTC | #16

# Attack vector: Miner Extractable Value (MEV)

Another possible attack vector is Miner Extractable Value (MEV), a problem that plagues Ethereum as we speak.

My understanding of MEV is this: Ethereum clients are modified to look for lucrative transactions going to certain smart contracts (for example, Uniswap). The modified clients (let's call them MEV clients) are designed to find these transactions, reorder them, and place the MEV client owner's own transaction in front, a transaction that would take advantage of the knowledge gained by viewing the order and gas prices of all transactions targeting certain smart contracts. Front running I believe is the general term for this type of behavior. There may be other forms of MEV, but the above is my understanding.

So basically, a miner can take advantage of its position of knowledge and reordering power to make money. This causes various issues, and IMO is not desirable and something to be avoided. It's a big problem with Ethereum and they're having a hard time dealing with it. It seems they've basically accepted it as unavoidable and are now dealing with it as a necessary evil.

The question is, would this be possible on the Internet Computer? I believe yes, assuming a subnet had 2/3+ of nodes running a modified replica designed to take advantage of certain canisters.

How can this be mitigated? Secure enclaves is one solution, assuming we can get attestations from the enclave that the replica has not been tampered with.

Another solution is...you guessed it, node shuffling! If as a node operator you never know which subnet you'll belong to, and thus which canisters you'll be hosting, it would become difficult to install the correct modified replica to take advantage of MEV. And 2/3 of the other nodes in the subnet would also need to install this software, so even if you convinced a cohort of buddies to join you in running the modified replica, you'd have to hope you'd all be shuffled into the same subnet that hosts the canisters the modified replica is designed for.

I'm not an expert on MEV and my understanding could be off, but as I see it node shuffling would provide an additional layer of protection against MEV. Combine that with secure enclaves that have code attestations, and I believe it would become nearly impossible for MEV to exist on the IC.

-------------------------

lastmjs | 2021-09-23 19:53:45 UTC | #17

# Attack vector: Secure enclave side-channel attacks

Secure enclaves will be an excellent addition to the security and privacy of the Internet Computer, helping to hide the execution and storage of canisters from node operators. This will make it hard for node operators to collude, considering if you don't know which canisters you are hosting it will be hard to perform a targeted attack against an individual canister. It will also make the IC more private, as it will become difficult for node operators to reveal canister data intended to be private and accessed only by authorized parties through the canister function interface.

Unfortunately, based on all of my learning on this subject over the past couple years, the consensus is that secure enclaves are not perfectly secure and probably never will be. There seems to be a catch-all attack vector known as a side-channel attack. Side-channel attacks in the context of secure enclaves are basically indirect attacks that use information such as power consumption, sound, electromagnetic radiation, and possibly other sources of information to read the supposedly "secure" or "private" information within the enclave.

This is where my knowledge really breaks down...I am not sure how long it takes for these attacks to be carried out. But my intuition tells me the attacks would not be simple, and would take a long time to perform, like hours, days, or weeks. Sophisticated equipment may be necessary, and the attack may need to be very specific, like to a canister, so again knowing which canisters are running on a node would not help this situation.

Assuming the above is all true or close to it, node shuffling again helps! As soon as an attacker has all of their equipment set up, the original canister they thought they might be targeting might have been whisked away. And even if the attacker is indiscriminate, the fact that the canisters running within the replica are always changing would not help them determine the patterns that they might need to perform the attack.

Instead of a node operator knowing which canisters are running on their nodes, and having indefinite time to perform a side-channel attack, we could greatly narrow that window, and hopefully make it shorter than a conceivable attack would take.

-------------------------

lastmjs | 2021-09-23 20:17:24 UTC | #18

# Attack vector: Uneven distribution of Byzantines, or lack of security as a network effect

This attack vector (if you can even call it that) perhaps embodies the core of my arguments for why node shuffling is necessary for the security of the Internet Computer.

Right now, security is not a network effect of the IC. As more nodes are added to the IC, the IC as a whole does not become more secure. Adding nodes to the IC only increases the throughput of the IC, and/or the security of individual subnets. Each subnet is an island of security, and is only in charge of securing itself. It does not inherit security from the security of other subnets, only in that ingress messages from those subnets would be more secure, and egress messages to those subnets would be more secure.

This is not ideal, because the distribution of Byzantines could become too concentrated within individual subnets, even if the BFT requirements of the IC as a whole are held. Phrased differently, even if 1/3 or fewer of all IC nodes are malicious, individual subnets can currently have more than 1/3 malicious nodes. Individual subnets do not inherit the security of the IC as a whole, they are left to fend for themselves.

With node shuffling, assuming proper randomness that guarantees proper probabilistic distribution of nodes across subnets (I am assuming this is possible), if the IC as a whole had 1/3 or fewer malicious nodes, then each subnet would also have 1/3 or fewer malicious nodes.

Security as a network effect thus seems like a very desirable property to have, and it is lacking from the current Internet Computer.

-------------------------

lastmjs | 2021-09-23 20:18:22 UTC | #20

And here are a few previous discussions on this topic by myself and a few others:

* My suggestion in the megathread with links to Twitter discussions: https://forum.dfinity.org/t/megathread-community-submissions-for-dfinity-foundation-s-roadmap/6175/11?u=lastmjs
* @victorshoup's thoughts on node shuffling during his community conversation on threshold ECDSA signing: https://youtu.be/MulbKPwv6_s?t=2097
* @Manu responding to me in detail in an AMA: https://www.reddit.com/r/dfinity/comments/nerppg/ama_we_are_manu_paul_and_diego_we_have_worked/gypf6m0/
* Me discussing prevention of side channel attacks on secure enclaves: https://forum.dfinity.org/t/amd-sev-virtual-machine-support/6156/9?u=lastmjs
* Me discussing MPC emulation with secure enclaves and shuffling: https://forum.dfinity.org/t/the-fleet-problem-opportunity-of-the-internet-computer/7159/11?u=lastmjs
* Me discussing node shuffling in detail on the Internet Computer Report: https://youtu.be/Std7Wo5tyWw?t=1695

-------------------------

lastmjs | 2021-09-23 20:27:31 UTC | #21

[quote="diegop, post:7, topic:7478"]
If we assume that 1/3 of the nodes in a subnet are malicious… and node memberships are being shuffled constantly, then won’t subnet Y eventually end up with a supermajority of malicious nodes? Could shuffling produce a situation where some subnets have 0 malicious nodes and some subnets have 2/3 malicious… and by the law of probability, eventually end up corrupting subnet Y.
[/quote]

Designed appropriately, I think the laws of probability would prevent this exact situation from happening. I remember earlier DFINITY materials which explained that with sufficient numbers of committee membership, the random beacon driving probabilistic slot consensus or some other complicated mechanisms would ensure with outrageous (I think the more correct term is overwhelming) probability that no individual committee would be more than 1/3 malicious. The committee size was something like 400, and committees were chosen out of the total number of nodes participating in consensus. Each committee was in charge of validating for a certain period of time or something like that.

-------------------------

MalcolmMurray | 2021-09-23 20:43:26 UTC | #22

Attack: Censorship

If some organization wanted to stop a canister they didn't like, they would currently only have to take down a few identified nodes.

If nodes were shuffled daily at random times, it would be very difficult to keep up with attacking the right nodes quickly enough before the canister gets passed on to new ones.

-------------------------

rubenhorne | 2021-09-23 21:43:25 UTC | #23

It seems the main question being posed is "Which attacks are potentially mitigated and which new vulnerabilities are opened by implementing shuffling node membership of subnets?"

The main attack vectors that could be mitigated seem to be well described by the others here. I see daily or similar interval of node shuffling as significantly raising the difficulty of node operator collusion or censoring nodes. The net effect seems to be increase the security of any individual subnet to approximate a subnet with all existing node operators as members rather than just the smaller node membership of any subnet.

To me it seems the concern for new vulnerabilities center on discoverability of other subnet members. As @diegop explains, a contingent of compromised (threatened, corrupted, hacked, etc.) but ostensibly honest nodes could eventually be randomly shuffled into a subnet together (which given enough shuffling should eventually happen), discover the joint membership of the other corrupt nodes to be >2/3, and coordinate to tamper with their subnet's canister, especially if that canister holds valuable digital assets or information. The best protection against this vulnerability would be to shield the identity of the node operators from one other in any subnet, which I do not know whether it is feasible to do. 

However the status quo now is that any motivated actor or group of nodes could work to corrupt known nodes on known subnets whose membership is static. I would rather have the moving target problem of shuffling subnets spontaneously being assigned compromised members than the non-moving target problem of static subnets with fixed (although distributed) points of failure.

-------------------------

dansteren | 2021-09-24 05:14:58 UTC | #24

I don't have any new attack vectors to contribute to the discussion, but I think the ones that have been laid out so far seem valid, are concerning to me, and are worth preventing. Obviously the Dfinity foundation has to juggle competing concerns on what to prioritize next so this may not be at the top of the list, but I believe they should be addressed at some point.

I've spoken with @lastmjs before about some of these attack vectors and they seem theoretically sound, and potentially not that difficult to even pull off. What worries me even more is that even though Jordan is very smart and has obviously thought about this a lot, he isn't even a security expert by trade. It makes me wonder what other even more obscure vectors might be discovered by those who have studied attack vectors for years.

I'm not saying that node/canister shuffling has to be the solution to these problems. Maybe there's a better way to tackle them. Or maybe there's some reason that these aren't valid. But if these attack vectors are as valid as they seem, and we know about them, it would be foolish to ignore them, regardless of how low the probability seems. As the internet computer grows so to will the incentive to exploit these. Once again, we may not need to fix them as the very next roadmap item. Hopefully for now the care we've put into selecting node providers will keep us safe. But we should have a game plan on these and shuffling seems like it would do the trick nicely. 

Also, thanks for everyone's comments so far. I'm not in the weeds enough yet on this so this thread had been really enlightening.

-------------------------

Manu | 2021-09-24 16:02:01 UTC | #25

Thanks everybody for the insightful comments! I understand that it’s a lot of effort to write all these thoughts down in detail, but I think this really helps for the discussion. To keep an overview, so far people brought up the following possible advantages of shuffling:

- node operator collusion (a majority of the nodes of a subnet are malicious and tamper with the subnet state)

- security as a network effect (very related to the point above)

- miner extractable value / front running

- attacking enclaves

- censorship resistance

I’ll first reply to the ones that I find less convincing, and then comment on the ones that i think are the best arguments in favor of shuffling.

**miner extractable value / front running**

A block maker in any blockchain can order messages in any way they like. That is because we haven't agreed on an ordering yet, we are placing this on a blockchain to reach agreement on an ordering. This means that a malicious block maker can always try to order messages in an order that is favorable for itself. A concrete example of such an attack is if the block maker sees a big sell order to some dex coming in, it can quickly place a sell order itself first, and make sure that appears in the block before the other sell order. Note that this attack only requires a single malicious block maker, it does not need a collusion between multiple node operators, because the MEV block maker will still propose a block that only contains valid messages, so all other replicas will accept this block. They cannot know that the MEV block maker is actually front running, because there is no agreed-upon ordering of messages yet.

> Another solution is…you guessed it, node shuffling! If as a node operator you never know which subnet you’ll belong to, and thus which canisters you’ll be hosting, it would become difficult to install the correct modified replica to take advantage of MEV. And 2/3 of the other nodes in the subnet would also need to install this software, so even if you convinced a cohort of buddies to join you in running the modified replica, you’d have to hope you’d all be shuffled into the same subnet that hosts the canisters the modified replica is designed for.

See my point above, this attack can be done by a single malicious replica, you don't need a majority. The idea that the node operator/replica doesnt know which subnet it belongs to and which canisters it hosts is very difficult to realize: the replica must only accept messages to canisters that are hosted on the subnet you belong to, so you couldn't properly validate blocks if you dont know which canisters you're running.

> How can this be mitigated? Secure enclaves is one solution, assuming we can get attestations from the enclave that the replica has not been tampered with.

I think this is a promising way to address MEV, and the foundation is already investigating how this can be done. So if we run this in some TEE and require attestations, it will be way harder to run a malicious block maker. Other countermeasures can be taken in the dapp itself. For example, if you are a dex, you could think of batching transactions and using secure randomness to shuffle transactions and execute them in a random order, making front running much harder.

**Attacking enclaves**

That's an interesting point that I didn't think about yet. I'm not an expert on these things, but I would imagine that best way to get info out of the TEE via side channels is to target the encryption key via side channels, not to attack canisters directly. Changing subnets would not change your TEE, so then shuffling wouldn't "reset" the side channel attack. I'll try to get some of the experts involved in this one.

**censorship resistance**

Right, so an attacker targeting a certain canister can figure out which subnet that canister is one and attack nodes from that subnet. @MalcolmMurray, what type of attack are you thinking of here? And how quickly do you imagine nodes would be shuffled? If a couple of nodes are swapped out say every hour or so, and the attack is just a simple DoS attack, then I'm not sure shuffling helps: an attacker could easily keep up with the subnet membership changes because it's easy to target a new node with a DoS attack. For attacks that take along time to perform, shuffling could indeed help.

Now, I think the main ones are left.
**node operator collusion and security from the network effect**

This is a super important one and I think the core of this discussion. The argument of “security from the network effect”, so the bigger the ICP grows, the more secure it becomes, is obviously super compelling.

So the internet computer could constantly switch nodes between subnets, such that for example, it makes sure that a node never spends more than 1 week on a given subnet. If we further assume that attacking a node takes > 1 week, then the adversary can no longer target specific canisters/subnets, which is a nice property to have.

> Designed appropriately, I think the laws of probability would prevent this exact situation from happening. I remember earlier DFINITY materials which explained that with sufficient numbers of committee membership, the random beacon driving probabilistic slot consensus or some other complicated mechanisms would ensure with outrageous (I think the more correct term is overwhelming) probability that no individual committee would be more than 1/3 malicious. The committee size was something like 400, and committees were chosen out of the total number of nodes participating in consensus. Each committee was in charge of validating for a certain period of time or something like that.

Right, you’re referring to our [old whitepaper](https://medium.com/dfinity/dfinity-white-paper-our-consensus-algorithm-a11adc0a054c). This is where things get more tricky. We now essentially sample a new subnet membership every week, and here we want to then draw from the security of the overall amount of nodes. Suppose our total pool of nodes is very large (so i can get away with doing binomial distributions). Let’s do some computational examples. Suppose up to `probability_corrupt` of all nodes are corrupt, we sample a subnet of size `subnet_size`, then we compute (using binomial distribution) how likely it is that we select a subnet with more than 1/3rd of the nodes being corrupt, which i call `p_insecure`. Below you see some examples, [here is the google sheet i used](https://docs.google.com/spreadsheets/d/1DcEs9VDiThpcEjLT83L0tIJp5TVDZlYyh9MXWgMpdz0/edit?usp=sharing). 
![image|690x326](upload://iaKu98Q7ui3AN8nvcRej2JACfAv.png)

So what you see is that if for example we assume 1/10th of all nodes in the IC are malicious, and we randomly select a 28 node subnet, there is a 2^-12 probability that the subnet is unsafe, because more than 1/3rd of the nodes are corrupt. This is a small chance, so that is good, but if we regularly choose new subnet members, then *every time* the sampling needs to be successful. If we have 50 subnets and do this every week, we do this ~2500 times per year, and each one of those must be successful. I think this is the main price of the reshuffling. If we don't reshuffle, we don't need to get it right 2500 times per year, but just once, which is obviously a better chance, so theoretically we could tolerate more corruptions overall. How do people feel about these numbers? 

So to recap, based on these numbers, I think we can say:
- shuffling nodes is nice against "adaptive" adversaries that target one subnet, assuming it takes some time to corrupt a node
- shuffling nodes is not so nice in the sense that we select new subnet memberships every time, and each time, we must select a secure set of nodes. We can get unlucky by combining too many malicious nodes, that can then collude to break the subnet
- static subnets are weak against adaptive adversaries
- static subnets are better against static adversaries

-------------------------

Maxfinity | 2021-09-24 21:40:49 UTC | #26

[quote="Manu, post:25, topic:7478"]
Right, you’re referring to our [old whitepaper](https://medium.com/dfinity/dfinity-white-paper-our-consensus-algorithm-a11adc0a054c). This is where things get more tricky. We now essentially sample a new subnet membership every week, and here we want to then draw from the security of the overall amount of nodes. Suppose our total pool of nodes is very large (so i can get away with doing binomial distributions). Let’s do some computational examples. Suppose up to `probability_corrupt` of all nodes are corrupt, we sample a subnet of size `subnet_size` , then we compute (using binomial distribution) how likely it is that we select a subnet with more than 1/3rd of the nodes being corrupt, which i call `p_insecure` . Below you see some examples, [here is the google sheet i used ](https://docs.google.com/spreadsheets/d/1DcEs9VDiThpcEjLT83L0tIJp5TVDZlYyh9MXWgMpdz0/edit?usp=sharing).
[/quote]

I think the question comes down to what is a more realistic modelling assumption. An adaptive adversary seems much more realistic to me, where there is some probability that a node is corrupted over time - say 5% in any time interval. In this real-world scenario you may expect for a node that has been corrupted to stay corrupted, so you would effectively "running repeated trials" - the only difference being that by not shuffling you have no mechanism to 
prevent the build up of colluding node providers. 

Also a factor of 2500, doesn't sound too high. I would imagine that this could be compensated for by a small increase in the number of nodes running the consensus? It should only need a logarithmic increase in the number of node providers?

-------------------------

rubenhorne | 2021-09-24 21:46:15 UTC | #27

Beautiful points. To further crystalize the implications of your fault analysis @Manu, I  added some columns and resulting graphs that forecast the odds of staying at or below Byzantine Fault tolerance in a canister. This is all as I best understand it, but someone please correct me if I am wrong. I take the time interval over a hundred years for various shuffling intervals (daily, weekly, monthly, quarterly, yearly, and never) and take the odds of a halt-free (no faults) canister for 1 instance of a subnet (1-p_insecure) raised to the power of the total number of shuffles (100 years*#ofShuffles/year), shown below, but all available at [this google sheet](https://docs.google.com/spreadsheets/d/1iPiSlFsmTsY4cU1x5WqZvayUkueoX4ldyqnYwwHLr3M/edit?usp=sharing).

![image|690x424, 50%](upload://z2Qd96U5D1BySI8t8CxQott8V9t.png)

![image|690x419, 50%](upload://zMARifR7xkeWOef2UxHccxDumQe.png)

![image|690x428, 50%](upload://c9ZwbMvahb2RwQVn6ZrwjNi9jHZ.png)

Basically, the message of these graphs is the less shuffling the better to avoid matching corrupted nodes into a subnet, assuming a given level of corruption. Having more nodes in a canister obviously helps reduce the likelihood of a fault. (In the spreadsheet I calculate up 50 and 100 nodes too, but omitted here for the sake of space).

However, I think the model assumes a percentage of corrupted nodes regardless of shuffling, even though intuitively it seems the temptation to collude in a static subnet should be more and would grow over time. If we assume that staying on a subnet for a long time comes with a greater temptation to collude, then we would have to run some new numbers. The problem is, the math gets more complicated and requires its own batch of assumptions. Maybe someday soon.

-------------------------

Manu | 2021-09-25 14:35:15 UTC | #28

Very nice @rubenhorne! 

[quote="Maxfinity, post:26, topic:7478"]
Also a factor of 2500, doesn’t sound too high.
[/quote]

I think Rubenhorne's data above actually shows that it still hurts quite a bit. We need that subnets are going to be secure with overwhelming probability. We see that if we assume at most 5% of all nodes are malicious, and we sample a 28 node subnet, the probability that it will be insecure is 2^-20.78, so almost 1 in 2 million. That seems pretty good. But now if we do this 2500 times and we need to be right every time, it's only 2^-9.5, so more than 1 in a thousand. I think that probability is not small enough to call secure. 

We can indeed increase the subnet size to counter this. As an example: sampling a 28 subnet once (with overall corruption 5%) has roughly similar success probability as sampling a 46 node subnet 2500 times.

-------------------------

lastmjs | 2021-09-26 03:08:50 UTC | #29

I agree with the importance of the assumption of adaptive adversaries. I'm more concerned with node operators that start out honest or seem honest at first, and then start becoming corrupted over time.

I have some rebuttals to arguments made and some deeper information I hope to get out next week.

-------------------------

free | 2021-09-26 07:36:34 UTC | #30

[quote="Manu, post:11, topic:7478"]
To calculate an example: if we assume a 300GB replicated state, and nodes have 10Gb/s bandwidth, then a node could download that state in 4 minutes. It can pull different parts of the state from different nodes simultaneously, so if it’s pulling data from 15 other subnet members, they would all only spend ~7% of their bandwidth on helping the new node catch up for the duration of 4 minutes.
[/quote]

I will point out that 4 minutes is very much a theoretical lower limit. In practice, it would be on the order of hours rather than minutes, for a number of reasons:

1. Some node operators only have 1 Gbps and that may only be burst traffic, not sustained. I believe we currently require 10 Gbps from new node operators, but again that may be for burst, not sustained traffic. As a specific example, a few months ago when I was testing using QUIC for XNet messaging, the highest sustained throughput I saw between Europe and US West Coast was 400-500 Mbps.

2. Most subnets are well below the 300 GB limit right now but that's exactly because we explicitly tried to delay this occurrence for as long as possible (by adding subnets to spread the load) specifically so we don't require many, many hours to state sync a node should it need replacement. In the long term we will want to use as much of that 300 GB capacity in order to make the IC economically viable.

3. An admittedly less important, but still worth mentioning point is that transferring 300 GB assumes the state of the subnet does not change (much) for the duration of the state sync. This may be a good enough approximatioon if state sync actually takes 4 minutes, but if we go with a more realistic 1 hour estimate, then by the time we're transferred the whole 300 GB it is likely that some of that will have changed. So we'll need to state sync those changes and then do that again and again, until we're fullly caught up. Which means it is likely we may need to transfer quite a bit more than 300 GB.

4. Even given an actual 10 Gbps of sustained bandwidth per data center, there is an underlying assumption here that nothing else is using that bandwidth. I.e. no gossip, no ingress or canister-to-canister traffic, no replica or OS upgrades and (more importantly) a single node doing state sync at a time. Some discussion here suggested swapping out a node every 1 hour.If so, you would definitely have more (and possibly a lot more) than 1 node state syncing at any given time, resulting in only a fraction of this (very much theoretical) 10 Gbps available to each node. If you now have to do an emergency swap of a node on a subnet (because some othe rnode went up in smoke or whatever) the situation becomes even worse (particularly for this emergency swap).

All of the effects above are cumulative: if you only have half the theoretical 10 Gbps available for sustained traffic; the subnet size is very close to 300 GB; you need to transfer 150% of the state size before you're fully caught up; half of the available bandwidth is used by higher priority traffic (gossip, ingress, XNet); and one other replica is doing state sync; you suddently have 1/8th of the bandwidth to transfer 450 GB, i.e. you're 12x under the theoretical maximum.

-------------------------

MalcolmMurray | 2021-09-27 05:31:40 UTC | #31

[quote="Manu, post:25, topic:7478"]
@MalcolmMurray, what type of attack are you thinking of here? And how quickly do you imagine nodes would be shuffled?
[/quote]

I'm thinking more along the lines of political or even military attack. The more often the better, subject to practical constraints.

-------------------------

MalcolmMurray | 2021-09-27 05:51:39 UTC | #32

[quote="Manu, post:25, topic:7478"]
So to recap, based on these numbers, I think we can say:

* shuffling nodes is nice against “adaptive” adversaries that target one subnet, assuming it takes some time to corrupt a node
* shuffling nodes is not so nice in the sense that we select new subnet memberships every time, and each time, we must select a secure set of nodes. We can get unlucky by combining too many malicious nodes, that can then collude to break the subnet
* static subnets are weak against adaptive adversaries
* static subnets are better against static adversaries
[/quote]

"Adaptive" adversaries who are tempted by specific target seem more realistic and relevant to me than "static" adversaries who are malicious on principle and would attack indiscriminately.

Also, if I understand correctly, this analysis relies on the assumption that these universally malicious nodes would instantly know when they have been grouped in a subnet with collaborators. I guess this is possible if they know each other ahead of time, but I think it's more likely that they would need to discover each other, which is a risky and time-consuming process, made less likely by node-shuffling.

-------------------------

senior.joinu | 2021-09-28 01:37:57 UTC | #34

What if we could transfer canisters between subnets? E.g. to have a special API, that once called will make the canister reappear at some other random (and available) subnet. The canister itself can cover all the expenses. The developer behind the canister can decide for themselves whether they want to use this mechanism to provide a more secure experience or not.

Like an update_code call, but update_subnet.

-------------------------

senior.joinu | 2021-09-28 01:50:59 UTC | #35

This is so much better.

This could even create a positive side effect. Since node providers are publicly known, it is in their interest to keep their reputation. Otherwise devs would just migrate their canisters to better subnets leaving bad boys without money.

-------------------------

coin_master | 2021-09-28 05:13:42 UTC | #36

This looks like a good idea but I can see that it would also open a new set of problems:
1 - What if many projects decided to move to a specific subnet at once, wouldn't that congest this particular subnet?
2 - If you allow a canister to be moved at will then a canister can select an attractive subnet which holds a lot of value and try to DDOS this subnet or congest it by taking all the traffic or state.
3 - What if a canister that's not allowed in some jurisdictions decided to move to this jurisdictions, would it cause legal issues to the node provider?

-------------------------

Manu | 2021-09-28 09:27:52 UTC | #37

[quote="free, post:30, topic:7478"]
I will point out that 4 minutes is very much a theoretical lower limit. In practice, it would be on the order of hours rather than minutes, for a number of reasons:
[/quote]

Thanks a lot for all this info @free! So I was a bit optimistic in my concrete numbers. It does also point to the fact that currently we allow subnet sizes that might pose challenges for state sync, that need to be addressed separately. For the purpose of this topic, would you say it's reasonable that we could at least swap out a couple of nodes per subnet per day? 

[quote="senior.joinu, post:34, topic:7478"]
What if we could transfer canisters between subnets?
[/quote]

That's a good idea, this is definitely something that the internet computer will need soon (eg to load balance between subnets). I know some of my colleagues are already thinking about this. However, I don't think this really addresses the same problem as shuffling subnet membership, and it has some downsides:
* moving canisters does not make the subnet more secure. In particular, we don't get the property of "every subnet becomes more secure the larger the internet computer grows" 
* moving a canister between subnets will very likely involve downtime for the canister (or at least, doing it this way will make the problem at lot easier). Adding and removing nodes to/from a subnet does not stop the subnet since its a fault tolerant system.

-------------------------

free | 2021-09-28 16:19:46 UTC | #38

[quote="Manu, post:37, topic:7478"]
For the purpose of this topic, would you say it’s reasonable that we could at least swap out a couple of nodes per subnet per day?
[/quote]

With the current data center sizes (<20 nodes) and bandwidth (10 Gbps), sounds doable. As at a subnet size of 13, it would mean 3 nodes (on average) doing state sync within a 24 hour period.

But given larger data centers (without a commensurate increase in bandwidth), smaller subnets or more than a small handful of nodes switched out per subnet per day, you would run into bandwidth limitations at some point.

-------------------------

diegop | 2021-10-01 15:43:13 UTC | #40

[quote="Manu, post:37, topic:7478"]
moving canisters does not make the subnet more secure.
[/quote]

@Manu but if we deem the attack as "*I want to minimize the expected amount of time that a malicious node may have access to a canister*" (as per @skilesare suggested), would that not be a relevant mitigation tactic.

That being said, i do not have an opinion on how highly that attack lands in the stack ranking of risks.

-------------------------

Fulco | 2021-09-30 11:40:20 UTC | #41

The examples mentioned around shuffling so far mostly take a completely random sample of the population. Wouldn't we want to do this semi randomly based on other factors like:

- Optimising for the biggest number of different data centers
- Optimising for biggest number of different node owners (if that is possible)

-------------------------

Manu | 2021-09-30 12:56:50 UTC | #42

[quote="Fulco, post:41, topic:7478, full:true"]
The examples mentioned around shuffling so far mostly take a completely random sample of the population. Wouldn’t we want to do this semi randomly based on other factors like:

* Optimising for the biggest number of different data centers
* Optimising for biggest number of different node owners (if that is possible)
[/quote]
That's a great point @Fulco, i think that we should definitely take that into consideration. 

[quote="diegop, post:40, topic:7478"]
@Manu but if we deem the attack as “ *I want to minimize the expected amount of time that a malicious node may have access to a canister* ” (as per @skilesare suggested), would that not be a relevant mitigation tactic.
[/quote]
It would, all i meant was that increasing the security of subnets is nicer, because then all canisters automatically get more security. If you can only get this increased security if the developer moves their canister all the time, and canister migration would incur some downtime, then you need to choose between convenience+availability and increased security.

-------------------------

lastmjs | 2021-09-30 14:06:53 UTC | #43

[quote="Manu, post:25, topic:7478"]
See my point above, this attack can be done by a single malicious replica, you don’t need a majority. The idea that the node operator/replica doesnt know which subnet it belongs to and which canisters it hosts is very difficult to realize: the replica must only accept messages to canisters that are hosted on the subnet you belong to, so you couldn’t properly validate blocks if you dont know which canisters you’re running.
[/quote]

Thank you for clarifying the consensus mechanism. Even if a single malicious replica can reorder transactions, that replica I imagine would need to be modified to reorder transactions for specific canisters.

It seems reasonable to me that MEV-specific replicas would need to be created with individual canisters in mind. So I think shuffling would still help, as the node operator would only be able to reorder transactions and reap MEV when a member of a subnet that has the targeted canisters.

Perhaps MEV can still be programmed into a replica without targeting specific canisters, but just classes of canisters...seems less likely. And perhaps if a sufficient number of replicas are MEV-modified for 100s or 1000s of canisters, then shuffling wouldn't help.

But it seems that shuffling would still make it relatively more difficult to pull off MEV, than without shuffling. Combine that with enclave attestations and in-canister mitigations and I see MEV is nearly impossible to achieve.

But perhaps shuffling offers little benefit relative to attestations and in-canister mitigations, I am not sure.

-------------------------

lastmjs | 2021-09-30 14:11:36 UTC | #44

[quote="Manu, post:25, topic:7478"]
That’s an interesting point that I didn’t think about yet. I’m not an expert on these things, but I would imagine that best way to get info out of the TEE via side channels is to target the encryption key via side channels, not to attack canisters directly. Changing subnets would not change your TEE, so then shuffling wouldn’t “reset” the side channel attack. I’ll try to get some of the experts involved in this one.
[/quote]

I agree we need expert opinions. But I imagine side-channel attacks, even if directed at extracting the private key, require knowing something about the code that is executing inside the TEE. That is the assumption I base this argument on, and if that is true then not knowing which canister is running within the TEE, or knowing it only for a short time, I imagine would make the side-channel attack harder to execute.

-------------------------

lastmjs | 2021-09-30 14:33:15 UTC | #45

I think another interesting thing to consider is the distinction between types of collusion attacks. There seem to be two main types of attacks: halting a subnet (halt attack) and completely taking over a subnet (takeover attack).

A halt attack requires controlling greater than 1/3 of nodes in a subnet.

A takeover attack requires controlling 2/3 or greater of nodes in a subnet.

As far as I understand the IC, a halt attack will be much less destructive than a takeover attack. Halt attacks I believe would be easily detected, as the majority of the subnet would be able to identify the malicious nodes. Upon detection of the attack, the subnet I believe shuts itself down to protect further damage, until BFT guarantees are restored. I would imagine there is or will be some sort of slashing to penalize or remove the malicious nodes. A halt attack will cause down-time but will not be able to corrupt state (which I would posit is nearly a worst-case scenario).

A takeover attack would be horrendous. In this case, arbitrary state changes can be pushed through, and possibly without detection, since technically BFT has not been violated. It seems reasonable that in this case human beings would need to manually detect this kind of attack, and manual NNS proposals would need to be made to fix the subnet. But at that point it might be too late, data could have been manipulated, money lost, and perhaps even state deleted.

This is just what I imagine could happen based on what I know, those with deeper IC protocol knowledge please correct any false information.

With those explanations above, takeover attacks are clearly worse. What's interesting is that the shuffling probabilities are much better in regards to takeover attacks. Someone would need to do the math (@rubenhorne @Manu) so we could compare.

But even with those numbers, I think the probabilities would still show that not shuffling gives us a better chance of an honest subnet configuration than shuffling many times would.

So for a static amount of corruption, shuffling hurts. But again, I think the adaptive adversary threat is probably more important. And if the takeover probabilities are acceptable, then shuffling might hurt less than we thought.

-------------------------

lastmjs | 2021-09-30 14:46:40 UTC | #46

TLDR We should try to hide as much information about subnet membership and node and canister configurations as possible.

I don't want to derail the conversation, but I want to put this here for future consideration once we feel comfortable with the attacks that can be mitigated by shuffling.

Privacy or hiding certain information from node operators could greatly enhance the security of the IC and improve the efficacy of node shuffling. I know some of what I'm about to describe may be extremely difficult to implement or maybe impossible with current technologies, but I think it's something to strive for.

Basically, I'm trying to imagine the most secure IC possible (within reason). Here's what it would look like:

Node operators know nothing about the canisters they host nor the subnets they are members of. All they know is that they are running the correct binary of the IC replica in their TEE.

The TEE provides attestations that the blessed replica binary is what is running in every node. Those attestations are included in consensus, so that if a sufficient number of nodes break the attestations then BFT is broken.

The IC unpredictability shuffles the members of subnets and assigns canisters to their initial subnets. Subnet configurations are hidden. Canisters do not even know which subnets they are members of.

I might have taken some of the hiding too far, but the security benefits of hiding certain information I believe are very clear. If a node operator does not know which canisters it hosts and does not even know how many other node operators are in their subnet and cannot know who the other node operators are...how exactly could node operators then collude? TEE would help prevent malicious replica binaries, and shuffling could help prevent any leaks in the hiding (node operators might still create and maintain offline relationships somehow) and side-channel attacks to the TEE.

-------------------------

rubenhorne | 2021-09-30 23:46:58 UTC | #47

I have generated some charts (again, borrowing @Manu's fault analysis sheet) that show how shuffling would protect against an adaptive (corruptible) adversary. I assume a constant "probability of a node turning corrupt in a year" from 0.1% to 80%. Then for a 100 year interval, I calculate the probability of a subnet to be halted (surpass Byzantine fault tolerance) for various shuffling intervals (never, yearly, quarterly, monthly, weekly, or daily). (In a separate chart not shown here but at the link, I plot how likely a subnet is to remain "takeover-free" (corruption<2/3) for 100 years for the same shuffling intervals). When reshuffling occurs, I assume that the corruption (in this case, collusion with fellow-subnet members) is reset to 0. Here is the [link](https://docs.google.com/spreadsheets/d/1XHGl6TAJUhw1qdOXdVs5RNC3VgIqRXDBKfUhkroo0No/edit?usp=sharing) to the analysis:

![image|690x420, 75%](upload://41BP28m4wGrq2ntZVuAn27YpEyI.png)

![image|690x424, 75%](upload://3C62xm1ybo8qw4OOrXcFhAPQmJH.png)

![image|690x431, 75%](upload://aQqHXTBHNGmCt8u9Qo0RBIJ2zuS.png)

What is interesting to me is that shuffling in this case is clearly protective. The more nodes that are on the subnet, the less likely it is for enough of them to corrupt each other to halt the subnet. What is alarming to me is how easily a low rate of corruption like 2%/year can eventually halt (and also takeover) a subnet when it is never shuffled, even for a 34-node subnet. 

What seems to be the fundamental question then is which attack do we consider most risky/likely?  Is it nodes influencing each other to for evil on a subnet (which we protect against by shuffling) or is it a distributed baseline of evil that we don't want to randomly match together (for which we would protect by not shuffling or shuffling with blinding).

I think @lastmjs has a point about halts being much less harmful than takeovers. If shuffling protects against takeovers better while ceding ground in halts, I am ok with that. 

One take-home message too is that in any case, more nodes are better, and 7 nodes in particular are quite vulnerable to this type of attack.

I hope to post more "takeover" data soon.

-------------------------

lastmjs | 2021-09-30 21:59:02 UTC | #48

[quote="rubenhorne, post:47, topic:7478"]
What is alarming to me is how easily a low rate of corruption like 2%/year can eventually halt (and also takeover) a node when it is never shuffled, even for a 34-node subnet.
[/quote]

Did you mean to say "can eventually halt (and also takeover) a **subnet**"?

-------------------------

rubenhorne | 2021-09-30 23:47:51 UTC | #49

Yes I did. Should be fixed now

-------------------------

rubenhorne | 2021-10-01 17:38:51 UTC | #51

I agree. It seems one big takeaway from this discussion is that regardless of how you weight the risks of static vs adaptive node corruption, being able to conceal the identities of subnet members from each other would dramatically protect against both types of threats. Therefore, if feasible, implementing subnet membership concealment should take precedence over shuffling. 

If it's not feasible, then we need to figure out which, if any, shuffling frequency offers the best mix of protections against both static and dynamic threats with as little drawback as possible.

-------------------------

Manu | 2021-10-06 13:29:25 UTC | #52

[quote="lastmjs, post:45, topic:7478"]
I think another interesting thing to consider is the distinction between types of collusion attacks. There seem to be two main types of attacks: halting a subnet (halt attack) and completely taking over a subnet (takeover attack).

A halt attack requires controlling greater than 1/3 of nodes in a subnet.

A takeover attack requires controlling 2/3 or greater of nodes in a subnet.
[/quote]
I think this is an important point. Changing the state arbitrarily or executing invalid transactions should indeed be impossible without corrupting 2/3 of the nodes (or 2f+1 to be precise), but I think we can already have bad attacks with only 1/3rd or > f corruptions. This is because we want to reach consensus without making any assumptions on the network. So unlike bitcoin, where things rely on everybody seeing the longest chain, we do not want to make such assumptions. This problem of asynchronous consensus can only be solved with f < n/3 corruptions (see eg [wikipedia](https://en.wikipedia.org/wiki/Consensus_(computer_science)#Solvability_results_for_some_agreement_problems)). That means that also in the internet computer, if you can corrupt f+1 parties and in combination with that you can do a network attack, then you can already "fork" the blockchain (two distinct chains could be finalized). This already allows double-spending attacks. So in summary, I think we should do all calculations aiming to keep f < n/3.

-------------------------

lastmjs | 2021-10-11 15:00:36 UTC | #53

Can you explain this attack in more detail, or provide some external reading? I don't understand how the finalization could occur here with only f+1 parties and a network attack, since I thought 2/3 (2f+1) are absolutely required to finalize anything. Would love to understand better, thanks.

-------------------------

lastmjs | 2021-10-11 15:03:56 UTC | #54

Also, to add more to why I don't understand the danger here, I thought that the Internet Computer had mechanisms in place to detect when 2f+1 parties are not able to come to agreement, and that the subnet would then "freeze" in this case, until the malicious nodes can be identified and removed. Is that not the case? How capable is the system at detecting when BFT guarantees are being broken, and what are the mitigations the protocol provides?

-------------------------

Manu | 2021-10-11 16:42:15 UTC | #55

[quote="lastmjs, post:53, topic:7478, full:true"]
Can you explain this attack in more detail, or provide some external reading? I don’t understand how the finalization could occur here with only f+1 parties and a network attack, since I thought 2/3 (2f+1) are absolutely required to finalize anything. Would love to understand better, thanks.
[/quote]
Of course! Let me start with a quick recap: So replicas in our consensus protocol create notarization shares to indicate valid blocks. They might create notarization shares for different blocks at one height. When they see a full notarization (which consists of 2f+1 notarization shares), they move on to the next round. If there exists only 1 notarization in a round, that means we have agreement (because chains must consist of fully notarized blocks at every height). To help identify this, notaries also create a finalization share on a block b at the end of the round if they did not create a notarization share on any block at that height other b. If a block b collects 2f+1 finalization shares, we consider it finalized, and replicas trust that this is the agreed-upon blockchain. 

This is safe on a subnet of size n = 3f+1 when at most f nodes are corrupt, meaning that a finalization on block b means that no other notarized block b' at that height can exist. That is because if we have 2f+1 finalization shares, that means those nodes say they did not notary-sign any other block at that height. f of those may be corrupt, so they might have lied, but it means at least f+1 nodes are honest and really did not create notarization shares for other blocks at that height. Since we only have 3f+1 replicas, and we know that f+1 did not create notarization shares for any other block b' at that height, it only leaves 2f nodes that could have possibly created notarization shares on b', but this is less than the threshold 2f+1, concluding the proof. 

Now the attack: suppose the attacker controls more than f (say f+1) corrupt replicas, and additionally has full control over the network. For simplicity, let's look at a 4 node subnet consisting of nodes A, B, C, and D, and the adversary controls 2 nodes (which is f+1, more than 1/3rd). The notarization/finalization threshold is 3 for a 4 node subnet. Let's say A and B are the corrupt nodes. The attacker can make sure there are two valid blocks at height h, b1 and b2. It makes sure that replica C only receives block b1, and replica D only receives block b2 (using the fact that it fully controls the network). C will create a notarization share for b1, and D for b2. Using its control over replicas A and B, the attacker can complete both notarizations for b1 and b2, and show the full notarization on b1 to C and the full notarization on b2 to D. C will now create a finalization share on b1 (since it only created a notarization share on that block), and similarly D will create a finalization share on b2. Now again using A and B, the attacker can complete two finalizations on b1 and b2. This completes the attack: C thinks that block b1 is final, while D thinks b2 is final, and they are conflicting blocks. 

So this shows that an attacker having more controlling more than f out of 3f+1 nodes is problematic. You cannot just sign arbitrary blocks, but we cannot guarantee agreement anymore. 

[quote="lastmjs, post:54, topic:7478, full:true"]
Also, to add more to why I don’t understand the danger here, I thought that the Internet Computer had mechanisms in place to detect when 2f+1 parties are not able to come to agreement, and that the subnet would then “freeze” in this case, until the malicious nodes can be identified and removed. Is that not the case? How capable is the system at detecting when BFT guarantees are being broken, and what are the mitigations the protocol provides?
[/quote]
As demonstrated by the attack above, if f+1 nodes are actively malicious and the adversary can control the network, then we immediately have a problem. If f+1 nodes are faulty in the sense that they are offline but not actively malicious, then the subnet would be stuck, but the NNS can replace nodes in the subnet.

-------------------------

MalcolmMurray | 2021-11-16 08:17:48 UTC | #57

[quote="Fulco, post:41, topic:7478, full:true"]
The examples mentioned around shuffling so far mostly take a completely random sample of the population. Wouldn’t we want to do this semi randomly based on other factors like:

* Optimising for the biggest number of different data centers
* Optimising for biggest number of different node owners (if that is possible)
[/quote]

Building on the theme of semi-random node shuffling, here another idea (somewhat tangential).

Some subnets could be designated as zero-carbon subnets, only using node providers running on renewable energy. Instead of using batteries to deal with supply intermittancy, node-handover is used instead. When renewable power is available at a specific node's location, it joins the subnet; when not available, it goes dormant. This would decrease operating cost, but increase capital cost because the subnet would need more replication.

Electricity can't be efficiently moved around the globe, but with the Internet Computer, compute can. In other words, we'd be using Chain Key to enable a subnet to "follow the sun". A blockchain network that's more environmentally friendly than the traditional cloud? Now that would turn heads.

Notes:
* I suppose this would require tweaks to the remuneration structure for node providers, so as not to overly penalise correctly-handled downtime.
* I guess this could be done with without any random shuffling, actually, so should this be a new topic instead?

-------------------------

saikatdas0790 | 2021-11-17 15:58:52 UTC | #58

This sounds so cool 😋

-------------------------

akup | 2021-11-19 06:18:37 UTC | #59

Hello! I think that shuffling node membership makes network not more secure, but less secure, at some point of view.

Here is the explanation of my thesis.

If I'm the malicious node and I'm monitoring the state of my subnet (to gather sensitive data), then to gather all the data I need to have my node or node of my partners at every subnet. It's quite hard to do and at the moment to achieve the goal of knowing the whole state I need to control/partner-with about 1/13 of the nodes. But when I'm going to be shuffled across the subnets, I just need to wait while the whole state will be gathered.

What do you think? Am I missing something?

-------------------------

rubenhorne | 2021-11-19 17:23:26 UTC | #60

What you @akup are describing is data privacy, and yes, the data would be less private with node shuffling (unless some sort of encryption were used to obscure the data from the node owner). However, the "security" of a subnet being discussed above was regarding how many nodes would need to gang up to manipulate or crash the data fidelity in their subnet, which is a harder feat.

-------------------------

akup | 2021-11-19 20:28:54 UTC | #61

[quote="rubenhorne, post:60, topic:7478"]
However, the “security” of a subnet being discussed above was regarding how many nodes would need to gang up to manipulate or crash the data fidelity in their subnet, which is a harder feat.
[/quote]
I definitely understand this. Just thought that it should be mentioned by authority for the community :) Thanks!

-------------------------

jzxchiang | 2021-11-19 23:14:59 UTC | #62

Really interesting idea, although that would mean a subnet's nodes are no longer geographically distributed across the world. That could hurt latency (like for a DFN) as well as hurt decentralization (e.g. at night a subnet could be fully running in a single geopolitical region of the world).

Maybe instead of completely zero-carbon subnets, we could tweak the node incentives or shuffling algorithm to prioritize renewable energy data centers (although I don't know how you would prove that you're using renewable energy...).

-------------------------

NickM | 2022-03-25 13:46:40 UTC | #63

@MalcolmMurray thank you for raising this point, which should have greater prominence, maybe even a dedicated thread.

Looking into the environmental impact of blockchain txts and nfts, etc. led me to the IPFS (FileCoin) website where they describe their approach. Firstly, they publish all energy usage. Secondly, they trialed a scheme at the end of last year where "bakers" (their miners) are awarded certificates for purchasing energy from renewable sources. If successful, it looks like peeps publishing / storing on IPFS can choose to have their data hosted by these certificated bakers (not a hundred percent sure I have the terms right, but am sure you get what I mean).

I'm very interested to understand the environmental impact of different dapps and activities on the IC from a climate impact perspective. 

Personally, transparent reporting of energy consumption just feels like a no-brainer starting point. 

Does this fall into the realm of a community proposal to the NNS?

-------------------------

MalcolmMurray | 2022-03-25 16:53:43 UTC | #64

Thanks @NickM

I started a thread a while ago, which didn't gain much traction: https://forum.dfinity.org/t/zero-carbon-subnets/8704?u=malcolmmurray

Yes, I'd love to see an NNS proposal for the community to voice sustainability as a priority and a potential competitive advantage for the Internet Computer.

-------------------------

NickM | 2022-03-25 17:44:30 UTC | #65

Thanks @MalcolmMurray, I have added to that thread :slight_smile:

-------------------------

Zane | 2022-03-26 16:20:45 UTC | #66

Have there been any updates on this topic? It seems to be scheduled for Q4 2022.

-------------------------

jzxchiang | 2022-04-18 03:16:29 UTC | #67

Is this feature related to the feature on the [roadmap](https://dfinity.org/roadmap/) titled "Nodes can be reassigned to a different subnet"? That one is marked as Deployed.

-------------------------

Manu | 2022-04-19 09:52:49 UTC | #68

That feature "nodes can be reassigned to a different subnet" is what the name suggests: Before nodes could only join one subnet and then have to be redeployed before they can join a different subnet (for no good reason), and that limitation has been taken away. This feature is indeed done. 

Yes you could say they are related: being able to switch subnets is a prerequisite for subnet shuffling.

-------------------------

