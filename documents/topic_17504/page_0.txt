dominicwilliams | 2022-12-17 04:31:25 UTC | #1

https://forum.dfinity.org/t/discussion-public-subnets/16503?u=dominicwilliams

Regarding the referenced thread, I want to share some technical explanations to help clarify discussion, and also provide some points for consideration. I will start by explaining how and why Generation I & II blockchains support the local processing of blocks and transactions, then explain how and why the Internet Computer, as a Generation III blockchain, uses a different model, which does not require the local processing of blocks using e.g. "local nodes." Finally, I'll raise some technical issues that would be involved in adding that functionality for use in special cases, such as with the Network Nervous System DAO, and raise some societal, security and regulatory issues involved with making all its data, and the interactions with it, totally transparent.

**Why Generation I & II blockchains involve local processing of blocks**

Generation I (e.g. Bitcoin) and Generation II (e.g. Ethereum) blockchains provide a model for secure interaction that cannot work for Generation III blockchains. Their networks maintain copies of all the blocks of transactions added to the chain and processed. When a party wishes to securely interact with the blockchain, in a manner that does not involve trusting some centralized actor, such as a crypto exchange, or Infura or Alchemy, say, they must download a copy of the blocks, and re-run all the transactions locally to construct a copy of its current state. Since the blockchain protocols involved make it possible to verify that the blocks downloaded represent the correct chain, the party then knows they are interacting with a correct copy of the state, and they can also see past cryptocurrency transfers, and the results of smart contract computations, say. This enables them to safely interact, for example by creating new transactions. 

A self-hosted ("decentralized") Bitcoin wallet must download the blockchain's past blocks, and re-run the transactions they contain to calculate its bitcoin balances and obtain the transactions sent and received. Meanwhile, those creating Web3 services using Ethereum, say, will typically build the website on a cloud account (for example one provided by Amazon Web Services) by installing a web server, database and Ethereum "local node" that downloads its blocks, and re-runs the transactions, to create a trusted local copy of its smart contracts and data that the website code can interact with. A key problem has emerged with this model over time, which been that as these blockchains grow, it has become more and more expensive to download the blocks and re-run their transactions.

As early as 2013, I can remember my self-hosted Bitcoin wallet taking hours to initialize, and today Ethereum local nodes can often take days to initialize when started, even when running on a very powerful computer. As the result of this key problem, few people now use traditional self-hosted Bitcoin wallets, and when they do, they choose a self-hosted wallet that relies on recent state checkpoints, which involves trusting the checkpoint creators, or worse, and much more commonly, they just keep their bitcoin on a centralized crypto exchange like FTX. Meanwhile, the vast majority of Ethereum developers now simply build blockchain websites using code that interacts with centralized blockchain infrastructure services such as Infura and Alchemy, choosing to trust them as a tradeoff against the cost of running a local node, hoping that those services have not been hacked and aren't malicious, and also trusting the clouds that these services themselves run on.

For this and other similar reasons, the majority of participants in Generation I and II Generation II  ecosystems no longer verify their interactions with their blockchains in a trustless and decentralized way, and instead rely on trusting centralized actors. This not only reduces their security and resilience (for example, Infura has suffered outages because Amazon Web Services has gone down), but also lays them open to being disrupted, for example by regulators who impose conditions upon those centralized actors. This means the architecture that has become the status quo is far from desirable, and those building Generation III blockchain need to leave this old model behind. However, the referenced thread discusses adding support for this traditional model, which is not currently supported by the Internet Computer, for use in specific cases, 

**Why generation III blockchains cannot easily use the old model**

Because Generation III blockchains (today, only the Internet Computer) must operate at massive scale they can't easily use the old model. Ethereum only processes a handful of transactions a second, while the Internet Computer, was already recently processing more than ten thousand transactions a second, and we hope that one day it will process many millions or billions of transactions a second. Indeed, the network has already processed more than 1.5 billion blocks, and one day will have processed quadrillions of blocks. It is hard to see how developers could run local nodes that could download such large numbers of blocks and replay their transactions economically. Moreover, it would be extremely expensive for the blockchain to store every past block for later download by local nodes.

Another reason why the old model isn't generally suitable, is that Generation III blockchains must host smart contracts that can directly and securely serve content to end-users, including transmitting content into web browsers, whose pages then directly interact with hosted smart contracts, closing the loop ‚Äì as this is the only way to obtain the genuine end-to-end decentralization required for security, liveness and censorship resistance purposes, and to allow Web3 services to run fully on-chain under the absolute control of community DAOs, fulfilling a key Web3 promise. Even if some technically feasible way to transparently embed local nodes inside web browsers was found, perhaps by making the local nodes more efficient using zero knowledge proofs, say, so that direct interaction could be enabled using a hybrid of the old model, it's doubtful anyone would want this!

**Forming a blockchain from "subnet blockchains"**

The Internet Computer doesn't need the old model, but understanding why involves an understanding of how it is formed from "subnet blockchains", and how they work. Each subnet provides the blockchain with additional capacity for hosting smart contracts, allowing it to scale as required. The entire network is directly controlled and managed by a permissionless governance DAO called the "Network Nervous System" (NNS). This forms new subnets from node machines, which are special hardware devices owned and operated by independent "node providers" who install them in independent traditional data centers located in a variety of geographies and jurisdictions around the world.

The NNS creates new subnets very deliberately, by instructing sets of nodes in the Internet Computer network to combine that are sufficiently decentralized, when considering their node providers, data centers and locations, that a) when combined by the network protocols involved they can provide the necessary security and liveness guarantees expected of blockchain, while b) at the same time the minimum possible number of nodes can be combined to reduce the replication of data and computations and thus the cost of running smart contract software. There are different types of subnet, with different replication levels, which host smart contracts with different guarantees, and different operational costs. This approach is unique to the Internet Computer and is called "deterministic decentralization"

The NNS lives on its own subnet, which was, naturally, the first subnet created at mainnet Genesis, since it was responsible for creating every other subnet that exists. A game-changing innovation is that each subnet blockchain has its own unique "chain key," which it uses to sign its interactions with users and other subnets, and the public chain key never changes, even as the subnet has nodes added and removed. The chain key of the subnet that hosts the NNS serves as the Internet Computer network's "master chain key," which like other chain keys never changes, and the NNS uses this master key to sign each new subnet's chain key, so it can operate as part of the network.

**How subnets maintain and use their chain keys**

Chain Key Crypto is the name for the special protocol math and cryptography at the heart of the Internet Computer blockchain that makes it possible for subnets to have chain keys. A chain key is produced using threshold cryptography schemes, which involve one public key, and lots of individual "private key shares" that are held by the different nodes in the subnet. This might sound simple, but its not: the complexity is in creating the private key shares in the first place, using an NIDKG (Non-interactive Distributed Key Generation) protocol, and key resharing protocols, which allow the subnet to have nodes added and removed without causing a change to its public chain key. The resharing process in fact runs constantly, with the aim of defeating "adaptive adversaries" that wish to incrementally steal a consistent threshold of private key shares from nodes, by constantly remaking the shares.

So long as a threshold number (i.e. a subset of a certain size) of the nodes are operating correctly, then the subnet can create signatures using its chain key. This is important for the functioning of the blockchain consensus protocols, and also for signing the output of consensus, including the results of newly processed update call TXs, and pre-signing (pre-finalizing) a Merkle root of certified query call TX results, which can then be returned without going through consensus. Threshold signing is Byzantine fault tolerant, which means that faulty (i.e. arbitrarily bad nodes) cannot stop a threshold of correct nodes signing, which property is also leveraged by the blockchain protocols in the way they ensure that subnets are Byzantine fault tolerant (i.e. so they continue operating without any corruption to data or function even when a portion of their nodes are arbitrarily faulty).

**Enabling scaling and direct interaction using chain keys**

Given the current status quo, it is easy to imagine that a blockchain by its nature *must* allow users to download past blocks to verify its state, which leads to a misconception that it is a necessary part of blockchain security. However, the requirement is really just that a blockchain provides a tamperproof and unstoppable platform that supports autonomy and is trustless because no centralized actor can do anything other than submit legal transactions to create updates to the state. While it might seem necessary to download the blocks, and replay the transactions to reconstruct the current state, because of past practices, it is possible to devise other systems that can be proven equally unbreakable by mathematics in which cryptography takes care of the verification. This is what Chain Key Crypto protocols do for the Internet Computer.

The availability of chain keys is core to how this is done. When your software interacts with the Internet Computer, the results returned are signed by the chain key of the subnet hosting the smart contract software involved, which key in turn is signed by the blockchain's master key. Thanks to the way the protocol math works, if the chain key signature validates, this not only tells you that your interaction has not been tampered with, but also that the subnet blockchain returning the result is running correctly and that neither its state or computations have been tampered with (which could only be achieved by corrupting the subnet blockchain involved, for example by overwhelming the fault bounds of its Byzantine Fault Tolerant protocols). This means that it is not necessary for you to download its blocks and re-run the transactions so you can be sure of what is happening.

**Chain key signing thus makes three wonderful things possible. Firstly, when two smart contracts interact that are hosted on two different subnets, the subnets can securely pass the traffic involved without needing to download and process the other's blocks, since they can simply check the chain key signatures on the traffic instead to know that a) the traffic has not been tampered with, and b) the sender is also running correctly. This means that one unified blockchain environment can be created by combining any number of subnets, and that capacity can be scaled by adding new subnets. Secondly, user software, such JavaScript in a web page, can securely directly interact with smart contracts hosted on the Internet Computer by checking the chain key signatures on results, and indeed, the assets composing such web pages, can also be signed and directly served by smart contracts. Thirdly, subnet blockchains can discard blocks they have already processed when the protocol no longer needs them, saving node machines from having to store them ‚Äì which might quickly exhaust their memory if they are processing a block a second, say.** 

In summary, a blockchain is defined by the properties it provides to hosted ledgers and smart contracts, such as making them unhackable (tamperproof), unstoppable (always keeping them live) and supporting autonomy (i.e. tokens and smart contracts that exist independently of any centralized party, which provides for sub-properties, such as being censorship resistant). It is the provision of such properties that define what blockchain is, not the technical mechanisms used to create  a blockchain network. This means that the Generation I/II model in which participants locally download and process copies of a network's blocks to secure interaction and ensure copies of the "correct" chain are maintained, is just one possible technical solution. What matters are the provable mathematics of the protocols involved, and the associated security analyses, which show that the required properties are provided. Chain Key Crypto, while undeniably complex, meets these requirements while allowing for the production of Generation III blockchains that can play the role of a World Computer.

**Thinking more about what have been called "public subnets" in the referenced thread** 

Firstly, it's worth mentioning that things went a little awry because of the nomenclature used by the topic, which talked about "public subnets," which imply that today's subnets are private, or something, when in fact they are all public and can be accessed by anyone and run under the control of the permissionless Network Nervous System DAO. What was meant, was today's subnets do not allow arbitrary access to their current state, and that smart contracts can keep their data private (i.e. your only route to the data hidden inside a smart contract you do not control is through the logic it makes available to you). To gain "unauthorized" access to such smart contract data you need to obtain physical access to a node machine in the subnet that hosts it (and misusing such physical access will soon become harder when node machines switch on their SNP-SEV hardware privacy technology). Furthermore, subnets do not provide access to the previously processed blocks of transactions that created the current state.

The referenced post was only concerned with providing access to smart contract state, and the transaction history that created that state, for specific subnets, such as the subnet that hosts the NNS. Meanwhile, the use of the term "public subnet," which implied existing subnets are not public, created rather a lot of discussion. Regarding the proposed changes, I do not personally have super strong opinions one way or the other at the moment, but I will highlight some important things to think about.

**The technical challenges involved**

1. The NNS subnet is processing around 1 block of transactions a second. Should it make those blocks available for download, so that people could try to verify its state using the Generation I/II "local node" model, then each participant running some kind of local node would have to download and process 86,400 blocks a day. Depending upon how many people wished to do that, an enormous bandwidth overhead would be created, which would be expensive, and it's not clear how that could be measured and node providers compensated via the protocol or NNS. No doubt it could be worked out, but the network would still have to bear a very substantial additional expense. 

2. Thanks to Chain Key Crypto math, Internet Computer subnets do not need to keep old blocks around for long, which avoids filling up the memory of the node machines unnecessarily. With some modifications, however, nodes could keep, say, the last 3,000 blocks around (50 minutes worth), which would allow those running "local nodes" to resync if they get disconnected momentarily. Every sync/resync would be very expensive though: a) first a syncing "local node" would have to begin downloading a checkpoint "snapshot" of the entire state, while storing every streamed block thereafter, then applying the collected blocks to the fully downloaded snapshot state to catch up, and b) any "local node" that got disconnected for more than 50 minutes would have to restart this expensive procedure. This magnifies the bandwidth expense involved in streaming the blocks, and might create a DDoS vulnerability that attackers could go after. Moreover, even if only one snapshot were created an hour (and the previous one discarded), this would halve the amount of state that could be stored on a subnet.

3. One of the most amazing things about the Internet Computer is that it is self-updating. That is, a proposal can be submitted to the NNS DAO to update the protocol using a new binary image produced by some referenced source code, and if the proposal is adopted, the image is then used to update the node machines in the network ‚Äì entirely automatically. The problem here is that the meaning and processing of blocks can change as the protocol design and implementation evolves. This means it would be impossible (without insane engineering effort) to create a "local node" system that could record a state snapshot today, say, and then every block after, for weeks, months or years, that would be sufficient to re-create the current state, since the logic would have to change whenever a block height is hit where a protocol upgrade occurred. Therefore, syncing a "local node" will always involve downloading a snapshot that is less than an hour old, and applying subsequent blocks ‚Äì which I feel isn't what people really want here.

**Questions about "opening" the NNS** 

The essential idea is that by creating a new kind of subnet, people will be able to see the insides of the Network Nervous System/NNS, and every transaction (smart contract function call) that updates it.

1. Given the aforementioned technical challenges involved in creating a new kind of subnet that can support "local nodes", if it were decided that the insides of the NNS must be made available to all, then there is an easier way forwards that involves a tiny fraction of the work, which is simply to add functions to the NNS's smart contracts to allow people to obtain any information inside they want. Of course, alone this would not reveal the transactions (i.e. how users have interacted) with the NNS to create the information being made available. However, with some rather more substantial work, we could also add logging to the NNS, so that anything and everything anyone did within the NNS, such as staking a neuron, configuring a follow, or voting, would be maintained, say for a couple of days, and also made available. Because the Internet Computer's Chain Key Crypto protocols create subnet blockchains that are tamperproof, there would be no risk that information returned through such APIs were modified by malicious actors, and we could securely make everything available to anyone.

2. We must ask serious questions about whether doxxing people's NNS interactions and data would really contribute to a healthy democracy. For example, there are reasons that when you go to a polling booth in an election, your vote is anonymous and private. Arguably, for similar reasons, how much you've got staked in neurons, and for how long, who you follow, how you vote on motions, and the links to e.g. your balances of ICP and other governance tokens, should also be kept private. Our community is not immune to toxicity, and those with substantial voting power might find themselves being pressured by zealots keen get their way who wish them to change their neuron follows and/or vote for/against specific proposals. Moreover, they might even be blamed and persecuted for how they voted in the past. The danger is that quiet and reasonable people might be forced out of the governance community, and the useful inputs they provide through voting would be lost.

3. Doxxing people's NNS interactions and data could cause both security and regulatory risks. Firstly, it would provide attackers with accurate information on exactly who they had to nobble, blackmail or extort, to push through proposals that further their goals. Where gangsters are involved, and sadly nowhere is completely free of dangerous actors, participants in the governance community could find themselves in horrible and dicey situations, and bad actors might cause some real harm to the network by pushing through bad proposals, for example to profit from shorts on ICP. Secondly, aggressive regulators out to make a name for themselves by harming the Internet Computer ecosystem might seek to hold participants in the governance community responsible for the adoption of proposals that they do not like, according to how they voted. Thus arguably, revealing this information could create serious risks for both individuals and the network. 

**Summary**

As is often the case in blockchain, unpacking problems often reveals them to be more complex than they seem on first sight. It is necessary to understand how the technology works in detail, how it might be modified, and consider the potential regulatory, security, game theoretic, tokenomic, socionomic and economic implications. The potential R&D costs and distraction involved with making NNS interactions and data transparent, should we wish to do that, also have to be weighed against the need to ship and polish many other things, such as the SNS functionality, or Ethereum chain key integration.

A last comment is that we can more easily travel in the direction discussed in the referenced thread with the governance token ledgers. Currently, they are implemented in the mode of a blockchain-within-a-blockchain. That is, transactions are recorded in a hashed chain in which each transaction forms one block that lives on the Internet Computer. This, for example, is what enables crypto exchanges to interact with the ICP ledger via the Rosetta API, and meet regulatory demands that require them to have knowledge of every transaction. We could look at ways to modify this so that the signatures on interactions that ultimately caused a transaction are stored with the transaction. This would make it possible to re-run every transaction on the ledger, and prove that some user did not transfer some tokens that they owned, and therefore still should still possess them, independently of what the Chain Key Crypto protocol of the subnet hosting the ledger says. Note that even here there are non-obvious challenges though ‚Äì what would be the situation if the user had originally received a balance in question via a transfer made by an autonomous smart contract invoked by a blockchain heartbeat!? The tl;dr is that leaning on the hard math of Chain Key Crypto is much easier.

-------------------------

nlh | 2022-12-17 05:12:04 UTC | #2

Thank you, Dom. I have to admit that everytime I read your writing I learn a few new things about IC, and why some decisions were made from a technological point of view.

-------------------------

JxBrian | 2022-12-17 05:20:31 UTC | #3


I felt like a first grader reading this. Surely there is a lot I have to catch up on üôèüíØ Thanks for the in depth response regarding network systems. It sure does clear the air on some misconceptions that were going around.

-------------------------

Sormarler | 2022-12-17 05:24:39 UTC | #4

I had to have my phone read this to me. So much to unpack there. So much to learn.

-------------------------

Astrapolis-peasant | 2022-12-17 11:33:30 UTC | #5

We well understood the privacy concern with opening the NNS subnets. But shouldn‚Äôt that be done in a cryptographic way such as Zero Knowledge or Ring signatures. Hiding the data to only allow insiders to peek creates discrimination to regular end users but privileges to the insiders.

-------------------------

dominicwilliams | 2022-12-17 14:43:42 UTC | #6

Yes, I have long believed that we should probably make neuron follow relationships and voting inaccessible to even those who had physical access to the node machines involved (and, since the node machines will eventually, hopefully soon, turn on SEV-SNP hardware privacy protection, even to those who have physical access and can additionally break that protection).

The origins of the Network Nervous System can be traced to The DAO fiasco and hack on the Ethereum network in the summer of 2016. Firstly, this revealed the need for better DAO designs, which didn't depend on a cabal of high-profile human "curators" to make decisions and used mechanisms that couldn't be so easily gamed (although in the end, The DAO was exploited by a simple reentrancy flaw in its Solidity code, rather than through its vulnerability to nefarious voting strategies, and other game theoretic vulnerabilities that I was worried about). Secondly, because the fiasco resulted in a hard fork of the Ethereum network, the need for a blockchain network that could be easily and regularly updated by a DAO, rather than by cabal of blockchain insiders orchestrating a hard fork among the miners, with all of the political, ethical and security issues, and sheer friction, that hard forks entail, was revealed. Although I had been working on how to build a World Computer, funding limitations at the time meant that my early ambitions were pretty much limited to building a sister version of Ethereum that ran under the control of a DAO, with some limited improvements to consensus. 

During 2016, I created a simple proof-of-concept implementation of such a DAO, which was really just a privileged smart contract that had access to special  Ethereum op codes invented for purpose. Anyway, if you [look at my early proposals for a "Blockchain Nervous System" from January 2017](https://medium.com/dfinity/the-dfinity-blockchain-nervous-system-a5dd1783288e), you'll see that I proposed storing neuron follow relationships on user devices, in client software that would run e.g. on the phones of neuron owners. This provided a simple way of making the neuron follow graph completely inaccessible to all, and I also proposed introducing random time delays during automatic voting to disguise the follow graph from those temporally tracking voting activity (remember that on a simple Ethereum 1.0-like blockchain, every individual transaction can be downloaded).

Of course, today, almost 6 years later, many aspects of how I would wish governance to work have changed from the original vision, but I personally still believe that there is value in making the follow graph and voting completely private using cryptography. Voting could perhaps be disguised using cryptographic blinding. Disguising the follow graph would be more challenging using cryptography, although I'm sure DFINITY's cryptographers could find ways of doing that if set to the task, and if not, potentially we could revert to my original shortcut ‚Äì which involves the creation of special NNS client software to hold neuron follows, which would run on phones and laptops. 

A basic case for privacy was expressed in that original post: "One dimension of security we did not mention in the foregoing is privacy. The neurons are managed at the edge of the network for a reason ‚Äî at a fundamental level, it makes the follow graph and decision process unknowable, since it will be impossible for an adversary to collect the state of the thousands of privately operated client instances distributed around the Internet. This prevents an adversary targeting critical nodes in the graph, for example via extortion or kidnapping, to increase his chances of having proposals adopted. Furthermore, it also means that such critical nodes cannot be held accountable for decisions, for example by angry owners of systems that are frozen, or governments or agencies that believe legal liabilities should stem from decision making."

One final note. When describing the need for privacy in that original post, I said I wished to protect neuron owners from "angry owners of systems that are frozen." This should not be interpreted as a wish to support automated governance that generally censors smart contracts for political, competitive or other reasons (i.e. to advance the agendas of a majority whose voting might controls governance). I was talking specifically about the presence of "assassination markets" and "ISIS slave markets." Currently, the NNS does not perform any such censorship of smart contracts, and such nefarious and highly illegal systems are currently simply blocked by the operators of boundary nodes when discovered (and in the future, it will be for each boundary node operator to decide on their own black list to protect themselves from the consequences of forwarding such traffic).

I stand by the opinion that aberrations such as assassination markets and ISIS slave markets have no place on decentralized blockchains, whose aim is to enrich and improve the human conditions, and one way or another, we must develop acceptable ways of preventing them running on World Computer blockchains, which by their nature are vastly more powerful that traditional blockchains. The purpose of having boundary node operators maintain block lists is to decentralize censorship to those who will bear the practical consequences of forwarding such traffic (typically, a data center will disable a boundary node machine once it is shown to be serving illegal content after a complaint from law enforcement, say, unless remedial action is quickly taken). Hat tip to everyone who advocated for this approach in earlier community discussions.

-------------------------

JaMarco | 2022-12-17 22:35:51 UTC | #7

[quote="dominicwilliams, post:1, topic:17504"]
Given the current status quo, it is easy to imagine that a blockchain by its nature *must* allow users to download past blocks to verify its state, which leads to a misconception that it is a necessary part of blockchain security. However, the requirement is really just that a blockchain provides a tamperproof and unstoppable platform that supports autonomy and is trustless because no centralized actor can do anything other than submit legal transactions to create updates to the state. While it might seem necessary to download the blocks, and replay the transactions to reconstruct the current state, because of past practices, it is possible to devise other systems that can be proven equally unbreakable by mathematics in which cryptography takes care of the verification. This is what Chain Key Crypto protocols do for the Internet Computer.

The availability of chain keys is core to how this is done. When your software interacts with the Internet Computer, the results returned are signed by the chain key of the subnet hosting the smart contract software involved, which key in turn is signed by the blockchain‚Äôs master key. Thanks to the way the protocol math works, if the chain key signature validates, this not only tells you that your interaction has not been tampered with, but also that the subnet blockchain returning the result is running correctly and that neither its state or computations have been tampered with (which could only be achieved by corrupting the subnet blockchain involved, for example by overwhelming the fault bounds of its Byzantine Fault Tolerant protocols). This means that it is not necessary for you to download its blocks and re-run the transactions so you can be sure of what is happening.
[/quote]

But doesn't a subnet signing with chain-key just prove that the subnet nodes came to consensus on the blocks/state, not necessarily that the computation was performed correctly. How does a chain-key signature prove that the subnet wasn't overrun by malicious nodes and signed invalid blocks/state transitions? Only way to prove correct computation is to run a node yourself and download/run all the blocks and computation yourself (or generate zk proofs of the computation).

-------------------------

Sabr | 2022-12-18 18:11:46 UTC | #8

@JaMarco, But why should this downloading of all prior transactions/blocks always be necessary to prove state if state is always being confirmed by consensus too? 

It's the same concept as an audit of a company's trial balance, which is what financial statements are derived from. When financial auditors come in to audit the 2022 year of a company like GM, formed in 1908, they don't start by "proving the calculation" of its trial balance at Dec. 31, 2022 -- i.e., by recalculating all 114+ years of its general ledger transactions for the original company and its thousands of acquired and sold companies since inception. No one does that, nor will anyone ever do that again, even if they wanted to.

That said, it would not hurt to have the equivalent of the Wayback Machine, which tracks prior versions of Internet pages. Someone could store the "since inception" blockchain offline to analyze (and potentially investigate) it in great detail for any anomalies or lack of 100% consensus by the "node auditors". That could at least provide an additional level of confidence, even if it is not a real-time internal control.

-------------------------

JaMarco | 2022-12-18 19:48:06 UTC | #9

[quote="Sabr, post:8, topic:17504"]
@JaMarco, But why should this downloading of all prior transactions/blocks always be necessary to prove state if state is always being confirmed by consensus too?
[/quote]
Because consensus doesn't deterministically prove computation is correct, it just proves that the majority of nodes agreed it's correct. If that majority of nodes are malicious they could "agree" on invalid computation/state transitions.

-------------------------

Sabr | 2022-12-18 20:29:11 UTC | #10

Sure, just like any other blockchain, which could theoretically become compromised, either via incorrect calculations or human intervention. This is why I suggested a type of "Wayback Machine" to at least detect this retroactively, even though it would not prevent such a compromise in real-time. 

Either type of compromise could even happen on the Ethereum blockchain, though the latter (human intervention) would be more likely. The majority of Ethereum nodes are controlled by only a handful of EVM node providers and a couple of major hosting providers. Even without coercive force upon the few individuals necessary to take over the majority of the Ethereum blockchain nodes, all it would take is a billionaire with money to burn. Based on my understanding, that billionaire could simply spin up enough nodes to change the state of the Ethereum blockchain, since it is based on permissionless node providers (unlike the IC, which requires NNS permission and approved hardware to join).

However, as we have seen, a blockchain compromise could happen far easier than in that scenario. Ethereum, as the largest smart-contract blockchain in the world, has essentially been compromised already. The censorship power of a single, bugger-eating bureaucrat waiving around a little memo was all it took to threaten node providers enough that they were willing to shut down Tornado Cash and a whole list of censored ETH addresses. So is **any** smart contract blockchain truly "unstoppable" or "non-censorable" today? Not likely.

EDIT: One more quick point. What I like about the IC node providers being restricted by NNS approval and consistent hardware standards is that 100% of all divergences from majority consensus can be easily investigated, as they have been in the past (per DFINITY R&D folks). There have been almost no divergences at all since genesis, let along any that would come close to threatening a majority consensus. The few that did occur were all related to bugs or similar software/hardware upgrade issues, again per DFINITY R&D. Such an investigation would not be possible or even legally feasible to conduct upon all the diverging Ethereum node providers. Think about it.

-------------------------

Mr_Burkes | 2022-12-18 20:50:36 UTC | #11

This is my biggest gripe as well...

Just being able to verify that the IC performed and signed a computation does NOT necessarily guarantee that a computation was performed correctly. That is why all other blockchains allow you to download and verify their entire state and state mutations.

-------------------------

dominicwilliams | 2022-12-18 22:17:09 UTC | #12

Stepping back for a moment from the specific workings of different blockchains, we can generalize, and say that all good blockchains are versions of Byzantine fault tolerate (BFT) networks. That is, they use protocol math that guarantees that so long as the proportion of "faulty" nodes (i.e. nodes that can behave arbitrarily, including going offline, corrupting data, subverting the protocol, and colluding with other faulty nodes in any way they choose) stays beneath a defined "fault bound," then the network will continue running correctly.

What we care about is the *probability* that liveness might be lost, or that an adversary might have cause some invalid state transition. We also care about the presence of vulnerabilities to specific kinds of attack, such as DDoS, which might cause the network to lose liveness, say. To understand the probabilities, we must mathematically analyze the protocols, and also analyze the probability of the fault bounds being exceeded given the specific network model being used. We have to be careful here, because the mathematical analysis of BFT protocols, cryptography, and aspects of complex network designs, and reasoning about fault bounds is difficult, which has led the existence of simple blockchain design rubrics that are easy to understand, but oftentimes become misunderstood as necessary and immutable laws of design. The Internet Computer community often bumps up against these, because it takes a different approach to nearly all aspects of blockchain design.

As I mentioned in earlier posts in this thread, we must define "blockchain" by the properties it can provide as a platform, such being tamperproof, being unstoppable, and supporting autonomy, not by specific modalities of design, including whether someone can verify signatures on the transactions processed themselves, to add security. What matters, is that the blockchain provides guarantees that an invalid transaction isn't processed, say, which requires more sophisticated analysis. The Internet Computer is a Generation III blockchain, which does not work like earlier blockchains in nearly any way, but it provides the key properties that define blockchain extremely well.

Regarding the subnet blockchain design used, I think what you allude to is that if a large portion of a subnet blockchain's nodes were faulty (i.e. controlled by an adversary), then they could in principle modify the subnet's state without being detected. This is correct in theory ‚Äì but what matters with any theoretical possibility, is not just its potential impact, but the probability that it could occur. Also, what we discuss here is just one of many failure modes that can be experienced once a blockchain network's fault bounds are exceeded and its Byzantine fault tolerance stops working. For example, although blockchain networks incorporating vast numbers of validators that check the legality of every transaction and state transitions prevent unobserved bad behavior, they are often much more susceptible to adversaries gaining control of their networks, and should that occur, the validators performing the checks cannot also prevent the creation of new branches that reorder the transactions, perhaps allowing the adversary to double-spend a large token transfer, or wreak some havoc within DeFi in a way that profits them. Even when such network's fault bounds are not exceeded, simple design weakness can allow for this problem or that, as we see with [miner extractable value (MEV) on Ethereum](https://www.bis.org/publ/bisbull58.pdf). The importance of all potential issues, is some kind of product of the impact they would cause with the likelihood they will occur.

The question we must ask, then, is what is the likelihood that a "fiduciary subnet" on the Internet Computer, say, gets so corrupted that it can whir away doing illegal state transitions unnoticed. Such a subnet is currently comprised from a minimum of 34 node machines, although the protocols used actually scale very nicely, and allow subnets to be constructed from hundreds of nodes if desirable. The nodes involved are selected by the NNS using a system of deterministic decentralization. This means that they are owned and operated by independent node providers, and installed in different data centers in different geographies and jurisdictions. This is very different to randomly selecting anonymous nodes in a typical Proof-of-Stake network, within which an adversary might be running a large number of nodes ‚Äì such that if the random selection of a group is unlucky, nearly all the nodes might belong to the adversary. Moreover, an adaptive adversary will find it very difficult to corrupt honest nodes (or more specifically, their owners). Node providers are not anonymous, and if they maliciously collude and are found out, they might be held accountable for their actions, and any node provider approached by the corrupting adaptive adversary might only pretend to be corruptible, continuing in the mode of a mole to expose their operation. Further, where node providers are companies, numerous people within their organizations might choose to blow the whistle on suspicious activity. 

Internet Computer subnets are 3f+1 Byzantine fault tolerant ([which, according to simple math, is the best you can do in an asynchronous network](https://lianghan.org/2017/02/01/2017-02-01-ByzantineFaultTolerance/)), which means that less than one third of the nodes can be faulty to ensure that *no problems can occur*. However  that does not mean that *any problem can occur* if one third or more of the subnet's nodes are faulty. A supermajority of nodes is required to make the network run, which is 2f+1, Since f is 11, a supermajority is 23 nodes. If an adversary wanted to modify the network in secret, they would need to have installed at least 23 faulty nodes, which would a) stop talking to the remaining 11 correct nodes, b) perform the illegal state transitions desired, and c) start talking to the correct nodes again after a sufficiently long period of time that they are forced to resync using the illegally created state, rather than by re-running cached recent blocks to catchup. The question is, given the aforementioned nature of deterministic decentralization, what is the likelihood of an adaptive adversary corrupting 23 node providers, and getting them to modify the software their node machines run, without getting detected. We claim that this is extremely unlikely.

Before discussing PoS blockchains to cover why they need more validators, let's first review the three-layer consensus mechanism that Internet Computer subnet blockchain protocols use, and consider what can be achieved to add more security simply by adding nodes, without any need to create a system of external validators. 

The first layer is a random number generator, called Threshold Relay, which generates the random numbers using BLS threshold signatures (each signature is itself threshold signed to produce the next random number), which is possible because BLS signatures are unique and deterministic. The second layer is a highly-consistent blockchain protocol, called Probabilistic Slot Consensus, which is driven by the random numbers. Only blocks that are fully validated can be added to the chain. The third layer finalizes blocks in the chain, and is called an Optimistic Asynchronous Finalizer, which instantly anoints blocks as finalized when it is successful, such that they cannot be overtaken by a new branch, making it unnecessary to wait until a block is buried to some depth. The nodes ("replicas") only process the transactions in blocks once they have been finalized by this third finalization layer, which means that nodes never have to "rewind" its state after a branch collapses.

The blocks added to the chain by the second (blockchain) layer must be signed by a 2f+1 supermajority, which validates that they contain no invalid transactions (i.e. that there are not transactions that break the rules, and none which are incorrectly signed). If a correct node receives an invalid block, then the signatures on that block by faulty nodes would create an incontrovertible cryptographic proof that the faulty nodes were indeed faulty. So to advance the chain illegally, they need a supermajority, but they also need the supermajority so they can exclude the correct nodes to prevent them bearing witness. The exclusion of the correct nodes would inevitably raise attention, but that's another matter.

I don't want to go too deep here, but the tl;dr here is that rather than modifying the architecture, if we want/need to increase subnet security, and reduce the chance that some adversary could do this, the easiest means is simply to add more nodes, not adopt an old-fashioned validator model. It can be made arbitrarily hard for an adversary to corrupt a 2f+1 supermajority of faulty nodes by increasing the size of the subnet, and thus the number of node providers they must corrupt. For example, what if there were a thousand node machines from a thousand node providers! It would surely not be credible that an adversary could manage to bribe 667 node providers, and persuade them to collude, all without being reported and caught. The point here is that security exists on a cost curve, and after a while, security becomes so strong that additional expenditure provides near zero gains. While we can make a subnet arbitrarily secure by adding additional nodes, the gain from adding each additional node diminishes, while the extra expense involved with adding the new node, and thus replicating the protocol cryptography and smart contract data and compute, on an additional hardware device, remains constant. At some point it makes no sense.

Finally, it's worth touching on why traditional blockchains need so many validators. At the time of writing, [Ethereum has 488,586 validators](https://ethereum.org/en/staking/) slurping down Ethereum blocks, and checking that they are correctly formed, thus replicating the ledger and its currency transactions, and smart contract data and computations, 488,586 times. This is unbelievably expensive. These validators, in vast majority, are really just software instances running on centralized cloud services, rather than dedicated hardware, but still, if we imagine them as computers arranged in a line, 1 meter apart, then the line would stretch for an incredible 488km. Anyone can see that that's a lot of checking, and I certainly struggle to find good justifications for the expense as a crypto theoretician. There are other benefits that don't relate to security though, and these might be important: widespread staking in validators reduces the supply of ETH on the market, helping defend its price, and those staking often become vociferous advocates for the network. However, although Ethereum does not need such vast numbers of validators for security purposes, generally speaking, Proof-of-Stake networks hosted by anonymous validator nodes do indeed need more nodes for security purposes than if they were using deterministic decentralization, say.

Firstly, such networks need to prevent "Sybil" attacks. These occur when an adversary, who is anonymous, can keeping adding additional faulty nodes to the network, until he overcomes its fault bounds, and make it do bad things. For example, an adversary might accumulate a large holding of the blockchain's native cryptocurrency token through a hack, say, then use it to spin up large numbers of validators. To prevent him creating enough validators to breach the network's fault bounds, the network needs to be hosted by an enormous number of validators (which we take as a proxy for stake), which will make it too expensive for him to reach the fault bounds, since he will eventually run out of resources. In this sense, Proof-of-Stake networks hosted by anonymous nodes are similar to traditional Proof-of-Work networks (the Internet Computer is Proof-of-Useful-Work, which is very different), which require miners to do an enormous amount of hashing, which makes it infeasibly expensive for a malicious miner to obtain 51% of the overall hashing power, which would allow them to perform a double-spend attack or stop the chain. Obviously, if you want to go to the next level and provide a World Computer blockchain, this kind of approach is not a good idea.

Secondly, in these kinds of network, the probability that a subset of nodes chosen to construct a shard, or consensus committee in a larger network, say, contain a number of faulty nodes/validators that exceeds the fault bounds of the protocol, is calculated using hypergeometric probability ([you can find a good online hypergeometric calculator here](https://planetcalc.com/7703/)), since the anonymity of nodes always makes their selection random. It's rather like having a bag full of blue marbles, which are correct, and red marbles, which are faulty, and drawing some number from the bag without looking, and without replacement, and then seeing if the proportion of red marbles is too high. For example, if a dynamic shard were created with 34 nodes, drawn from a population of 5000, where 2000 were faulty having been created by a Sybil attacker, then the probability that the number of faulty nodes  >11 is 1-0.2322786150022 = ~77%. This is obviously unacceptable, and is another reason why Proof-of-Stake networks require vast numbers of validators to constrain by expense the proportion of faulty nodes that a Sybil attacker can add, and must also use larger group sizes. For example, if Ethereum had 450,000 nodes (let's pretend they have equal stake for simpler math), and only 50,000 belonged to the resource-constrained adversary after his Sybil attack, then the probability that a randomly consensus committee of 115 nodes selected by the Beacon Chain contains >38 faulty nodes is 1-0.9999999999817 = 0.00000000778%, which is fine.

**Important note:** fears that large numbers of nodes in Proof-of-Stake networks might belong to an adversary are not overblown. Proof-of-Stake "validator nodes" can nearly always run on cloud services, making easy to spin up thousands in seconds by running a script, perhaps using stake obtained through a hack (this compares to the difficulty of spinning up dedicated physical hardware devices, such as Internet Computer node machines, which must be built or purchased, then configured, then installed in data centers, which will also involve arranging minimum-term contracts for the racks used). Moreover, there is a significant risk that the cloud service running the validators, or some malicious employee working there, could surreptitiously commandeer the validator nodes themselves, and turn them into zombies that do their bidding. The potential for things to go wrong was demonstrated recently when [Hetzner flicked a switch and took more than 1,000 Solana validators in the blink of an eye, which number represented 40% of the Solana network](https://decrypt.co/113429/is-solana-decentralized-cloud-provider-hetzner-ban-raises-questions).

-------------------------

jzxchiang | 2022-12-19 00:45:13 UTC | #13

> A supermajority of nodes is required to make the network run, which is 2f+1, Since f is 11, a supermajority is 23 nodes.

Is this purely because of the 3rd bullet point below (i.e. to certify the replicated state)?

![Screenshot 2022-12-18 at 4.43.51 PM|690x351](upload://n0hlKpXP2jWyfBoojfK3NyWCo9H.png)

-------------------------

Mr_Burkes | 2022-12-19 01:57:28 UTC | #14

Dom, thank you for taking the time to explain the technical reasonings behind the IC.

I agree, having a network with 450,000x replication is extremely wasteful. Lots of work is repeated when a lesser degree of replication will do.

However, the benefit of networks like Ethereum is that ANYONE can join and start verifying blocks. This is not the case with the Internet Computer because becoming a validator is permissioned via the NNS. Thus, only node providers at the moment can tell if the state transitions executed by other node providers are legitimate.

What I would like to see, and I think others would as well, is a way to download and verify blocks on subnets. Is this possible? We needn't download the entire state of the chain, perhaps given an initial input state, one could follow the state transitions of canisters and verify that the transitions are progressing as expected. Sort of like a read-only node.

The problem that I notice in the community is that the IC is opaque to everyone but DFinity and node providers. We would like to see with our own eyes that the IC is functioning as expected. I think it would help build a lot of trust.

-------------------------

dominicwilliams | 2022-12-19 13:28:34 UTC | #16

> Is this purely because of the 3rd bullet point below (i.e. to certify the replicated state)?

(repost because didn't quote @jzxchiang above)

Yes that‚Äôs an example. Note that for some things f+1 is sufficient, since a) there are assumed to be at most f faulty nodes in the subnet, so b) a quorum of size f+1 will include at least 1 correct node. But this only works where no ‚Äúequivocation‚Äù is possible, such as in production of the random beacon. In that application, there can only be 1 one prior random number/signature to sign, since BLS is unique and deterministic, and the next signature (which is a random number) on that prior number will also be unique and deterministic, such that no fork is possible (it gives you consensus on a sequence of numbers without consensus‚Ä¶). So there is no wriggle room. But, when you get into consensus protocols things change, because faulty nodes can equivocate. For example, faulty nodes could sign e.g. two different states, or blocks, or finalizations, say, and suitably partitioned correct nodes (which cannot see both states) could be coaxed into helping to sign both (the network is assumed to be asynchronous, so in theory, just by accident, and anyway, the correct nodes might not detect the ‚Äúfork‚Äù but the model also considers that the adversary can could interfere with message delivery over the network to allow their scheme to get proceed). The only way you can prevent this is by requiring 2f+1 signatures. Incidentally, [the math behind this result is pretty simple](https://lianghan.org/2017/02/01/2017-02-01-ByzantineFaultTolerance/) and there is no way to get around it in an asynchronous network (where message delivery times are not guaranteed). Many blockchains have been designed in ways that pretend this result doesn‚Äôt exist, usually because the architects seem not to know about the result for some reason. We know this is the case with Solana, for example, and that‚Äôs why it kept running when Hetzner took down 40% of its nodes (more than 1 third). The result is that simple network perturbances, and nodes processing TX at different rates, can make it go wrong and fork, which has happened quite regularly, and indeed, with time, it would be easy enough to add a modified validator to its network that could fork their chain on demand.

-------------------------

dominicwilliams | 2022-12-19 14:05:14 UTC | #17

[quote="Mr_Burkes, post:14, topic:17504"]
However, the benefit of networks like Ethereum is that ANYONE can join and start verifying blocks. This is not the case with the Internet Computer because becoming a validator is permissioned via the NNS. Thus, only node providers at the moment can tell if the state transitions executed by other node providers are legitimate.
[/quote]

My point is: the only reason that this feature appears to be necessary and/or beneficial, is that "blockchain community consensus" provides misleading social proof that this is somehow technically necessary and good. This leads people to feel that the crypto math cannot be trusted, and that they themselves should be able to download every once-per-second block from every subnet and rerun all the transactions contained to replicate and check the multi-terabyte blockchain states, to ensure that no funny business goes on. After all, how could this not be necessary since this is how every other blockchain works, and they advertise the thousands of cloud validators that keep them safe...?

But the math does says otherwise (as it does about the safety of many other leading blockchains who use this model). In end, we have to trust the math and stay the course with the science, and eschew misleading blockchain rubrics. By adding a feature that enables people to slurp blocks from subnets and replicate their state, we are effectively saying that this is technically necessary, feeding the falsehood, and soon people will believe that any subnet not being replicated by an army of local nodes is unsafe.

There are also several technical challenges involved with this implementing feature (this is not to say they cannot be solved in special case, such as the NNS subnet). Crucially, it is not possible for node machines to store all past blocks forever, since they would eventually consume all their memory, and the math used doesn't require them to do that. This means that if a local node becomes disconnected for sufficiently long, it would have to resync from a recent state, rather than from the blocks it missed: thus, a malicious subnet could simply disconnect the local nodes while it illegally modified the state, keeping them disconnected for long enough they would be forced to later resync from the illegally modified, once the blocks that would have revealed the mischief are no longer available. And there are other issues too. The Internet Computer is just not designed to work like a Generation I or II blockchain ‚Äì albeit with a lot of work we could find solutions, but why?

I also strongly debate that removing the weak-medium privacy that the Internet Computer provides to smart contracts is a good idea. Most people like privacy. Currently, the only way to get data from a smart contract, is by interacting with it. It chooses what data somebody should be able to access. Unless you have physical access to a node machine, and can poke around on its SSDs. However, this isn't much different to the situation with cloud providers, where they could in principle also look at the private data of services they host, and, soon we can enable SEV-SNP on node machines, which will prevent even those with physical access snooping, unless they have special skills and are willing to invest a lot of time.

[quote="dominicwilliams, post:1, topic:17504"]
**Questions about ‚Äúopening‚Äù the NNS**
[/quote]

We can argue that because the NNS runs the whole network, that it needs special oversight, and it is a special case. I am not against that reasoning in principle. But regarding risk, I think I am far more concerned about the consequences of doxxing everybody's voting, neurons, balances etc in the NNS. Democracy depends on privacy to prevent coercion and reprisals. The real reason that some are demanding this technical feature has nothing to do with mathematical security, and lots to do with a feeling that they want to know why proposals they submit are not adopted, and perhaps a subconscious corollary belief that if those voting against them could be revealed, they would find it easier to get their way. To me, that looks like a potentially disastrous way forwards.

-------------------------

Astrapolis-peasant | 2022-12-19 14:15:24 UTC | #18

Currently, Dfinity seems to host a good amount of nodes. Taking that as a concern, technically Dfinity has the access to NNS data, while others don't. Talking about SEV, what if SEV is not properly set up on the node machines that lead the node providers to potentially leak the data

-------------------------

JxBrian | 2022-12-19 14:27:23 UTC | #19

What people should understand is that someone can say one thing (an idea) in say Chinese and when it is translated to English it would mean something else. 

But math is the language of the universe. Numbers don‚Äôt lie. Mathematical proofs are the same no matter in what language they are transcribed. 

That is why makes sense to use math to communicate ideas.

-------------------------

Astrapolis-peasant | 2022-12-19 15:11:01 UTC | #20

Regarding privacy&security issues, having no block histories makes DEFI teams and users feel really unsafe.

-------------------------

ildefons | 2022-12-19 15:31:10 UTC | #21

I don't see any concern for DEFI teams because what seems to be going on is actually 2 questions: 
1) Do we want public networks? which seems to be an easy yes IMO, so DEFI teams should be feeling just fine; 
2) Do we want NNS to be public? this is not clear because we may want to keep some information private like our staked maturiry, voting history and so on

-------------------------

dominicwilliams | 2022-12-19 16:00:43 UTC | #22

[quote="Astrapolis-peasant, post:18, topic:17504"]
what if SEV is not properly set up on the node machines that lead the node providers to potentially leak the data
[/quote]

The node machine SEV-SNP hardware attests that the virtual machine is running the replica/client software inside an "enclave." There's no way to make the SEV-SNP hardware do the attestation if that isn't the case. That's why it protects the data from anyone with physical access, including node operators (or at least from anyone that can't break SEV-SNP)

-------------------------

dominicwilliams | 2022-12-19 16:17:26 UTC | #23

This is just the social proof thing. People get accustomed to one way of doing things, and extrapolate that because everybody is doing it, then that's the only way that it can be done. But it's absolutely not necessary for security according to the math and science imo. 

They should be much more worried about the fact that on other chains a) they run their user experience on the cloud, or use the cloud, which lays them open to hacks as per BadgerDAO, and being blamed by regulators etc etc, and b) that oftentimes, they maintain a centralized "administrator backdoor" master key, which makes it possible for a team member to go rogue, which problem they could solve with a more sophisticated DAO like the SNS.

It IS true that some DeFi services need to track usage to avoid regulatory heat. The prevailing model does allow that to be done through blockchain explorers, such as Etherscan, without requiring anything extra of the developers. But that kind of history tracking is a terrible experience for real users, and is only suitable for blockchain analysis sleuths. 

If a DeFi service wants to make key events and transactions transparent (and this will often be very desirable) a much better way is just to log critical events and transactions inside the DeFi smart contracts, and then provide a nice UI for the log.

Note that something like a DEX will already wish to log transactions, since users will want to see how their trades happened, etc. Imagine having to go to Etherscan to work out how your trades got executed on something like ICDEX. It would be a nightmare. 

Final note: DeFi services running on the Internet Computer can also add features that allow people to run service-specific local nodes that track transactions/state changes. That's what's done with the governance token ledger, which behaves similarly to a traditional blockchain, and can be accessed via the Rosetta API, so that CeFi exchanges like Coinbase or Binance can interface with them.

The key takeaway is that the Internet Computer blockchain provides a tamperproof and unstoppable scalable compute platform for smart contracts. Such is the power of the platform, it's possible to build anything desired on top, including logging, and even facilities supporting service-specific "local node" validators that track every change and make sure it is legal

-------------------------

Astrapolis-peasant | 2022-12-19 16:52:28 UTC | #24

Saving DEFI transactions and logs on chain is contradictory as we want save Disk space. Having logs inside the blockchain state is much more expensive than having them on SSD as logs don‚Äôt need to be indexed and hashed in state for root hash. If we could have real blocks instead of Ledger‚Äôs fake block with no signatures we could save a lot of blockchain state as well as showing people the real signatures. On one side, IC is removing blocks history for saving Disk but on the other side, IC is encouraging Developers to waste on chain state. To keep in mind, we only got 500 GB memory state per Subnet. And if we are duplicating the logs, which could be done by caching the block instead of caching  them in state,  we are actually wasting the blockchain state that could be used for more DApp memory

-------------------------

JaMarco | 2022-12-19 18:55:25 UTC | #25

[quote="dominicwilliams, post:17, topic:17504"]
This leads people to feel that the crypto math cannot be trusted, and that they themselves should be able to download every once-per-second block from every subnet and rerun all the transactions contained to replicate and check the multi-terabyte blockchain states, to ensure that no funny business goes on.
[/quote]

What do you mean when you say "crypto math"?

-------------------------

Mr_Burkes | 2022-12-20 01:00:14 UTC | #26

Dominic... you're telling me to "trust the math" but barring me from seeing what math is being executed. Huh?

It doesn't matter if you put the math in a whitepaper, the network could be running something completely different. How do I know the network is doing what you say it will if I can't verify it myself?

Re: local nodes being fed a false state by a malicious subnet, wouldn't the concern be that the subnet is compromised, rather than a read-only copy being tainted?

I still see no social reason to not add local nodes. Sure, we can argue all day that the math works out and they're not strictly necessary from a purely technical perspective; but the IC cannot execute on its vision to "replace the internet" if we don't start winning brownie points with people and sway them to the IC. I'm just trying to help.

-------------------------

singularity | 2022-12-20 05:26:40 UTC | #27

Based on this, I would rather rely on the math, and to have the math strengthened, than on an army of validators slaving away to validate ( to validate what could anyway be modified state after an isolation in the absence of math). I would rather have the math and the SEVs strengthen, in order to have a functional but nimble blockchain; and to preserve the security of voting. This is generation III Blockchain anyway, and we can chart a new way forward, and a new normal, and not be bogged down by old thinking.

-------------------------

bitbruce | 2022-12-21 04:22:32 UTC | #28

I agree with the saying that what can be done with math, don't leave it to people.
However, the mathematical community is subject to a rule that results are openly verifiable.

**Let us explore some of the most fundamental questions of blockchain.**
1. Who has the ability to judge right and wrong?
2. What is the state of trustless?
3. What power do market users need?

**From an objective point of view, my opinions are.**
1. Who has the ability to judge right and wrong?
When a system has an impartial authority, it is easy to leave the decision of right and wrong to it. But such an authority does not exist in the world. And the question of right and wrong is sometimes easy to judge and sometimes hard to judge, even if what most people judge to be right at the time may not really be right later.
The POW system remains completely neutral, leaving the question of right and wrong up to the miners to decide. But to prevent the majority of miners from making the wrong decision, a fork mechanism is retained (this is to preserve more possibilities and ensure that the few deciders can continue to struggle to survive until the market finds out they are right), a bit like "biological evolution". The POW system does this at the cost of data non-finality. 
The POS system, and IC, avoids the chaos of forking and achieves data finality, but also loses the opportunity to "preserve more possibilities" by assuming that the "smart majority" can make the right choice at the time.
I'm not saying which is better, I'm just saying that it's a trade-off that each blockchain system makes for its own goals. Judging right and wrong is inherently difficult, and blockchain maintainers need to be as neutral as possible and not choose for users when they don't have to (giving the choice back to them).
2. What is the state of trustless?
Blockchain systems require a trustless environment, which is the most essential difference from AWS.
IC has Chain-Key technology and SEV hardware, but can these technologies alone solve the trustless problem? No! I'm not doubting the cryptography and hardware, but I'm not endorsing the implicit trust assumption that "we need to trust that the nodes are running the same open source programs and have not made any hack changes to the hardware". No matter how you prove it, we need to trust an organization, either the Dfinity Foundation or a company like AMD. This is not the trustless environment we want. You know, when a contract is worth tens of billions of dollars, 13 nodes would still love to get together and conspire to do something.
One proven solution for a trustless environment is: open source code + openly verifiable blockchain data.
3. What power do market users need?
The maintainers of blockchain project have more power over the system from the beginning, which is inevitable in the early stages of ecological development. But one power that the market needs to retain is the power to a publicly verifiable power and the power to choose. (power to choose can be achieved through NNS voting). The verifiable power may not be used by the market in the early days of the ecology, but it is important to reserve this power for the market, and as the ecology matures, it is about who makes the ultimate decisions on the ecology's development.

**Based on the unique properties of IC, open nodes cause data sync difficulties and cost, I also express my views.**
1) What the market wants first is a verifiable power and the power to choose, and only then is the question of cost considered. This power cannot be given up because of the high cost.
2) Data synchronization difficulties and cost issues can be solved by technical solutions. For example (some possible ideas. I am not an expert in this area)
a) The signed block hash tree is stored permanently and is fully public. The amount of data here is small and there is no privacy issue. Then the data of the nearest N blocks of the "public subnet" will be made public and an API will be provided, and the market players will decide whether to download it or not.
b) A disguised approach is to turn the public subnet into a low-storage subnet to reduce the data volume, and a possible solution is to significantly increase the Cycles cost of storage in the public subnet.
c) Create a layer-2 network between edge nodes to do light nodes to quickly verify the data and request the corresponding packets only when the original data is really needed.
3) I'm not against it: some subnets keep the status quo without disclosing data to meet some of the demand.

**If the Dfinity team is not sure if they need an "public subnet", could they open a few "public subnets" to the market and see how the market grows?**
**Years of experience in economics have taught me that the more freedom and opportunity you give the market, the more possibilities the market will create.**

@dominicwilliams

----
Regarding the assumption of "trust majority honest" in blockchain, I would like to add a few points to my view.
(1) The POW system does not assume "trust majority honest", it does not need such an assumption. It just assumes that every human node is selfish, just like the basic assumption of economics. The security of blockchain comes from probability and benefit-cost ratio.
(2) The POW system achieves this through features such as forking, non-finality, and open nodes. Thousands of non-consensus nodes, although not involved in consensus, have the ability to verify data and the power to choose. When most of the consensus nodes are found to be dishonest, some of the non-consensus nodes will join the consensus nodes, some may not accept the longest attacked chain, but accept and support the shorter unattacked chain, and finally the market will reach a new consensus according to its "selfish" interest drive. Most of the dishonest nodes have to continuously maintain their computing power, which is a cost they cannot afford, and "sensible" cheaters will not choose to attack in this way.
(3) POS systems require the assumption of "trust majority honest", but "open and verifiable block data" can minimize the damage caused when this assumption is broken. Because users know what is happening in time, they can take action as soon as possible. The worst case scenario is that everyone can leave the failed system as soon as possible. This in turn will exponentially increase the cost of their attacks, deterring most nodes from trying to attack from the cost factor.
(4) None of the new generation blockchain systems use the POW system, which in principle reduces security but increases efficiency, scalability and creativity, which is well worth it based on the goals pursued. However, many important security measures, such as publicly verifiable block data, cannot be easily discarded along with them.

**A trustless system is one that assumes that anything can happen and is designed to make each path of attack sufficiently costly.**

-------------------------

JxBrian | 2022-12-20 13:33:01 UTC | #29

[quote="bitbruce, post:28, topic:17504"]
But to prevent the majority of miners from making the wrong decision, a fork mechanism is retained (this is to preserve more possibilities and ensure that the few deciders can continue to struggle to survive until the market finds out they are right), a bit like ‚Äúbiological evolution‚Äù.
[/quote]


Evolution is, after all, amoral ‚Äì there is no standard of ‚Äògood‚Äô or ‚Äòbad‚Äô beyond the blunt dictum: If it works, it persists; if it doesn‚Äôt, it disappears. https://geneticliteracyproject.org/2019/07/09/explaining-the-debate-over-gmos-and-what-is-or-isnt-natural-through-the-genetics-of-chickens/ 

A little bit of exaggeration here, but would you let your friends eat chicken that are grown in a lab in a day or week? Well, with your argument it seems that as far as they are living organisms (chickens) then a chicken is a chicken. But it would be nice for their health if they had a farm grown chicken that grows naturally. 

That being said, there are other blockchains that are open that you can participate in/on. Why do you double down on the IC? 

The math will reveal itself when it is the right time.    For now just let developers work at their pace.

-------------------------

bitbruce | 2022-12-20 14:12:28 UTC | #30

My native language is not English. Please focus on the main point I'm going to make and don't dispute some words, it's pointless. If you have this idea, you can find many points to dispute. Some arguments are just to make my point.

[quote="JxBrian, post:29, topic:17504"]
Evolution is, after all, amoral
[/quote]

this is not about " morality ", this is about blockchain !
"Morality" is used to discipline oneself, not to kidnap others.

I have been involved in the debate on this topic. https://forum.dfinity.org/t/discussion-public-subnets/16503

Is it possible to make a point here normally? Don't try to convince the other person if they have a different point of view. I would like to hear other people's views. I won't continue to reply in this posting.

-------------------------

Sabr | 2022-12-20 16:35:23 UTC | #31

[quote="bitbruce, post:28, topic:17504"]
What the market wants first is a verifiable power and the power to choose, and only then is the question of cost considered. This power cannot be given up because of the high cost.
[/quote]
Excellent post, @bitbruce - worthy of reading no matter which side you are on in this debate. I've just highlighted one key point, but all your points are food for more thought.

Everyone tends to get caught up on all the qualities that a blockchain should or must have. However, if you examine all those qualities closely, they are generally driven by the same fundamental criterion: decentralization of power. All infrastructure dysfunction, socioeconomic dysfunction, environmental dysfunction, etc. can be traced to centralization of power inevitably leading to abuse of power and injustice against those without power (or with significantly less). Even when power can be successfully atomized via blockchain (for infrastructure) or DAOs (for human organization), there is still a very large collective power that needs to be verifiable as an internal control measure. And I think that is one key point (among others) that @bitbruce is highlighting here.

As a result, I think there needs to be a roadmap towards some sort of blockchain verifiability process, which will preferably have no discernible impact on data privacy or network performance. I suggested an analogous "Wayback Machine" of blockchain history that could at least be verified after the fact. If this is too cumbersome, then how about statistically significant random samples of blockchain segments that can be periodically verified by the public? If public verification exposes too much data, then how about an NNS-appointed blockchain auditor (or multiple auditors) to issue periodic verification reports? Again, such an internal control may not be critical now. However, when billions of dollars are at stake on the IC - thereby creating much higher motives to exploit centralized points of failure - I agree that the market will likely not be satisfied with a moon math black box, no matter how reliable it has been in the past (or is statistically projected to be in the future).

I don't have enough expertise to assess the reasonableness of ideas to address this issue. However, more ideas would clearly be helpful to find a potential solution to an understandable market need, even if this need is just "perceived" and not statistically necessary (per Dom). So I will continue to throw out what ideas come to my mind based on my limited background and experiences, even though most of my ideas may not be viable.

-------------------------

free | 2022-12-20 17:09:05 UTC | #32

[quote="Sabr, post:31, topic:17504"]
I agree that the market will likely not be satisfied with a moon math black box, no matter how reliable it has been in the past (or is statistically projected to be in the future).
[/quote]

OK, I'll bite. You mean in the same way in which most of the market does not trust HTTPS for credit card transactions?

And if you e.g. don't trust the IC's use of "moon math black box" threshold signatures, how could you possibly trust that I (or anyone else) cannot hijack _a single boundary node_ and inject a fake response that appears to be signed by the NNS? (As in "yes, that ICP you requested just got transferred to your wallet".) If that were possible, there would be no need whatsoever to hijack two thirds of a subnet's replicas, just pretend to be the subnet.

[quote="Sabr, post:31, topic:17504"]
statistically significant random samples of blockchain segments that can be periodically verified by the public? If public verification exposes too much data, then how about an NNS-appointed blockchain auditor (or multiple auditors) to issue periodic verification reports?
[/quote]

I have asked this before and never got an answer. How is this different from the same NNS "appointing" one more replica to said subnet? A replica belonging to an auditor. Because that is essentially what every node operator is, an auditor that ensures the rules of the protocol are being followed. All the actual work done by an IC subnet can be more efficiently handled by a single replica. All the other 40+ NNS replicas are quite literally NNS-appointed blockchain auditors.

-------------------------

Sabr | 2022-12-20 17:58:32 UTC | #33

[quote="free, post:32, topic:17504"]
How is this different from the same NNS ‚Äúappointing‚Äù one more replica to said subnet? A replica belonging to an auditor.
[/quote]

It may not be that different. But how many independent auditors are auditing the IC blockchain today and issuing audit reports on the integrity of the calculations, which they will need to replicate to validate? If no one in the general public can verify the calculations are correct and consistent over time, then how can I get some independent assurance that my hypothetical billion dollars of ICP or intellectual property on the IC is safe? In other words, if funding a replica for an NNS-approved auditor is an easy solution to the verifiability problem, then just say so. We can then put the verifiability problem on the backburner until the stakes become sufficiently high to warrant a proposal to fund this continuing cost.

[quote="free, post:32, topic:17504"]
And if you e.g. don‚Äôt trust the IC‚Äôs use of ‚Äúmoon math black box‚Äù threshold signatures, how could you possibly trust that I (or anyone else) cannot hijack *a single boundary node* and inject a fake response that appears to be signed by the NNS?
[/quote]
Because no one else has my private keys, which are only in my hardware wallet. Like when I transfer BTC, I should not need to trust boundary nodes or even HTTPS to transfer ICP. However, unless I'm severely mistaken somehow, what I do need to trust are the blockchain calculations protecting my hypothetical billion dollars of ICP or intellectual property on the blockchain. That means some sort of verifiability should be in place to justify or remove this trust, just like the collective trust in the BTC blockchain is justified / removable by the ability to validate the BTC blockchain calculations. I say "justified / removable", since, for the vast majority of people, that trust will merely be "justified" by the fact that it is "removable" by a sufficient number of independent experts on a continuous basis.

EDIT: I have one additional point to follow on my last two words of "continuous basis". For BTC, this continual ability to verify calculations is there, but it is not that important, since the BTC blockchain code is not changing. For the IC, however, its code is constantly changing with frequent updates. The general public has no way to validate the internal controls over developer access to change that code, in spite of the required NNS approval. That's why it is even more important to be able to verify the IC blockchain calculations on a continuous basis, at least in my view.

-------------------------

free | 2022-12-20 22:08:03 UTC | #34

[quote="Sabr, post:33, topic:17504"]
However, unless I‚Äôm severely mistaken somehow, what I do need to trust are the blockchain calculations protecting my hypothetical billion dollars of ICP or intellectual property on the blockchain.
[/quote]

Your ICP may well be safe. But if I'm supposed to transfer you some ICP and I can pretend to be the NNS (by faking a response and signing it with the NNS' private key; which should be doable according to you, if all that's protecting it is "moon math"); then I don't even need to bother taking control of the NNS. I can simply hack into the one boundary node that you connect to (or, if you talk directly to a replica, then that one replica); and whenever you query your ICP balance and list of transactions I can make it look like I've transferred the ICP I was supposed to. And no auditor is going to notice that, because I haven't even bothered touching the state of the NNS, I merely invented a response to your query only.

The Bitcoin equivalent would be that whenever you query your Bitcoin balance, I can quickly make up a few Bitcoin blocks with valid hashes that show me having transferred you any Bitcoin amount I want.

If you trust the latter to not happen, why don't you trust the former? Isn't the latter also "moon math"?

-------------------------

Sabr | 2022-12-20 22:19:42 UTC | #35

[quote="free, post:34, topic:17504"]
which should be doable according to you
[/quote]

No, that was never my assertion. If a single replica can never be sufficient for audit purposes because it can be faked via human intervention or some other means, then **your** implied suggestion (not mine) to use a replica was simply wrong and misguided. Another solution would be required to allow verifiability of the calculations. 

Also, if you don't like the term "moon math" or the lack of verifiability that it implies, then perhaps Dominic should stop using it. That term is not something I would personally use to describe how the IC protects billions of dollars in future assets without any planned ability to independently and continuously audit either the assets or how they are protected.

-------------------------

free | 2022-12-20 22:36:36 UTC | #36

It is not about the verifiability of calculations. That's what I keep saying. You can verify the correctness of the subnet all you want; and the subnet may behave perfectly correctly; but if I can fake a response from said subnet (which is trivial if I can get control of 9 out of 13 replicas on the average subnet; or 27 of the 40 NNS replicas) then I can make you believe whatever I want, without anyone else (auditor or otherwise) ever having a clue about it.

At that point, I don't even have to fool you. I can simply fool any canister on any other subnet (pretending to be the subnet I just hijacked), lying to it about the contents of the ledger, registry, or whatever else (e.g. "free just transferred Sabr 1000 ICP, please hand over the corresponding amount of Bitcoin").

All without even trying to mess with the correctness of any subnet. Looking at any one subnet's blockchain in isolation, everything would look perfectly legitimate.

And the above is just one example of how things could go horribly wrong if we assume that the only way one could trust a subnet is to validate every single block; with everything apart from that nice to have, but not trustworthy.

-------------------------

Sabr | 2022-12-20 23:41:32 UTC | #37

Why would an independent auditor validate only one subnet or node if this is not a reliable verification procedure? Similarly, if it is "trivial" for someone to compromise 9 or even 27+ nodes on a single subnet as you shockingly imply, then why would any blockchain auditor rely on only one subnet? Again, I never once suggested such a limited approach. An auditor would need to audit a majority, or at least a statistically relevant sampling, of nodes - and in sufficiently multiple subnets - to ensure that the blockchain data being audited is valid first. And only then would he, she or it (i.e., an offline program) audit the blockchain calculations to verify the blocks have not been collectively tampered with or calculated/hashed incorrectly. This could be due to a subtle error in the math not caught by NNS mass approval or due to malicious code inserted via a developer backdoor that no one else knows about. 

Again, we are talking about billions of dollars in future value here, all of which could go up in smoke with only one such error, especially if it is allowed to multiply unnoticed / unaudited over a long period of time. I have seen nothing in this thread that could give me comfort that my hypothetical billions could ever be verified as secure at any recent point in time. I can accept that real-time security requires some transitory trust in the calculations being correct and consistent with the past. However, I (and many others) would not be able to accept a permanent inability to independently verify my assets and what blockchain calculations have recently been in place to protect them. 

It's the same concept why we are willing to trust the transitory quarterly releases of financial statements for public companies as long as there is also an annual financial audit. Forget about all the technical restrictions for a second and think about the market need for this basic level of independent assurance and whether the IC has any current procedure or plan to deliver that assurance.

The other extreme of requiring the ability to "validate every single block" (i.e., across all nodes and all subnets since genesis) to get this assurance is not what I was suggesting either. This is not how independent audits are even conducted. To me, it sounds like you are basically arguing against two strawmen of your own creation. If DFINITY can't first acknowledge the very real market need for some basic independent assurance and come up with a viable solution that doesn't break something else, then at least frankly admit that no acceptable solution to address this need will ever exist under the current IC architecture.

-------------------------

quinto | 2022-12-20 23:48:31 UTC | #38

@free, if I may summarize what you are saying:

1. Publicly verifiable block chain data only goes so far to ensure safety, because things could also go horribly wrong (and undetected) even when everything is publicly verifiable.
2. "Trusting majority of nodes are honest" is an important assumption that lies behind the security promise of all blockchains, including BTC & ETH.

I think you are concluding that since we need to "trust majority honest" anyway, "verifiability" doesn't bring much additional benefit?

I tend to agree with both 1 & 2, but I'm not sure if I want to agree with this conclusion.

-------------------------

Sabr | 2022-12-21 00:06:32 UTC | #39

[quote="quinto, post:38, topic:17504"]
I tend to agree with both 1 & 2, but I‚Äôm not sure if I want to agree with this conclusion.
[/quote]
I agree that such a conclusion does not follow from the assumptions. Keep in mind that at least two things need to be verified, not just one: 

1. statistically sufficient (or majority) consistency among the nodes, and 
2. whether the blocks are being correctly calculated / added to the chain. 

For #1, this should be relatively easy, even for BTC, with statistically random sampling across a few thousand nodes for a high probability of assurance over consistency. For #2, random sampling of a few thousand blocks and independently recalculating them could likewise provide a high probability of assurance that the calculations are both correct and consistent, since even one deviation would indicate an unacceptable level of error. If it is possible to get this assurance from one blockchain but not another, then there is indeed a large difference in verifiability benefit between these blockchains.

-------------------------

Mr_Burkes | 2022-12-21 02:44:47 UTC | #40

Why have we devolved into man-in-the-middle attacks which any blockchain is susceptible to?

The original ask is to PROVE TO EVERYONE that the IC is doing what it says it is. That's it. 

We have many papers, blogs, and many tirades asserting what the IC does, but no PROOF that actual node machines are running what they say they are. I could go on all day about this. How is the data from dashboard.internetcomputer.org populated? Why can I not query for this data? Unless, of course, I can- in which case please correct me. But it seems to me that certain data about the IC network is privileged. How is that fostering an open alternative to the internet?

Perhaps querying one boundary node is not sufficient. Ok. So let's query a bunch, and achieve a consensus to guard against Byzantine faults and compromised nodes.

The entire reason anybody wants this is because we, as a community, have NO way to peer into the inner-workings of the IC, other than, as Dom says, to interact with canisters. However, by your argument, we cannot ask for read-only canister state and state transition data because a boundary node could be corrupted. If that's the case.... **then how can we trust the output of what the IC is doing at all???** By your logic, we should trust ABSOLUTELY NO OUTPUT from the IC, because a boundary node could return bad output.

Edit: I do not mean to come across as harsh, but I'm rather using forceful language to get questions answered.

-------------------------

JaMarco | 2022-12-21 02:40:12 UTC | #41

@dominicwilliams @free I think we are all just trying understand how the IC replaces the security of users being able to locally run a node themselves. You're saying "the math" is able to replace the security, but how exactly does "the math" replace it?

-------------------------

JxBrian | 2022-12-21 02:46:25 UTC | #42

[quote="Mr_Burkes, post:40, topic:17504"]
We have many papers, blogs, and many tirades asserting what the IC does, but no PROOF that actual node machines are running what they say they are. I could go on all day about this. How is the data from [dashboard.internetcomputer.org](http://dashboard.internetcomputer.org) populated? Why can I not query for this data? Unless, of course, I can- in which case please correct me. But it seems to me that certain data about the IC network is privileged. How is that fostering an open alternative to the internet?
[/quote]

How can you confirm the formula of say Pfizer or Moderna does what it is intended; they don‚Äôt share the formula. But everyone has come to understand the effectiveness of vaccination to boost hard immunity. Now poor countries would want to have the formula to design vaccine that cater for their distinct populations. But the same companies that advocate for eradication of covid19 won‚Äôt share that formula based on their reasons. It is the same with IC, you just have to use the product and join everyone in line in waiting for a stable release of what you are asking for.
You really have to take a step back and observe how the world works/ runs for you to understand the stability of IC.

-------------------------

Mr_Burkes | 2022-12-21 02:48:58 UTC | #43

@JxBrian your analogy does not make sense here. Using your analogy, this is like asking Pfizer or Moderna "does the vaccine work?" And them saying "you can't ask that trust me bro".

I don't want the formulation. I want the PROOF that it's doing what it says it does.

-------------------------

Denis | 2022-12-21 03:59:15 UTC | #44

[quote="Sabr, post:37, topic:17504"]
if it is ‚Äútrivial‚Äù for someone to compromise 9 or even 27+ nodes on a single subnet as you shockingly imply, then why would any blockchain auditor rely on only one subnet?
[/quote]

To be fair, @free did not state it is trivial to compromise nodes, he stated that *should* someone compromise enough nodes on a given subnet, it would be trivial for the attacker to then fool any auditor into believing the subnet was behaving fine.
I think we are once more facing a clash between Dfinity's engineering and math mindset vs the community's crypto mindset. It's quite sad, because DW's notes, and Free's inputs are rather beautiful, they fill me with wonder at what Dfinity has accomplished. 
But the organisation can't seem to understand the importance of appearances. If people committed enough to a network to spend hours on dev forums say they feel independent, public verification is important, the response ought not to be the equivalent of, "You are idiots who don't understand the math". Because the network's success will depend on convincing millions of idiots like us, which can only be done by *appearing* to have a trustless system in place. (And to clarify, nobody at Dfinity has called anyone an idiot or implied anyone is an idiot, it is just my construction for effect). Right now, from the idiot's perspective, Dfinity seems to be saying, "It is as trustless as it can get, trust us."

-------------------------

lastmjs | 2022-12-21 04:29:32 UTC | #45

[quote="JaMarco, post:7, topic:17504"]
But doesn‚Äôt a subnet signing with chain-key just prove that the subnet nodes came to consensus on the blocks/state, not necessarily that the computation was performed correctly. How does a chain-key signature prove that the subnet wasn‚Äôt overrun by malicious nodes and signed invalid blocks/state transitions? Only way to prove correct computation is to run a node yourself and download/run all the blocks and computation yourself (or generate zk proofs of the computation).
[/quote]

I haven't read the whole thread, but yes I believe you are correct. I've been saying this for a long time, and it's not just me. It's not like we magically can throw away the need to verify the computations, we've instead taken a trade-off.

I feel to say otherwise is misleading.

On the ZK side, that's part of why I'm so interested in eventually replacing the IC's VM with a zkWasm VM. Discussion here if you're interested (and there is a team already working on zkWasm): https://forum.dfinity.org/t/zero-knowledge-internet-computer-virtual-machine/9129

Edit: but then again, if a 2/3 majority is malicious, they could produce erroneous proofs. The proofs would have to be correct, but I believe there are certain attacks that would still be possible, probably double-spending

-------------------------

JxBrian | 2022-12-21 05:49:44 UTC | #46

[quote="Mr_Burkes, post:43, topic:17504, full:true"]
@JxBrian your analogy does not make sense here. Using your analogy, this is like asking Pfizer or Moderna ‚Äúdoes the vaccine work?‚Äù And them saying ‚Äúyou can‚Äôt ask that trust me bro‚Äù.

I don‚Äôt want the formulation. I want the PROOF that it‚Äôs doing what it says it does.
[/quote]

Maybe you did not understand the point I was bringing across is .. it is up to developers Pfizer, Moderna or IC in this case to release their project design at the right time.  And you can not tell them what the right time is and here is why‚Ä¶ 

Say you design a futuristic ship. You are in the ocean and moving at godly speeds through a storm. Your dashboard shows you are moving at 1000 miles an hour. This ship has communication capabilities and you are able to interact with the outside world broadcasting on real time on your achievements. 
An outsider comes along and says how can we trust the ship is moving at 1000 miles an hour‚Ä¶ ‚Äúlet me in, let me peek at the dashboard, I want to confirm xyz.‚Äù
So you tell them that is not possible at the current time, they have to wait till the storm clears up. Maybe when the waves clear up and the water settles you might let them in the ship to look at the dashboard. Mind you, you are still working on the ship..probably taking their consideration to have a feature that would let them have what they are requesting. 
But they insist on being let in to verify that the ship is moving at a certain speed. To them, maybe the reason they want to confirm the reading on the dashboard is because Einstein theorem of relativity states that based on a point of observation the speed might be different. But while designing your ship you knew that. https://futurism.com/newtonian-physics-vs-special-realtivity 
How can you let them in while there is a storm and you are moving at godly speeds? Though you would want them to see that your dashboard is working as intended, they have to wait till the storm fades away or the water clears up. That is the only way you can protect the ship, yourself and them.

-------------------------

Sabr | 2022-12-21 05:58:02 UTC | #47

The vaccine analogy also fails because a vaccine is scientifically and independently tested first on animals and much smaller populations. Also, a vaccine formula is not subject to frequent updates like the IC is, thereby mostly invalidating the efficacy of prior independent testing. Finally, there is no survival imperative to use the IC like there is for vaccines. In other words, there is no survival risk to individuals if they simply walk away from the IC because they have no independent assurance to rely upon it. That will make market traction for the IC unnecessarily difficult.

As for protection of proprietary formulas, that could actually be another purpose of an independent auditor, in addition to obviating the need for private subnet data to become public for the purpose of public blockchain verification. Independent auditors could also protect proprietary blockchain algorithms while auditing them if DFINITY's patent protections are not considered sufficient to do so. However, my understanding was that all the IC algorithms are open source anyway, with only legal usage restrictions on forking the IC. So that means everyone should be able see exactly how the algorithms work without relying on any NNS-approved auditor for privileged algorithm access.

-------------------------

Sabr | 2022-12-21 06:18:55 UTC | #48

[quote="Denis, post:44, topic:17504"]
he stated that *should* someone compromise enough nodes on a given subnet, it would be trivial for the attacker to then fool any auditor into believing the subnet was behaving fine.
[/quote]

But at that point, it is arguably no longer just an "attack", is it? It's basically a complete hijacking or conquest. That's analogous to saying, "should someone compromise enough nodes on the bitcoin or ethereum blockchains, it would be trivial for the attacker to then fool any auditor into believing the blockchain is behaving fine." Well, yeah, the attacker just conquered the entire blockchain consensus, so no more "fooling" is technically even required. The blockchain consensus has been hijacked and will now actively protect only the malicious blockchain version. Everyone understands this theoretically remote risk, but that is not the risk we are focused upon here. In other words, I don't think it is fair to present this extreme risk (which all blockchains face to some degree) as a reasonable argument against the value of an independent audit.

-------------------------

jzxchiang | 2022-12-21 07:23:02 UTC | #49

> Edit: but then again, if a 2/3 majority is malicious, they could produce erroneous proofs. The proofs would have to be correct, but I believe there are certain attacks that would still be possible, probably double-spending

Exactly! If you need a 2/3 majority to achieve consensus on the ordering of inputs, then why bother with generating proofs, especially if it is at least 5 orders of magnitude more expensive than just replicating the computation without any proof (which is simply "subnet size" more expensive)? In any case, you'll need to trust the 2/3 majority for the inputs.

Ignoring the zk proof part of this, I agree that running a node yourself, verifying all the blocks, and rerunning all computation would solve this.

One thought experiment: what if tomorrow suddenly 2/3 of the NNS subnet's nodes were hijacked and they managed to illegally fudge some transactions, e.g. double spend, mint new ICP, etc? Let's say after 10 minutes the network got restored. How would anyone be able to detect this? The consensus protocol would be chugging along just fine. Because blocks are not public (and in fact blocks are thrown away after a short period of time), the erroneous state (for which consensus was achieved) would be taken as given, and it would go unnoticed until the end of time...

Or am I missing something?

EDIT: Would the ledger's "blockchain in a blockchain" fix this? I can't think of how it would.

-------------------------

Mr_Burkes | 2022-12-21 06:29:12 UTC | #50

ZK and it's applications within the IC are finally starting to make sense to me, having argued the importance of computational proof in distributed systems.

@lastmjs I would be interested in helping forward a zkWasm VM, although I can probably only contribute distributed systems knowledge and a bit of Rust. My Wasm experience is near nil.

-------------------------

JaMarco | 2022-12-21 07:19:05 UTC | #51

[quote="jzxchiang, post:49, topic:17504"]
One thought experiment: what if tomorrow suddenly 2/3 of the NNS subnet‚Äôs nodes were hijacked and they managed to illegally fudge some transactions, e.g. double spend, mint new ICP, etc? Let‚Äôs say after 10 minutes the network got restored. How would anyone be able to detect this? The consensus protocol would be chugging along just fine. Because blocks are not public (and in fact blocks are thrown away after a short period of time), the erroneous state (for which consensus was achieved) would be taken as given, and it would go unnoticed until the end of time‚Ä¶

Or am I missing something?
[/quote]

I think their argument is "the math" says 2/3 of the NNS subnet‚Äôs nodes getting hijacked and illegally fudging transactions without being detected is never going to happen in the first place. Kind of like how we assume SHA256 collisions will never happen even though it's possible but the math says it basically won't.

-------------------------

jzxchiang | 2022-12-21 07:28:19 UTC | #52

The math only says that ***if at least 2/3 of the nodes aren't hijacked***, then you can trust the computation without downloading the blocks yourself.

> Thanks to the way the protocol math works, if the chain key signature validates, this not only tells you that your interaction has not been tampered with, but also that the subnet blockchain returning the result is running correctly and that neither its state or computations have been tampered with (which could only be achieved by corrupting the subnet blockchain involved, for example by overwhelming the fault bounds of its Byzantine Fault Tolerant protocols).

> That is, they use protocol math that guarantees that so long as the proportion of ‚Äúfaulty‚Äù nodes (i.e. nodes that can behave arbitrarily, including going offline, corrupting data, subverting the protocol, and colluding with other faulty nodes in any way they choose) stays beneath a defined ‚Äúfault bound,‚Äù then the network will continue running correctly.

^ These are quotes from Dom earlier in the post.

His argument that 2/3 of the nodes won't be hijacked is based on deterministic decentralization, i.e. it's difficult to hijack 20+ nodes in different legal jurisdictions and owned by separate node providers in a short period of time.

Quote:
> The nodes involved are selected by the NNS using a system of deterministic decentralization. This means that they are owned and operated by independent node providers, and installed in different data centers in different geographies and jurisdictions. This is very different to randomly selecting anonymous nodes in a typical Proof-of-Stake network, within which an adversary might be running a large number of nodes ‚Äì such that if the random selection of a group is unlucky, nearly all the nodes might belong to the adversary.

-----

My concern from above is... OK let's say that we unfortunately do have a situation where 2/3 of the nodes get hijacked. How can we recover from that? How can we detect that?

-------------------------

Denis | 2022-12-21 07:37:41 UTC | #53

[quote="jzxchiang, post:49, topic:17504"]
One thought experiment: what if tomorrow suddenly 2/3 of the NNS subnet‚Äôs nodes were hijacked and they managed to illegally fudge some transactions, e.g. double spend, mint new ICP, etc? Let‚Äôs say after 10 minutes the network got restored. How would anyone be able to detect this?
[/quote]

I think the point made was that as long as there was one honest node, there would be a divergence between nodes, which would be detected.

-------------------------

lastmjs | 2022-12-21 08:03:11 UTC | #54

[quote="jzxchiang, post:49, topic:17504"]
Ignoring the zk proof part of this, I agree that running a node yourself, verifying all the blocks, and rerunning all computation would solve this.
[/quote]

I'm not so sure downloading and verifying all blocks yourself is much better than the ZK proof, I would guess it's worse. In that case, you still have to download the blocks from the potentially malicious nodes. If 2/3 or malicious, wouldn't you be in the same boat as with the ZK proofs?

This all breaks down if the bounds on the number of Byzantines are broken.

But, I would guess ZK proofs would be more favorable than downloading and manually verifying all state, given that some protection is given you. If at least one node is honest you could at least detect issues (I believe this is the case both with downloading and verifying and with ZK).

-------------------------

JaMarco | 2022-12-21 08:20:45 UTC | #55

[quote="dominicwilliams, post:12, topic:17504"]
Internet Computer subnets are 3f+1 Byzantine fault tolerant ([which, according to simple math, is the best you can do in an asynchronous network](https://lianghan.org/2017/02/01/2017-02-01-ByzantineFaultTolerance/)), which means that less than one third of the nodes can be faulty to ensure that *no problems can occur*. However that does not mean that *any problem can occur* if one third or more of the subnet‚Äôs nodes are faulty. A supermajority of nodes is required to make the network run, which is 2f+1, Since f is 11, a supermajority is 23 nodes. If an adversary wanted to modify the network in secret, they would need to have installed at least 23 faulty nodes, which would a) stop talking to the remaining 11 correct nodes, b) perform the illegal state transitions desired, and c) start talking to the correct nodes again after a sufficiently long period of time that they are forced to resync using the illegally created state, rather than by re-running cached recent blocks to catchup. The question is, given the aforementioned nature of deterministic decentralization, what is the likelihood of an adaptive adversary corrupting 23 node providers, and getting them to modify the software their node machines run, without getting detected. We claim that this is extremely unlikely.
[/quote]

[quote="dominicwilliams, post:17, topic:17504"]
My point is: the only reason that this feature appears to be necessary and/or beneficial, is that ‚Äúblockchain community consensus‚Äù provides misleading social proof that this is somehow technically necessary and good. This leads people to feel that the crypto math cannot be trusted, and that they themselves should be able to download every once-per-second block from every subnet and rerun all the transactions contained to replicate and check the multi-terabyte blockchain states, to ensure that no funny business goes on. After all, how could this not be necessary since this is how every other blockchain works, and they advertise the thousands of cloud validators that keep them safe‚Ä¶?

But the math does says otherwise (as it does about the safety of many other leading blockchains who use this model). In end, we have to trust the math and stay the course with the science, and eschew misleading blockchain rubrics. By adding a feature that enables people to slurp blocks from subnets and replicate their state, we are effectively saying that this is technically necessary, feeding the falsehood, and soon people will believe that any subnet not being replicated by an army of local nodes is unsafe.
[/quote]
But he's saying here that the math of deterministic decentralization and network security/monitoring says a situation where 2/3 of the nodes get hijacked without being detected won't ever happen so we don't have to worry about it.

-------------------------

Zane | 2022-12-21 11:04:28 UTC | #56

Imho Dfinity relies to much on Deterministic Decentralization, a.k.a the assumption that:
1) It's possible to identify individuals in a decentralized manner with a high degree of certainty.
2) Being doxed is enough to prevent attacks against the network.

I have serious doubt about both, it's just a matter of time until the network starts securing enough value and they'll be put to the test.

-------------------------

free | 2022-12-21 13:08:31 UTC | #57

Let me take a step back and try to summarize my understanding of how things fit together. Even though it may not look that way, this is not intended as a categorical argument against public subnets. All I'm saying is that public subnets are nowhere near a silver bullet. That being said, please let me know if I've missed or misrepresented anything. Which is entirely possible.

AFAICT there are 2 failure modes that public subnet blockchains and independent third party validators are intended to protect against:
1. A malicious supermajority (two thirds plus one) of a subnet's replicas fully taking over a subnet.
2. Bugs in the protocol or its implementation allowing a malicious party to take over a subnet without having control of a supermajority.

The underlying assumptions (again, as I understand them) are that:
* an independent validator would be able to detect if a subnet is not behaving correctly (e.g. minting ICP out of thin air; or issuing transactions not authorized by users);
* (partially) that there is no other way of gaining an equivalent level of trust in a subnet; and 
* (partially) that tampering with the state of a subnet would be the main vector of attack.

## 1. Malicious supermajority
Parenthesis: I never claimed that taking control of a two thirds plus one supermajority of subnet replicas would be trivial. I was merely starting from it as an assumption that others had made [in this](https://forum.dfinity.org/t/addendum-to-the-public-subnets-thread-helpful-info-on-subnet-workings/17504/28?u=free) and [the earlier thread](https://forum.dfinity.org/t/discussion-public-subnets/16503/67), as a clear cut example of what independent validation might protect against.

Second parenthesis: I did (incorrectly) claim that "it would be virtually impossible to hijack a subnet without being detected", even in the absence of independent validators. But Dom then described a smarter attack: if one can just stall the handful of honest replicas for long enough; and then have them sync up to the malicious supermajority after the state has already been tampered with; the honest replicas would be none the wiser about it. A public subnet blockchain and independent validators would indeed be able to detect this specific malicious behavior In this specific situation.

The issue with starting from the assumption of a malicious, colluding supermajority is that it causes pretty much everything else to break down, because no consensus algorithm can possibly deal with it. Yes, an independent validator may be able to detect a subnet going rogue if it does so in the most egregious way possible (e.g. tampering with the state by creating transactions out of thin air). But if that has happened, then the harm is already done (e.g. up to and including draining all Bitcoin held by canisters; on the Bitcoin network, not merely on the IC).

And (my other earlier point) if a majority of a subnet's replicas are maliciously colluding, there are myriad other ways to cause chaos without even tampering with the subnet's blockchain/state: the attacker now has the subnet's secret key, so they can simply sign fake responses to ingress messages; or send fake canister requests to other subnets. There is literally no way to prevent the former, since it's just an interaction between a user and a boundary node. It is in no way reflected in the subnet's blockchain or state. As for the latter (malicious subnet firing off arbitrary, legitimate-looking requests to other subnets), unless you have independent validators for every single subnet; and they cross-reference every single canister message received by any subnet as having been sent by the other subnet; again, there is no way to detect it. The fake request from subnet A to subnet B would look like a totally legitimate request to subnet B's validators; and, according to its blockchain and state, subnet A never produced the fake request, so a subnet A validator would also not detect anything fishy.

It is only if you have a full clone of the IC running on the side that you would be able to detect a malicious supermajority on one subnet. But, since not everyone (or anyone) will be able to afford their own private IC clone, how is that materially different from simply adding one more replica to each subnet?

And even with a fully independent IC clone in place, the attacker may (very likely, I'm not 100% sure on this point) cause mischief simply by taking control of the subnet's consensus algorithm. That way, without directly messing with the state of the subnet (so again, impossible to detect for an independent third party validator), you can simply pick and choose which transactions make it into blocks and which don't. And again, with knowledge of the subnet's secret key, the attacker may prevent your ingress messages from getting included into blocks while still making it look to you as if they did get included. And were successfully processed.

My conclusion from the above is that while independent validators may be able to detect egregiously misbehaving malicious subnets, detection is not prevention; and an intelligent attacker can reach their goals without tampering with the state of the subnet at all. In other words, once someone gains control of a subnet, all bets are off, regardless.

## 2. Bugs in the protocol

I don't have much here, except that if a handful of malicious replicas can fool the majority, why wouldn't they be able to use the exact same mechanism to fool independent validators? They are running the exact same code as the fooled replicas, after all.

In other words, if a handful of malicious replicas can make the subnet behave as if there was a supermajority of malicious replicas, then see above.

## Wrap-up
It may appear from the above that I've thoroughly discredited the IC's security model. And that it is hopeless to even try and fix it.

On the contrary, all it shows is that (as already pointed out by others) security is not a binary property; not even a gradient; but a complex interaction between many moving parts. And thus, there is no one size fits all solution. Including public subnets and independent validators. They would increase security by some amount, against some brute force and not particularly subtle attacks. But that increase in security must be weighed against the cost (in cycles; development time; privacy; chilling effect on NNS/SNS voting; etc.). And against the opportunity cost of not implementing other security measures instead.

Including simple stuff, such as bumping the number of NNS replicas. Or designating specific high-replication fiduciary subnets and limiting access to the ICP ledger and Bitcoin canister to canisters running on such subnets. And so on, and so forth.

Of course, there is the intangible aspect of

[quote="Mr_Burkes, post:26, topic:17504"]
winning brownie points with people and [swaying] them to the IC
[/quote]

by making (some) subnets public, pointing to them and saying: "Here, we're just as secure as every other blockchain because we take the exact same approach to security as them". And other ways of engendering user trust by taking measures that appear to increase security while in fact providing very little measurable benefit (what may unkindly be labeled as "security theater"). I don't have anything on that. As an engineer, I would very much not be swayed by it, but then I guess I'm by no means representative of the general population.

-------------------------

Denis | 2022-12-21 12:13:34 UTC | #58

[quote="Sabr, post:48, topic:17504"]
But at that point, it is arguably no longer just an ‚Äúattack‚Äù, is it? It‚Äôs basically a complete hijacking or conquest.
[/quote]

This is correct, but having an independent audit will do nothing to prevent such attacks, will it? By definition an audit is a mechanism to detect mishandling of accounts not a defence against mishandling. It does have a deterrent effect with its assurance of detecting mishandling, but cost-benefit considerations always apply. 
The question is: what is the value of such an after-the-fact measure on the IC given it would be very expensive and in some cases, described in the two public subnets threads, unable to detect fraud (any fraud would of course become visible through incidents such as the draining of funds)?
I am struggling to find a good answer for why what @free called "security theater" is required, but am also convinced it is. I'm racking my brain for real world examples of similar measures which might convince engineers of their necessity. Will post if I think of one.

-------------------------

JaMarco | 2022-12-21 16:46:41 UTC | #59

[quote="free, post:57, topic:17504"]
My conclusion from the above is that while independent validators may be able to detect egregiously misbehaving malicious subnets, detection is not prevention; and an intelligent attacker can reach their goals without tampering with the state of the subnet at all. In other words, once someone gains control of a subnet, all bets are off, regardless.
[/quote]

Do you see this as a conclusion unique to the IC or do you think it applies to Bitcoin/Ethereum as well?

-------------------------

Sabr | 2022-12-21 16:48:21 UTC | #60

[quote="JaMarco, post:55, topic:17504"]
But he‚Äôs saying here that the math of deterministic decentralization and network security/monitoring says a situation where 2/3 of the nodes get hijacked without being detected won‚Äôt ever happen so we don‚Äôt have to worry about it.
[/quote]

It's not just about node divergence from 100% consistency. It's also about verifying the blockchain calculations, i.e., that they are working exactly as intended, especially after the frequent upgrades across all nodes. 

[quote="free, post:57, topic:17504"]
AFAICT there are 2 failure modes that public subnet blockchains and independent third party validators are intended to protect against:

1. A malicious supermajority (two thirds plus one) of a subnet‚Äôs replicas fully taking over a subnet.
2. Bugs in the protocol or its implementation allowing a malicious party to take over a subnet without having control of a supermajority.
[/quote]
But what about option #3 of bugs in the protocol causing incorrect blockchain calculations in 100% of all nodes? These bugs could have been missed in NNS review or maliciously inserted via a developer backdoor or some other means. Without any independent audit over the internal controls to prevent such a scenario, how would anyone get assurance this cannot realistically happen -- or worse, has already happened long ago? Given how frequently updates occur to node machines and the protocol, I don't think it is unrealistic to think this is a scenario to be concerned about. 

[quote="free, post:57, topic:17504"]
by making (some) subnets public, pointing to them and saying: ‚ÄúHere, we‚Äôre just as secure as every other blockchain because we take the exact same approach to security as them‚Äù. And other ways of engendering user trust by taking measures that appear to increase security while in fact providing very little measurable benefit (what may unkindly be labeled as ‚Äúsecurity theater‚Äù).
[/quote]
Again, you are entirely missing the point here by conflating "security" with "assurance". Security is a real-time control against attacks, and I actually have a lot of confidence in the IC there. By contrast, assurance is a longer-term control to engender confidence in the protocol and to prevent unnecessary multiplication of losses. If I have no way of independently verifying my assets on the IC blockchain, or even to verify the blockchain calculations that are protecting them at a recent point in time, then I basically know that I can never get the assurance I need. So why would I even begin to invest in the IC (as a new investor or developer) or to invest even more (as an existing investor or developer)? 

To explain it another way, "security" mainly relates to how existing assets on the IC blockchain are protected, whereas "assurance" relates more to how future assets on the IC would be protected and current assets are verified. Note that "verified" also comes with a multitude of personal benefits like using ICP for financial leverage, increased emotional comfort, decreased anxiety over risk, etc., which you are obviously ignoring. However, even this distinction is not entirely accurate, since "assurance" can also indirectly protect existing assets too, which is a more substantive argument. To explain, let's take the BTC blockchain as a more extreme example so that we temporarily get off the hot button of focusing on the IC. 

Right now, there are a huge number of large-scale BTC miners on the brink of bankruptcy. So how difficult would it really be for a billionaire to quietly buy up a majority of the nodes at a deep discount to conquer the entire blockchain consensus? He could then edit the state to give himself enough BTC to make this evil venture profitable, *assuming no one could validate the blockchain calculations to show that malicious edits have been made to the blockchain state*. He could then simply sell those stolen BTC for a tidy profit and still own the entire BTC blockchain. Eventually, of course, trust in the blockchain would slowly collapse after anecdotal stories of stolen BTC become too loud, but he would have made even more profit by then. 

However, this horrible scenario obviously assumed (per my italics) that no one could verify the incorrect BTC blockchain calculations after the fact, when in fact anyone can verify those calculations almost instantly. This is why no billionaire would even bother to try this ruse, since it would either be shut down immediately somehow or the entire value of all BTC would drop to zero before he could have any chance of profiting from his conquest of the blockchain. So, in this case, it would not be any blockchain "security" that stops the would-be attacker from a successful exploit, but rather the post-facto audit assurance that would make his potential exploit essentially worthless.

This extra level of independent assurance **beyond blockchain security** is actually what makes the BTC blockchain both unstoppable and unhackable. It is this one critical off-chain attribute that is missing from the IC blockchain. I can't see independent verification assurance as ever being optional to the mainstream success of the IC. It will eventually become a requirement, if it is not already for most individuals.

-------------------------

free | 2022-12-21 18:15:04 UTC | #61

[quote="JaMarco, post:59, topic:17504"]
Do you see this as a conclusion unique to the IC or do you think it applies to Bitcoin/Ethereum as well?
[/quote]

I guess it's more obvious in the case of the IC, since you have blockchains with on the order of 10 to 50 validators. Which makes it more obvious that all you need is control of two thirds of replicas. And hacking into 10 servers; or paying off 10 node operators; or some combination thereof; appears within the realm of the possible.

In reality, Bitcoin and Ethereum suffer from the exact same problem. It may be more difficult to tamper with them without being detected, but it's not really more difficult (or easier) to tamper with them

. And they (and their users) pay a huge price for that privilege, And the mechanics of PoW and PoS networks strongly encourage economies of scale, so it may well be that you don't even need 9 parties to collude in order to take control of a PoW or PoS network (whereas in the case of the NNS you would definitely need more than 9).

[quote="Sabr, post:60, topic:17504"]
But what about option #3 of bugs in the protocol causing incorrect blockchain calculations in 100% of all nodes?
[/quote]

Incorrect behavior on 100% of nodes would mean an (accidental or intentional) bug in the code. But you would be using the exact same code to do validation. Meaning that your local validator would also have the same bug and thus see the behavior as correct.

You could, of course, have multiple independent replica implementations. But even just the deterministic state machine that is part of the replica is (likely, because I don't have any direct experience with Ethereum of Bitcoin code) orders of magnitude more complex than other blockchains. It includes a scheduler, messaging queues, prioritization, instruction and memory limits and so on and so forth. Plus, as pointed out by JaMarco above, it changes weekly. So while not exactly impossible to maintain multiple independent implementations, it would definitely require mind-boggling effort.

[quote="Sabr, post:60, topic:17504"]
This extra level of independent assurance **beyond blockchain security** is actually what makes the BTC blockchain both unstoppable and unhackable.
[/quote]

I'm not qualified to comment on the security of Bitcoin (because, as said, I am only familiar with the protocol at a very high level) but IMHO what makes BTC unstoppable and unhackable (and I believe the same applies to Ethereum) is the fact that it is one single blockchain, with very, very limited throughput and thousands upon thousands of validators. If the IC becomes amazingly successful one day, it is conceivable that one may expect the same for the NNS and maybe a couple of other very important subnets (assuming their blockchains are published).

But it is entirely unrealistic to expect this across every single subnet. Or even across all "important" subnets. A subnet is virtually equivalent to one (slightly underpowered, when compared to an AWS instance) server. Imagine for a moment how many subnets you would need to put together to build something like a decentralized banking app for millions of people (to stay within the realm of reality). Now imagine how likely it will be for each such subnet to be validated by a healthy number of independent auditors. With each machine required to validate such a subnet costing tens of thousands of dollars (my desktop machine is literally more than an order of magnitude away from the 512 GB of RAM required). All this for one single large-ish app.

Finally, it may be entirely possible to raise hell about someone taking over the Bitcoin network; walk back a few blocks (at one block every 10 minutes); fork the network; and give everyone back what they lost (modulo any valid transactions that happened within those rolled back blocks). It is definitely not possible to notice an IC divergence; raise the issue; analyze it; and still hope to roll back or fork anything meaningful at that point. Even if you do have a full block history. A malicious subnet will have sent messages to many other subnets (or outside the IC) and those subnets and outside systems will have long since acted on those messages.

Which is why I'm inclined to believe that detecting IC subnet divergence is not particularly useful beyond proving that it didn't happen. And independent validators is exactly what node providers are supposed to be.

-------------------------

JaMarco | 2022-12-21 18:28:37 UTC | #62

[quote="free, post:61, topic:17504"]
I guess it‚Äôs more obvious in the case of the IC, since you have blockchains with on the order of 10 to 50 validators. Which makes it more obvious that all you need is control of two thirds of replicas. And hacking into 10 servers; or paying off 10 node operators; or some combination thereof; appears within the realm of the possible.

In reality, Bitcoin and Ethereum suffer from the exact same problem. It may be more difficult to tamper with them without being detected, but it‚Äôs not really more difficult (or easier) to tamper with them

. And they (and their users) pay a huge price for that privilege, And the mechanics of PoW and PoS networks strongly encourage economies of scale, so it may well be that you don‚Äôt even need 9 parties to collude in order to take control of a PoW or PoS network (whereas in the case of the NNS you would definitely need more than 9).
[/quote]

So to be clear, you disagree with Vitalik Buterin's assertion about 51/67% attacks?

> Many people have the mentality that "if a blockchain gets 51% attacked, everything breaks, and so we need to put all our force on preventing a 51% attack from ever happening even once". I really disagree with this style of thinking; in fact, blockchains maintain many of their guarantees even after a 51% attack, and it's really important to preserve these guarantees.

https://old.reddit.com/r/ethereum/comments/rwojtk/ama_we_are_the_efs_research_team_pt_7_07_january/hrngyk8/

-------------------------

Sabr | 2022-12-21 19:32:57 UTC | #63

[quote="free, post:61, topic:17504"]
Incorrect behavior on 100% of nodes would mean an (accidental or intentional) bug in the code. But you would be using the exact same code to do validation. Meaning that your local validator would also have the same bug and thus see the behavior as correct.
[/quote]

No. An independent auditor or random cryptographic expert would likely be running the blockchain calculations as publicly designed and documented, along with blockchain code prior to the upgrade, to compare against the block results of the code recently implemented in order to verify that no unexpected discrepancies occurred (i.e., that no erroneous code was implemented). Why would an auditor trust the new code first in order to verify that same code? That makes absolutely no sense. 

You then digress again onto the need for "multiple independent replica implementations" when I'm not talking about auditing for node consistency, but rather block (or state) calculation accuracy. Even replicating the entire IC won't help you verify that the blocks are being calculating correctly if a single upgrade affecting the entire IC causes incorrect calculations (vs. the public open source design). Therefore, the calculation verification I'm referring to would not have to be done on all (or even most) nodes or subnets, since existing node consistency controls would detect if even one node discrepancy occurred and highlight it for immediate investigation.

[quote="free, post:61, topic:17504"]
I‚Äôm not qualified to comment on the security of Bitcoin (because, as said, I am only familiar with the protocol at a very high level) but IMHO what makes BTC unstoppable and unhackable (and I believe the same applies to Ethereum) is the fact that it is one single blockchain, with very, very limited throughput and thousands upon thousands of validators.
[/quote]

No again. As I stated, both BTC and Ethereum are still very fault tolerant, so their entire blockchains could still be hijacked by taking over a sufficient number of nodes. In other words, blockchain security is not theoretically enough to protect them. What makes them unhackable and unstoppable is the additional control of post-facto verification assurance, which destroys (almost) any rational incentive to hijack their blockchains. I say "almost" in parentheses because the U.S. government could still have a rational incentive to hijack any blockchain that could realistically threaten the U.S. dollar (or a CBDC equivalent) as the world's reserve currency, even if this hijacking destroys the competing blockchain's market value instantly. BTC and ETH are clearly not a threat in this scenario for various reasons, but another cryptocurrency might be.

[quote="free, post:61, topic:17504"]
Finally, it may be entirely possible to raise hell about someone taking over the Bitcoin network; walk back a few blocks (at one block every 10 minutes); fork the network; and give everyone back what they lost (modulo any valid transactions that happened within those rolled back blocks).
[/quote]
Yes, which is one reason why post-facto verification assurance on top of blockchain security can add significant value to blockchains like BTC. In addition, you are missing the far more important value of this internal control: it is a massive deterrent against even *trying* to hijack a blockchain with strong post-facto verification assurance, since it would never be profitable to do so given the public's quick reaction time.

I realize that an obsessive focus on security (i.e., on-chain controls) is your job, and I'm actually quite glad and comforted that you are very good at that job. However, I think this focus is not allowing you to see some of the very real benefits of off-chain controls, like even a minimum level of independent verification assurance over blockchain calculations, independent audits over DFINITY's internal controls on node and protocol upgrades (in spite of NNS approval being required), etc.

[quote="free, post:61, topic:17504"]
It is definitely not possible to notice an IC divergence; raise the issue; analyze it; and still hope to roll back or fork anything meaningful at that point. Even if you do have a full block history. A malicious subnet will have sent messages to many other subnets (or outside the IC) and those subnets and outside systems will have long since acted on those messages.
[/quote]

Thank you for conceding to one of my most important points on the value of post-facto verification assurance. Indeed, asset losses can accumulate and multiply far higher and far longer on a hijacked IC than on a hijacked blockchain like BTC or ETH. More important, the fact that this is "definitely" so, as you confirm, means that the motivation to hijack the IC might actually become a rational malicious objective. By contrast, it can never become a rational objective on the BTC blockchain.

-------------------------

free | 2022-12-21 20:34:59 UTC | #64

[quote="JaMarco, post:62, topic:17504"]
So to be clear, you disagree with Vitalik Buterin‚Äôs assertion about 51/67% attacks?
[/quote]

I don't. If all you have is a single, public, persisted blockchain with thousands of validators, everything he says is true, But then he proceeds to explain why if you go through a bridge between two networks (which is not the same, but a close approximation of how subnets communicate) you can lose arbitrary amounts from a 51% attack (or 67% in the IC case) on a single network/subnet. Even if the examples he gives are Ethereum and Solana (two public blockchains with thousands of independent validators each), not "secretive" IC subnets.

You can achieve the same guarantees on the IC as on Ethereum as long as you're happy with only having a single subnet. And commensurate costs.

[quote="Sabr, post:63, topic:17504"]
An independent auditor or random cryptographic expert would likely be running the blockchain calculations as publicly designed and documented, along with blockchain code prior to the upgrade, to compare against the block results of the code recently implemented in order to verify that no unexpected discrepancies occurred (i.e., that no erroneous code was implemented).
[/quote]

What you are describing is a virtual machine (like the EVM or Wasm) for a programming language. The IC's deterministic state machine is more like an OS managing virtual machines (individual canisters) than a glorified interpreter / JIT compiler. Not all of it is described in a specification. It could be, but it would have to be many, many thousands of pages long, because it would have to describe details such as how many messages and how many bytes can be enqueued into a stream going to an application vs. a system subnet; or which messages count versus which memory limit on which subnet type; or an exact description of how messages from various sources are prioritized when being routed; down to details such as, when using the random tape as a source of randomness, in which order the various random values are generated; and so on, and so forth.

Furthermore, because this is basically an OS that is under continuous development, such details do change from one release to the next (e.g. because someone realizes that in order to protect the NNS subnet from DoS attacks, some new rule or limit needs to be enforced in just the right place under just the right circumstances). So you couldn't possibly use one replica version to validate the correctness of another replica version, because they would most likely diverge a couple of blocks in. For reasons that have nothing to do with calculation accuracy per se.

[quote="Sabr, post:63, topic:17504"]
I‚Äôm not talking about auditing for node consistency, but rather block (or state) calculation accuracy.
[/quote]

See above. You cannot possibly validate the behavior of a canister in isolation from its subnet (because you couldn't even tell when it gets executed, much less what its inputs would be, or in what order). I imagine (even though I have no clue whether this is so) that Ethereum's EVM simply executes every transaction in a block, in the same round, in the exact order that they appear in. An IC subnet inducts said messages into canister input queues (with a host of rules regarding what is considered a valid message; and a whole lot of possible outcomes for inducting each message). The canisters are then scheduled, with some messages executed in the same round, some message executions spread across multiple rounds, and some left in input queues for future rounds. Executed instructions are counted, as are dirtied memory pages and execution continues until some limit is hit. Rate limits are imposed on canisters and the subnet as a whole. If the instruction limit is not hit, messages from local canisters to local canisters are routed; in a specific order; and everything starts from the top. In the same round.

In many respects, validating the IC would be closer to validating an (admittedly fully deterministic) OS by recording and replaying keystrokes and mouse movement; than it would be to validating the correctness of an EVM instance.

[quote="Sabr, post:63, topic:17504"]
More important, the fact that this is ‚Äúdefinitely‚Äù so, as you confirm, means that the motivation to hijack the IC might actually become a rational malicious objective. By contrast, it can never become a rational objective on the BTC blockchain.
[/quote]

I would recommend reading [Vitalik's post](https://old.reddit.com/r/ethereum/comments/rwojtk/ama_we_are_the_efs_research_team_pt_7_07_january/hrngyk8/) (linked by JaMarco above) about why Ethereum (and BTC) would be fine with virtually any amount of malicious, colluding validators. And why this does not apply across blockchains (which also includes IC subnets). It has very little to do with the presence or absence of independent validators; and very much to do with the fact that, as soon as you have a system consisting of more than one virtual machine (blockchain / network), if something is temporarily out of whack on one blockchain, it has persistent effects on the other, even if the former issue is later sorted out.

BTW, I'm by far not a security expert. I'm a lowly systems software engineer, working on the ICs message routing. I'm simply aware of the complexity of the code that I helped write and of the fact that fully documenting its behavior so it can be reproduced identically by someone else would require insane effort. And I've been oncall for the IC and helped put together some of the monitoring infrastructure. And listened in on many, many conversations about security, determinism and whatnot.

-------------------------

Sabr | 2022-12-21 21:51:27 UTC | #65

So, @free, what you are basically saying is that blockchain calculations on the IC are impossible to independently verify or audit, no matter what, even after the fact. In other words, "trust me, bro" is the answer the general public ultimately has to be satisfied with, even after trillions in assets might be sitting on the IC in the future. OK, then, I guess that discussion is over.

Putting this doomsday conclusion aside for a moment, that VB post does give me some comfort regarding the protection of blockchain assets, particularly this part:

"Even if 99% of the hashpower or stake wants to take away your ETH, everyone running a node would just follow the chain with the remaining 1%, because only its blocks follow the protocol rules."

However, it would only give me corresponding comfort on the IC if I had some independent assurance that the IC protocol rules would never allow a 51% (or 67%) attacker to take away someone's ICP (or intellectual property on the IC). Where can I get that assurance today? And where can I get it after each protocol upgrade, any one of which could violate those sacred protocol rules? Even if those cryptography rules exist on the IC too, I suspect that I won't be able to get similar assurance from the IC, simply because the blockchain history is not persisted anywhere like it is for BTC or ETH. So if I had 1 million ICP and someone edited that state to zero a month ago, I'm not sure there would even be a sufficient audit trail to show that I ever had any ICP. 

At the very least, there should be some way to create a master continuity schedule to audit the consistency of state for all neurons. For example, if my ICP drops from 1 million to zero, another neuron must go up by the same amount via a mutually signed transaction between those two neurons. An independently verifiable continuity schedule of ICP balances across all neurons should always be obtainable. Are you implying that even this is not possible without malicious actors faking balances via boundary nodes, sending fake node messages or other means? 

If so, then how can anyone even disprove today (or any day in the past) that up to a majority of ICP balances in neurons are not even real? Or how can we even disprove that ICP might in fact just be one big Ponzi scheme in a black box that we are not allowed (or forever unable) to look into, while we continue to invest more and more real money into that black box? I'm struggling here to get even a basic level of point-in-time verification assurance over assets on the IC. I find this persistently troubling and increasingly frustrating, in spite of my relatively high confidence in the IC's on-chain security over transactions.

-------------------------

free | 2022-12-21 22:27:32 UTC | #66

There exists a full history of all NNS blocks. As well as all the replica binaries that produced and executed them, And the source code they were built from, although I believe that for the first few months we did not have reproducible replica builds.

It has not been published because it contains all of the II logins and it would be trivial to use those to map per-application principals that are supposed to keep users anonymous back to the original user principals (you're not supposed to be able to reverse engineer a user principal from an application principal). The II canister has recently been migrated to a different subnet, hence Manu bringing up the topic of publishing subnet blockchains now.

ICP balances are kept in the ledger canister, hosted by the NNS subnet. So it is possible to replay and fully verify the full history of the NNS, since genesis (there exists an ic-replay tool that is used for this purpose, as well as for debugging or disaster recovery).

Regarding my "doomsday conclusion", I never said it would be impossible to independently verify or audit an IC subnet. All you'd need is a machine that is at least as powerful as an IC node (64 cores, 512 GB of RAM, 6 TB of server grade SSD), since no subnet state is persisted for more than about half an hour; so you'd have to replay everything since Genesis and then keep up with the subnet. You can either use the replica binaries that were running at the time to do so; or rebuild them from source code (and closely review it); or you can analyze and fully describe the behavior of each replica version, then create a clean room implementations from that.

-------------------------

Sabr | 2022-12-21 23:29:32 UTC | #67

[quote="free, post:66, topic:17504"]
Regarding my ‚Äúdoomsday conclusion‚Äù, I never said it would be impossible to independently verify or audit an IC subnet. All you‚Äôd need is a machine that is at least as powerful as an IC node (64 cores, 512 GB of RAM, 6 TB of server grade SSD), since no subnet state is persisted for more than about half an hour; so you‚Äôd have to replay everything since Genesis and then keep up with the subnet. You can either use the replica binaries that were running at the time to do so; or rebuild them from source code (and closely review it); or you can analyze and fully describe the behavior of each replica version, then create a clean room implementations from that.
[/quote]

This is what I already suggested, either done by someone in the general public or by an independent auditor with special access funded by the community. Then you said this was impossible or stupid because boundary nodes could be faked, messages from other nodes could be faked, entire subnets could be hijacked, etc., etc., etc., making any single replica node (or even replica subnet) effectively useless as a source of assurance. Even when I asked for the bare minimum verification assurance on ICP balances, this too is not possible due to reverse engineering or other concerns. Have you not been listening at all to what I've been saying about an NNS-approved auditor to validate blockchain information that is supposedly too sensitive to release? 

At every turn, you fanatically resist *any* suggestion that could verify assets on the IC as being valid, which is raising all kinds of red flags in my view. Sorry, I'm not engaging in any more of this absurd tail chasing. Either provide direct access to the public to validate blockchain state or produce independent verification assurance from a third-party of blockchain state reliability on a regular basis. If you can't do either, then sit down and stop asking the crypto industry to trust the "moon math" like they trusted SBF. I'm sorry, but after all this waffling, you are no longer credible in my view. I'm muting any further responses from you.

-------------------------

JxBrian | 2022-12-22 01:29:18 UTC | #68

[quote="Sabr, post:67, topic:17504"]
If you can‚Äôt do either, then sit down and stop asking the crypto industry to trust the ‚Äúmoon math‚Äù like they trusted SBF. I‚Äôm sorry, but after all this waffling, you are no longer credible in my view. I‚Äôm muting any further responses from you.
[/quote]

üòÇüòÇ But some of us in the community like challenges. This ‚Äúmoon math‚Äù is a paradox ‚è± That only the dedicated mathematicians can achieve with time. You want this to happen right now and don‚Äôt want to put hard work in doing research and figuring out the right way to do things. Of course this is not supernatural knowledge; it is something that man created hence man would solve the math.

-------------------------

Choiseongin | 2022-12-22 01:40:22 UTC | #69

Get rich by exploiting the alleged vulnerability.
Learning from failure solves everything.

-------------------------

Mr_Burkes | 2022-12-22 02:11:55 UTC | #70

@Sabr Yup, this frustrates me as well.

Seduced by "trust moon math", when asked to prove that the math is actually being executed, Dfinity never hestitates to shift the goal posts to "uhhh, we can't actually show you the math. Trust me bro".

-------------------------

bitbruce | 2022-12-22 10:07:48 UTC | #71

Blockchain = 1/3 cryptography + 1/3 economics + 1/3 game theory
The former leverages technology.
The latter two leverage the selfishness of human nature and the transparency of data. Without data transparency, economics and game theory cannot work on blockchain.

-------------------------

Denis | 2022-12-22 07:56:31 UTC | #72

I think I have understood the difference between the two sides in philosophical terms. Dfinity's representatives on this thread, @dominicwilliams and @free are Utilitarians, as engineers often are. All their arguments make sense from a Utilitarian standpoint. But Utilitarianism fails to comprehend certain basics of human nature. 
This issue goes beyond just public subnets. Take the case of seed investors. Williams has often argued that they have nothing to complain about since their profits are high even at the current all time lows for the coin price. On a thread about seed investors, I gave the example of the capuchin monkey fairness experiment to show that ideas of fairness and justice have a great impact on perceptions even among animals. Seed investors are justified in feeling unfairly treated no matter the size of their profits. A Utilitarian perspective simply cannot grasp that.
https://forum.dfinity.org/t/node-compensation-and-runaway-massive-inflation/12609/82

We do not live our lives in Utilitarian ways. Think of the sheep trapped in a mine shaft in Wales which was saved after a huge and expensive rescue effort. From a Utilitarian perspective, it makes no sense to save a sheep at great cost while also eating sheep for dinner. The sheep at dinner is defined by a certain monetary value while the sheep in the mine shaft is defined independently of it. This seems contradictory and yet it speaks profoundly to what we are as human beings.
Crypto has a foundational ideology which involves individuals being able to verify for themselves the correctness of a blockchain. Of course, as blockchain sizes get bigger, this becomes progressively less practical, and there is progressively less Utilitarian justification for incorporating public verifiability. There is therefore good reason to walk back commitments to public verification. And there certainly exist forms of crypto fundamentalism among people like BTC maxis which take the ideology too far, as every ideology is taken too far by some factions. But equally, the crypto community would simply not exist without crypto ideology, so it is crucially important to pay some heed to that. In fact, there are Utilitarian reasons to respect it based on how widespread the belief system is within the developer, investor and customer base.
I would therefore urge the Dfinity foundation to institute some form of public verifiability for the IC blockchain, no matter the cost. The feature is essential if the foundation wants the IC to be perceived as a trustless, decentralized blockchain rather than one controlled and operated entirely by the Foundation itself. 
Taking off from the Genesis metaphor, Dfinity can plan a Noah's ark of data for the inflection point dividing the period of no public verifiability from one when the IC becomes a verifiable blockchain. Whatever data cannot be made public because it will endanger privacy can be left out, I am sure the community will understand, because individual privacy is also an element of core crypto ideology.

-------------------------

Jonathan | 2022-12-22 09:07:51 UTC | #73

> On a thread about seed investors, I gave the example of the capuchin monkey fairness experiment to show that ideas of fairness and justice have a great impact on perceptions even among animals. Seed investors are justified in feeling unfairly treated no matter the size of their profits.

I think it's worth pointing out that capuchin monkeys have also been known to throw their feces and eat their young.

The clip you reference is one that I've played for several people. The range of reactions I've seen is interesting. Some feel more justified in their feelings of jealousy. Others are embarrassed for having acted like monkeys.

I do think you have a point about humanity. One thing that sets the NNS apart in my mind is how very human it actually is. A single NNS proposal can potentially wreck the system. We can set up guardrails and invoke the virtues of a "trustless" environment, but ultimately how we behave is up to us.

-------------------------

free | 2022-12-22 11:37:00 UTC | #74

[quote="Sabr, post:67, topic:17504"]
If you can‚Äôt do either, then sit down and stop asking the crypto industry to trust the ‚Äúmoon math‚Äù like they trusted SBF. I‚Äôm sorry, but after all this waffling, you are no longer credible in my view. I‚Äôm muting any further responses from you.
[/quote]

I am sorry you feel that way. You have my sincere apologies.

I was chasing down what I felt were inaccuracies and too high expectations in your and others' statements (that taking over a subnet was trivial; that if a subnet was taken over, an independent audit would serve as much more than a belated notification; that the security measures of a single blockchain can be applied to a collection of blockchains and achieve the same level of security/trust/assurance). And neglected your central point, which was about producing and publishing audits of a subnet's blockchain.

In my (somewhat flimsy) defense, I started from the assumption that merely having a continuous quorum of replicas validate a subnet would be essentially the same as auditing it. But it is true that the only output of that process is merely the current state of the subnet, not an explicit audit of exactly which blocks were validated by which replica and whether any divergence was detected in the process.

In the meantime I kept repeating to whoever would listen that I am not fundamentally opposed to either publishing or auditing subnet blockchains, but merely trying to gauge how useful the proposed solutions were likely to be for various ends. And hoping to land on a sweetspot between cost/overhead and benefit, considering the above.

My conclusion thus far (and it is merely my opinion, not something I'm trying to force on anyone) is that adding some form of public audit trail to each subnet replica would be a reasonable (and cost-effective) first step. On top of that, one can argue for or against the utility of making the NNS and/or other subnets' blockchains fully public; or having a second class of replicas (that may or may not run a different implementation of the deterministic state machine) as subnet auditors; or rely on the replicas themselves to act as auditors; having a layer on top of that to ensure that every canister message inducted by a subnet was actually produced by some other subnet. And so on and so forth.

I for one have learned a lot from this discussion, both in terms of what properties one may expect from a network such as the IC (e.g. Vitalik's post, linked above) and in terms of having an online conversation.

Edit: In the hope of calming down the spirits that I helped stir up, I will point out a couple more things:
1. I do not speak for DFINITY here. I am a team member, but that's it. The above are my opinions. They may be wrong and other DFINITY team members will likely disagree with much or some of what I said. I wasn't asked by anyone to state this, just thought I'd explicitly say out loud something else that I had previously assumed everyone was aware of.
2. DFINITY has not, should not and very likely will not turn down an NNS motion proposal requesting public subnets; public audit trails; or whatever else; as long as it is technically achievable. Whatever my or DFINITY's position on said issue might be.

-------------------------

Sabr | 2022-12-22 15:57:51 UTC | #75

[quote="bitbruce, post:71, topic:17504"]
Without data transparency, economics and game theory cannot work on blockchain.
[/quote]

And a fanatical insider resistance against any reliable way -- even off-chain and after the fact -- to obtain independent audit assurance in the absence of public data transparency is a huge red flag that the economic game is already afoot.

-------------------------

Denis | 2022-12-22 18:32:52 UTC | #76

[quote="Jonathan, post:73, topic:17504"]
The clip you reference is one that I‚Äôve played for several people. The range of reactions I‚Äôve seen is interesting. Some feel more justified in their feelings of jealousy. Others are embarrassed for having acted like monkeys.
[/quote]

I trust you have then told them they need not feel embarrassed, for resisting inequity is quintessentially human. The animal experiments only show it goes even deeper than just homo sapiens.

-------------------------

Jonathan | 2022-12-22 23:12:34 UTC | #77

[quote="Denis, post:76, topic:17504"]
I trust you have then told them they need not feel embarrassed, for resisting inequity is quintessentially human. The animal experiments only show it goes even deeper than just homo sapiens.
[/quote]
I did not tell them this, in part because I try to refrain from telling people what they need to feel or not feel. But also because I don't agree with the premise.

Justice, I think, is a critical component of human society. I don't understand how it can be quintessentially human, though. There are people who don't resist inequity for a variety of reasons (generosity, greed, temperance, indifference, incapacitation). They are still human, even moreso I think than the capuchin monkey.

-------------------------

Denis | 2022-12-23 03:26:07 UTC | #78

[quote="Jonathan, post:77, topic:17504"]
I did not tell them this, in part because I try to refrain from telling people what they need to feel or not feel. But also because I don‚Äôt agree with the premise.

Justice, I think, is a critical component of human society. I don‚Äôt understand how it can be quintessentially human, though. There are people who don‚Äôt resist inequity for a variety of reasons (generosity, greed, temperance, indifference, incapacitation). They are still human, even moreso I think than the capuchin monkey.
[/quote]

I don't understand why you would show many people the clip as you claim to have done, in that case, but each to their own.
If something is quintessentially human, it does not follow that it must be a behaviour visible in every single human being. Also, there are hundreds of modes of resistance, few of which rise to the level of visible rebellion. But we digress.

-------------------------

JaMarco | 2023-02-06 07:20:31 UTC | #79

[quote="dominicwilliams, post:1, topic:17504"]
Even if some technically feasible way to transparently embed local nodes inside web browsers was found, perhaps by making the local nodes more efficient using zero knowledge proofs, say, so that direct interaction could be enabled using a hybrid of the old model, it‚Äôs doubtful anyone would want this!
[/quote]

Why wouldn't anyone want that?

-------------------------

