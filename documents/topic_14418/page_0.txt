karim | 2022-07-20 08:35:31 UTC | #1

I'm amazed at the results that DALL-E and other AI image generation models deliver and so I've started thinking about AI and machine learning on the IC. One of the advantages of the IC is that (as far as I can tell) almost any application can now run directly on-chain (aka served from a blockchain). The only exception I'm seeing are compute-heavy applications like AI. Am I right to assume that even if someone ported all the necessary libraries commonly used in machine learning (Pytorch, etc.) to Rust (or Motoko?), it would be a waste of resources to run these compute-heavy applications on the IC because all the processes would end up being replicated on multiple machines? Are there other reasons that make AI on the IC an impossibility or just a stupid idea?

-------------------------

mnl | 2022-07-21 10:39:59 UTC | #2

I think that depends on what you're referring to. Today you could train neural network outside of IC, and upload the trained model to your canister built with Rust (I'm sure there are webassembly-compatible crates which can consume PyTorch or TensorFlow pre-trained models). If you'd then query that canister using `raw.ic0.app` endpoint, it would return without running the query on multiple nodes (therefore 0 waste), however `.raw` may return incorrect results if the node that was randomly assigned to resolve your query is malicious, therefore it's better to use `ic0.app`, but that has to go through the consensus, therefore the query will have to run on multiple nodes. 

Whether that's feasible to do currently or not, depends on the size of the trained neural network model. If you'd even [be able to fit DALL-E model inside the canister](https://www.reddit.com/r/dalle2/comments/vii9w9/comment/idddnbc/?utm_source=share&utm_medium=web2x&context=3), I think, given its complexity, making it run could be a challenge. However, I can totally imagine successfully running [MNIST classifier](https://en.wikipedia.org/wiki/MNIST_database) on the IC today. These two examples represent a) state of the art, and b) hello world, and what's currently possible (i.e. the limit) lies somewhere on the spectrum between those two.

If you're referring to training neural network, then your assumptions are correct, the training process would be (with current status quo) a huge waste of resources. I'd also take a very long time as currently the only processing units available on IC are CPUs, and we have no way to attach dedicated hardware to the IC (like tensor processing units, or GPUs).

-------------------------

cymqqqq | 2022-07-20 14:27:00 UTC | #3

The same question on me, 
Maybe it's possible to deploy the model on IC unless we explore a unique way to break the current BP algorithm.
Maybe reinforcement learning? But who knows?

-------------------------

jzxchiang | 2022-07-21 07:02:20 UTC | #4

I think you'd need a good use case for AI on a blockchain. What does blockchain offer AI that traditional computing cannot?

One idea that's really exciting to me is a "universal classifier". For example, imagine some massive neural network running on the blockchain. It's owned and controlled by no one party. Why is that good? That means people could trust it with their training data. In exchange for tokens, people around the world can provide training data, which the classifier will then validate and train on if validated, in an online setup. The more and the better the training data you provide, the more tokens you get. The tokens would be used to pay for inference.

This incentivizes people who have data but maybe don't have ML expertise or GPU resources to contribute their data, without fear that it would be used maliciously by Big Tech. The end result would be  humanity working together to create the most powerful model in the world (for a given domain), instead of the status quo where multiple corporations train with their own siloed datasets.

-------------------------

MCLYang | 2022-07-22 03:17:20 UTC | #5

I think there are 2 prerequisites to enable IC ML/DL practice: 

1. We need a package like motokoTorch/RustTorch to handle tensors. The back-propagation and autoDiff must be carefully implemented by hand. I would like to refer to Cornell professor Rush's module how to build torch from scratch: https://minitorch.github.io/

2. We need install hardware in IC data center supporting parallel computing. 

The (1) would require some human effort but I believe it's feasible. However, I don't believe we can achieve (2) in a short period time. It is depends on Dfinity if they would like to add this usecase in the roadmap. Meanwhile, the motoko seals the compelling at application-level and we can't program canister to decide which subnet your application deploys on. It is impossible to allocate memory of tensors to which chip unite like CUDA based on current ICP architecture. I am not a programing language expert but I am pretty sure It will be a huge upgrade to unseal the motoko compelling. 

Here are some of my thoughts. It would be an interesting project to build an L2 consensus layer on ICP to focus the parallel computing and AI development usecase. It would be a very tough but very impactful. To avoid fancy hardware upgrade and complicated parallel computing, you also can try to train your model locally and deploy for inference in IC and run everything via IC data center CPU, but still require someone to implement(1) and automated tool to translate from Python to Motoko/Rust. 

Correct me if I am wrong plz

-------------------------

cymqqqq | 2022-07-22 01:26:26 UTC | #7

I agree with you about the "handle tensor".
I think if we want to implement deep learning on IC.
The first problem that we must resolve is how to implement a large amount of matrix computation.
And, you are right, we need to install the specific hardware to implement parallel computation and tensor computation.

-------------------------

integral_wizard | 2022-07-27 21:00:17 UTC | #8

Just like Badlands can/ should be a separate subnet, can dedicated hardware be special subnets. I would expect exactly this to happen at some point.

Traditional  cloud providers offer a wide array of node types and to eventually bring over most adoption  we should offer more capabilities than general compute. It comes down to priorities and probably, imo, we could get around special hardware for now.

-------------------------

mnl | 2022-07-29 16:22:10 UTC | #9

Badlands could be the same subnet as the one for dedicated hardware, sort of `plug-n-play hardware wild wild west`. Few classes of devices could be supported:
|device class|example hardware|use case|
|-|-|-|
|microcontrollers|Arduino, ARM, RISC-V|?|
|mini-pc|Rasbperry Pi, smartphones|badlands|
|GPU|GeForce, Radeon|3d rendering, light-to-medium ML|
|TPU/IPU|NVIDIA A100, Graphcore|heavy ML|
|storage|NAS Storage devices|data archive|

You plug the device to the network, network then checks if the device is capable to support computation required by forementioned `device class` (standardized test), if it can't it gets kicked out, otherwise stays.

-------------------------

integral_wizard | 2022-07-29 17:38:44 UTC | #10

Yeah, smth like that. I like that. A playground for all types of experiments.

-------------------------

jzxchiang | 2022-12-07 06:47:46 UTC | #11

The incredible (even mindblowing) quality of ChatGPT responses makes me think supporting AI/ML on the IC would be a killer feature for a blockchain (if not for training, at least for inference). Compared to other L1s, the IC's model of deterministic decentralization—and its resulting performance properties—seems like a great fit.

It would unlock very interesting use cases, I think.

The exported model of one of these large language models would be O(gigabytes), so it could theoretically fit in stable memory without a problem. The challenge is reducing inference time, which would require GPUs or TPUs (and maybe RAM?).

-------------------------

GLdev | 2022-12-07 07:04:05 UTC | #12

I am as blown away as you are after playing around a bit with ChatGPT. The tech is absolutely amazing, and it's only gonna get better.

I am having trouble, however, finding a good value proposition to having this on a blockchain. Training seems to be out of the question, but even for inference, what's the gain here? Cost aside, would you really need a replicated, verified inference?

Speaking about cost, it would be really hard to come up with advantages of using 8x hardware to get 1x benefits at the end of the day. I'm in the middle of a large ML project at work, and finding cheap GPUs for both training and inference is challenging as it is. I'm not seeing the benefits of 8x-ing that cost at the moment. Perhaps for some extreme niche use-cases? Maybe, but I'm not convinced.

LLMs are already at the multiple gigs sizes, and only getting larger. The open-source ones, GPT-J and GPT-NeoX need 8/16 - 40GB of VRAM just for inference, probably multiples of that for training. Having that hardware further multiplied for consensus would make the cost prohibitive even compared to the "big 3" cloud providers.

Furthermore, there's a challenge with time-to-market and the ever-evolving ecosystem. By the time you get to implement something (and it's probably gonna take some time to get to a production-level replicated, consensus-aware solution) it's probably gonna be outdated. Just looking at the StableDiffusion space, the speed of development and upgrades in the 6 months since it's been released is amazing. It got from ~10 seconds of inference w/ 16gb VRAM to ~4 seconds and 2-4gb VRAM needed, inside a couple of months. They've added Adam optimizers, fp16 and so on. It's gonna be really challenging to support anything like that speed of development in a blockchain space.

-------------------------

jzxchiang | 2022-12-11 01:34:17 UTC | #13

> Cost aside, would you really need a replicated, verified inference?

I totally agree with you that naive replicated execution won't cut it, even for inference (and especially for training).

I think that the IC may eventually need to consider more efficient ways to implement replicated state machines without actually replicating computation. Here is a fascinating [paper](https://eprint.iacr.org/2020/195.pdf) I found. Here is [another](https://dl.acm.org/doi/abs/10.1145/2641562) one. It seems like the key lies in cryptographic proofs (whether zero-knowledge or otherwise), and making proof verification as efficient as possible.

Another interesting thing to consider is that transformer neural networks are easily parallelizable (compared to its predecessors), which may lead to interesting performance properties when combined with verifiable state machines perhaps?

@JensGroth @Manu - curious if internally you guys have considered verifiable state machines as I described above, especially for expensive computations like ML?

-------------------------

JensGroth | 2022-12-12 10:45:03 UTC | #14

The flavor of the papers is that instead of replicating computation, you outsource it to a single party who does the computation and also provides a proof the computation is correct. The most efficient proofs are much faster to verify than redoing the computation, so now only one party has to do the computation but everybody can trust the outcome because they can cheaply verify the proofs of correctness. 
Unfortunately, the cost of proving the computation is correct is several orders of magnitude higher than the cost of doing the computation. For blockchains with huge replication this is a tradeoff that may be worth taking, but since the IC operates at modest replication (most subnets have 13 nodes) it does not pay off, it is cheaper to do replicated computation as we currently do.
Having said that, I'll not discard the approach for ML. Sometimes special purpose computation is of a form that is amenable to more efficient construction of proofs. This is not something we're working on inside DFINITY but I don't think it needs any platform changes anyway, afaict you just need to verify the outside computation is correct, and since verification is cheap you can run a  verifier in a canister (probably the more costly part, depending on application, will be the bandwidth of an ingress message to update the state of the canister to reflect the outcome of the computation)

-------------------------

neeboo | 2022-12-12 10:54:39 UTC | #15

I personally tried running a zk-paillier program on canister recently.  However the construction of proofs requires a lot of instructions, which exceeded the limits of single message can handle. Even before we did try to migrate some deep learning algo to canister which is also limited by the instructions.

-------------------------

JensGroth | 2022-12-12 11:10:37 UTC | #16

Random idea building on this (and somewhat orthogonal to ML): could one imagine a data marketplace. You can sell encrypted data to it for tokens. And buyers of data can use the future [threshold decryption](https://forum.dfinity.org/t/threshold-key-derivation-privacy-on-the-ic/16560?u=jensgroth) feature to get decrypted data. Running on the IC may give new opportunities, e.g., the canister smart contract may restrict who data can be sold to or how many times it can be sold.

-------------------------

jzxchiang | 2022-12-13 06:59:11 UTC | #17

> Unfortunately, the cost of proving the computation is correct is several orders of magnitude higher than the cost of doing the computation.

I am by no means an expert in this field, but judging by the pace of progress in the last decade in making cryptographic proofs more efficient (in some cases, a trillion times more efficient), I wouldn't count out the [possibility](https://a16z.com/2022/04/15/zero-knowledge-proofs-hardware-decentralization-innovation/) of huge leaps in efficiency in this decade, whether from hardware or better algorithms!

>  afaict you just need to verify the outside computation is correct, and since verification is cheap you can run a verifier in a canister

This is a really interesting point and something that worries me—do we even need blockchain to do *compute* or is storing data on a blockchain enough? Ethereum seems to be taking the latter approach, by outsourcing transaction computation to L2 rollups. If an external party can do the expensive ML computation off-chain and simply provide a proof of it (without incurring the efficiency costs of consensus), why would we need blockchain for stateless ML inference? Or am I missing something?

> Random idea building on this (and somewhat orthogonal to ML): could one imagine a data marketplace.

This is actually quite deep... Online marketplaces like Uber, Amazon, and Airbnb work fine on web2 because the good or service is usually physical, and there's no fear of the marketplace itself stealing it. But a web2 data (or even ML model) marketplace wouldn't work, because the marketplace could steal the unencrypted IP unbeknown to the seller. (Although I'm not sure whether the Signal Protocol running on web2 servers could solve this problem as is...) So IIUC, the on-chain encryption (or threshold key derivation) feature could make this marketplace possible (using web3 tech).

-------------------------

hugoelec | 2022-12-13 12:13:08 UTC | #18

would icoracle has the solution for this kind of problem? utilizing off like computing power to submit calculations works with database handling. but it seems jump outside of current ICP structures.

![Screenshot_20221213_200824|292x500](upload://udykOkl56fih2y5cSgQqoSpc4SH.jpeg)

-------------------------

MCLYang | 2023-01-30 15:15:59 UTC | #19

Before jumping into the tensor multiplication, did anyone try to build a linear regressor on IC? I guess the "ICML" can be built in the following order to approach deep learning eventually:
Linear Regression -> K-means -> Decision tree -> Autodifferentiation -> Tensor object and operation -> CNN & Transformer -> ResNet, BERT -> GPT, Dalle etc.

-------------------------

cymqqqq | 2023-01-30 15:17:02 UTC | #20

There are many machine learning and deep learning libs based on rust, but I haven't tested them on IC. 
Maybe the difficulty is not to build tensor computation on IC but how to run a pre-trained deep learning model.

-------------------------

cymqqqq | 2023-01-30 15:23:26 UTC | #21

[quote="MCLYang, post:19, topic:14418"]
Linear Regression → K-means → Decision tree → Autodifferentiation → Tensor object and operation → CNN & Transformer → ResNet, BERT → GPT
[/quote]
Hi there.
Linear Regression -> K-means-> Decision tree => machine learning.
CNN & Transformer -> ResNet, BERT ->GPT => deep learning model.
Now I think it's impossible to train a model on IC, I'm researching a rust lib whether it can be deployed on IC.

-------------------------

MCLYang | 2023-01-30 15:25:55 UTC | #22

I didn't code much in Rust on IC. If the canister is able to load the models from Rust plz give let me know lol :sweat_smile:

-------------------------

cymqqqq | 2023-01-30 15:29:12 UTC | #23

Sure, I will post the result here

-------------------------

cymqqqq | 2023-01-31 01:56:30 UTC | #24

Hi everyone, I have tested the pre-trained deep learning model named "albert2" on IC.
I add an onnx model into the contact folder and use Path lib to read it.
Then I add the tokenizer,json file for performing natural language tasks.
Here is my Cargo.toml:
[dependencies]
candid = "0.7.14"
ic-cdk = "0.5.2"
ic-cdk-macros = "0.5.2"
tract-onnx = "0.19.2"
ndarray = "0.15.6"
tokenizers = "0.13.2"
tract_onnx: a library that can load the model built by pytorch, tensorflow, or jax.
ndarray; a library that can perform multi-dimensional tensor computation.

[target.'cfg(target = "wasm32-unknown-unknown")'.dependencies]
getrandom = { version = "0.2.6", features = ["js"]}
The errors come out for the following aspects:
1. random dependencies:
When you deploy the canisters that needed the random something, you must add:
getrandom = { version = "0.2.6", features = ["js"]}
2. head file error:
This was what I meet here:
![image|477x500](upload://hqVLTv3TWKl6KlfUIo53AMzeQGZ.jpeg)
![image|437x500](upload://3o92L7bW1IO7LSfpBC3FzknmcuQ.jpeg)

In one word, the first experiment failed. 
But I already have some road maps for the next experiment, so I will continue to post my experiment result here.
So if anyone has some ideas, you can post them here and let me know, thx :grinning:
BTW, all code was written by rust.

-------------------------

Sormarler | 2023-01-31 06:12:47 UTC | #25

Following your experiment. I think this is a domain the foundation should definitely invest in.

-------------------------

MCLYang | 2023-02-03 09:38:22 UTC | #26

do u have a GitHub repo link?

-------------------------

MCLYang | 2023-02-03 09:49:59 UTC | #27

As far as we practice, IC currently has a limitation for updating the var state because each round of consensus will end up creating a greater memory heap. Meanwhile, I am not so sure how canister handles the time-out problem for the execution. If training takes a while and computation is going on, how does the canister manage the call?

-------------------------

Severin | 2023-02-03 10:13:28 UTC | #28

[quote="MCLYang, post:27, topic:14418"]
If training takes a while and computation is going on, how does the canister manage the call?
[/quote]

We have DTS now which helps a bit but is still limited. What you can do is repeated self-`await` because every `await` is a commit point and starts a new message (and therefore the execution limit). If you could e.g. perform one epoch of training in a single message then you can just `await` every epoch and you should be fine.

-------------------------

cymqqqq | 2023-02-03 10:44:21 UTC | #29

No, not training, just upload your model to IC for inference.

-------------------------

cymqqqq | 2023-02-03 10:44:57 UTC | #30

sorry, I delete it, but I will update new repo later.

-------------------------

icpp | 2023-02-06 00:24:13 UTC | #31

I am also very interested in exploring the inference option using C++ compiled to wasm .

-------------------------

cymqqqq | 2023-02-06 13:58:22 UTC | #32

Hi all, so there is a question: for my experiment, there must be a basic random lib based on rust that can generate different kinds of distribution , but now the most rust random lib can not be compiled on IC. So I want to ask you guys who can recommend a simple random lib? We can build a basic IC random lib.
Here is the code from the open storage:
pub struct CanisterEnv {
    rng: StdRng,
}

impl CanisterEnv {
    pub fn new() -> Self {
        CanisterEnv {
            // Seed the PRNG with the current time.
            //
            // This is safe since all replicas are guaranteed to see the same result of
            // timestamp::now() and it isn't easily predictable from the outside.
            rng: {
                let now_millis = time::now_nanos();
                let mut seed = [0u8; 32];
                seed[..8].copy_from_slice(&now_millis.to_be_bytes());
                seed[8..16].copy_from_slice(&now_millis.to_be_bytes());
                seed[16..24].copy_from_slice(&now_millis.to_be_bytes());
                seed[24..32].copy_from_slice(&now_millis.to_be_bytes());
                StdRng::from_seed(seed)
            },
        }
    }
}

impl Environment for CanisterEnv {
    fn now(&self) -> TimestampMillis {
        time::now_millis()
    }

    fn caller(&self) -> Principal {
        ic_cdk::caller()
    }

    fn canister_id(&self) -> CanisterId {
        ic_cdk::id()
    }

    fn random_u32(&mut self) -> u32 {
        self.rng.next_u32()
    }

    fn cycles_balance(&self) -> Cycles {
        ic_cdk::api::canister_balance().into()
    }
}

-------------------------

mnl | 2023-02-06 14:14:49 UTC | #34

Hi @cymqqqq, to get random value try calling https://docs.rs/ic-cdk/latest/ic_cdk/api/management_canister/main/fn.raw_rand.html

-------------------------

cymqqqq | 2023-02-09 07:13:08 UTC | #35

Hi there, I'm glad to tell you guys that I have successfully deployed a word2vec canister model on IC, it's my second machine learning experiment. Later on, I will post the github repo link.
Though it's a toy version, I will continue to update it.
![image|690x318](upload://assWgupISnOlCWJQWzP390qJYHw.jpeg)

-------------------------

mnl | 2023-02-09 12:03:39 UTC | #36

congrats, fantastic work!

-------------------------

cymqqqq | 2023-02-18 09:38:35 UTC | #37

It's a toy version machine learning canister,  so everyone can deploy and test it.
BTW, it's a shit version :rofl:
https://github.com/cymqqqq/machine_leanring.git
But, I will continually update a tensor library that can perform machine learning algorithms written in Rust like KNN, simple neural networks, etc.... And I will post the link later. :laughing:
So everything started from the first code.
I hope the ecosystem of machine learning in IC will be more advanced in 2023.

-------------------------

MCLYang | 2023-02-20 05:16:19 UTC | #38

Jesus Chris… that can’t be real. I am currently very packed, but I will try your code ASAP.

-------------------------

MCLYang | 2023-02-20 05:31:27 UTC | #39

If inference is possible (no training) on ic, that would be very mind blowing, because it means we can start to gently say good-bye to AWS’ lambda and Sagemaker👋👋 bth I don’t believe it’s happening now.

-------------------------

cymqqqq | 2023-02-20 05:57:16 UTC | #40

Hi, I understand your points, now I just write simple machine learning code to run. 
If we want to perform more machine learning computation on IC,  there is a lot of work to do now, the first is to write a basic tensor library, that's what I am doing now.
Now it's impossible to run large scale machine learning model on IC. :grinning:

-------------------------

cymqqqq | 2023-02-20 06:02:39 UTC | #41

[quote="MCLYang, post:39, topic:14418"]
If inference is possible (no training) on ic
[/quote]
Yes, it's possible in theory, as follows:
1. to transform pre-trained deep learning models into .wasm format(like onnx for .onnx format, keras for ,h5 format and the others), 
2.  to put the wasm file inside a canister and deploy it on IC.

-------------------------

Gamris | 2023-03-12 10:07:02 UTC | #42

Now that someone got [LLaMA and Whisper inferencing running on a Macbook M1](https://github.com/ggerganov/llama.cpp), I'm hoping that these models can somehow be ported to the IC more easily.

-------------------------

cymqqqq | 2023-03-16 02:00:10 UTC | #43

Hi all, I have a question, https://docs.rs/ic-cdk/latest/ic_cdk/api/management_canister/main/fn.raw_rand.html
So raw_rand is the Verifiable Random Function(VRF)? right?

-------------------------

Severin | 2023-03-16 08:15:46 UTC | #44

Have a look at this thread: https://forum.dfinity.org/t/is-raw-rand-a-vrf/18615

-------------------------

jb747 | 2023-04-12 18:29:53 UTC | #45

training a leading-edge, commercial large language model is very expensive (fine-tuning a LLaMA 64b variant is much easier but it cannot be used commercially) and it is done on clusters of high-end TPUs and GPUs designed to spread the load rather than duplicate it. The Gen-2 IC nodes and blockchain consensus aren't designed for this. 

The latest language models are increasingly capable of providing instructions to weaponize dangerous viruses and perform other dire acts so distributing the weights for inference is becoming a sensitive issue. The rapid increases in parameter count and training have resulted in sudden jumps in the ability of these models to reason. The benchmarks used to measure them are becoming significantly harder and the models are likely to surpass average human scores within the next 1-2 years. Dfinity and the 2nd gen clouds might well have to satisfy governmental scrutiny before they will be allowed to host inference weights for the newer models. I don't really use Siri or Alexa that much, but some believe that the new models will give a significant boost to the assistants and transform user interfaces. Regardless, the backends are very likely to make heavy use of the new models. If so, then persuading one of the (commercially-licensed) models to jump to the IC, at least for inference, might be important.

-------------------------

jb747 | 2023-04-13 18:55:04 UTC | #46

perhaps an optional inference co-processor or specialized inference software for the Gen-2 nodes would help.

-------------------------

Severin | 2023-04-13 19:02:37 UTC | #47

For now we're trying to keep nodes pretty homogenous because it keeps node management and node provider rewards substantially more simple. Mandating specialised HW for that seems a bit overkill for now. But for the further future having specialised nodes sounds really useful

-------------------------

jb747 | 2023-04-14 16:55:45 UTC | #48

it does seem likely that the move to 3nm will allow CPUs to dedicate more space to neural co-processors.. as the mobile/desktop ARM designs are doing.

-------------------------

jb747 | 2023-04-14 17:05:19 UTC | #49

if ChatGPT is not hosted on the IC, then it seems that a mechanism like https outcalls would be used. If so, my understanding, from https://internetcomputer.org/docs/current/developer-docs/integrations/http_requests/http_requests-how-it-works/, is that multiple requests would be sent to ChatGPT to satisfy a request from a canister and the results must satisfy a consensus check before they affect the state of the canister. ChatGPT intentionally randomizes its output to increase the appeal of the text it generates. Would this require a custom consensus check? Perhaps there are alternatives.. just curious at this point.

-------------------------

Gamris | 2023-04-15 14:49:47 UTC | #50

Vicuna (13B parameters, Llama based model) was finetuned for a low cost. It is possible to finetune or LoRA ICP and Motoko documentation then quantize to 4-bits to shrink the model down to a canister-friendly size.

As for inferencing, CMU utilized webGPU and wasm to run Vicuna 4bit over the web. A 10Gb+ VRAM GPU can run 13B-Q4 Vicuna locally, so I'm excited if WebLLM can do the same. https://github.com/mlc-ai/web-llm

-------------------------

cymqqqq | 2023-04-15 16:01:52 UTC | #51

If you want to inference it on IC, the challenge is how to generate candid file.

-------------------------

superduper | 2023-04-16 22:33:27 UTC | #52

[quote="jb747, post:45, topic:14418"]
might well have to satisfy governmental scrutiny before they will be allowed to host inference weights for the newer models
[/quote]

lol... ask permission? scrutiny of who 80 year olds who have 0 clue?  more like do it and ask for forgiveness later.  it's 1st come 1st serve in nascent markets... we want this stuff running on the IC and controlled by the NNS (has a nice safe guardrails spin to it lol)

-------------------------

superduper | 2023-04-16 22:42:44 UTC | #53

[quote="Severin, post:47, topic:14418"]
Mandating specialised HW for that seems a bit overkill for now.
[/quote]

who talks about mandating?  why isn't it possible to have various h/w subnets competing for rewards... instead of just going with some plain vanilla stuff... competition is always good esp when there is subsidies... otherwise some other networks will eat our lunch and we are inflation the ICP supply for less value then we extract out of these nodes existing (bad business imho)

-------------------------

superduper | 2023-04-16 22:41:05 UTC | #54

[quote="jb747, post:45, topic:14418"]
training a leading-edge, commercial large language model is very expensive (fine-tuning a LLaMA 64b variant is much easier but it cannot be used commercially) and it is done on clusters of high-end TPUs and GPUs designed to spread the load rather than duplicate it. The Gen-2 IC nodes and blockchain consensus aren’t designed for this.
[/quote]

this is rather unfortunate... we should want these billions of fiats flowing into this *very expensive* activity instead of subsidizing a bunch of nodes burning up electricity for nada they can start earning money for actually providing computation

![kfsibhcmsztjbc13ygtm|500x500](upload://yY8h0vfWonJSyovgWbAzsASzgtv.gif)

-------------------------

Severin | 2023-04-17 08:01:58 UTC | #55

[quote="jb747, post:49, topic:14418"]
ChatGPT intentionally randomizes its output to increase the appeal of the text it generates. Would this require a custom consensus check? Perhaps there are alternatives… just curious at this point.
[/quote]

There's a concept called `idempotent requests`, where multiple calls to the same API will produce the same result. So you'd have to have some mechanism to use the same randomness for multiple calls, otherwise there's no way to achieve consensus between the nodes

[quote="superduper, post:53, topic:14418"]
who talks about mandating? why isn’t it possible to have various h/w subnets competing for rewards…
[/quote]

The IC's consensus does not use competition between nodes like PoW or PoS. And subnets are independent from each other in the sense that they don't work on the same data/requests, so there's nothing to compete over either

-------------------------

Gamris | 2023-04-17 11:47:55 UTC | #56

Does it help that LLaMa inferencing is ported to rust and can run off the CPU? https://github.com/rustformers/llama-rs

Dolly 2.0 by Databricks is fully open. So you're not restricted like LLaMa to research purposes only when running that LLM.

-------------------------

cymqqqq | 2023-04-18 03:54:03 UTC | #57

Hi Gamris, I have read the code of this github.
But, unfortunately, it can not be compiled and deployed on IC now, because it has rand library and C library.
Maybe we can rewrite it in pure rust, and try some ways to deploy it on IC.🚀

-------------------------

abc | 2023-04-20 22:13:36 UTC | #58

[quote="MCLYang, post:5, topic:14418"]
It would be an interesting project to build an L2 consensus layer on ICP to focus the parallel computing and AI development usecase.
[/quote]

Just found https://edgematrix.pro/
 Looking forward to being helpful to you

-------------------------

superduper | 2023-04-18 23:15:24 UTC | #59

[quote="Severin, post:55, topic:14418"]
The IC’s consensus does not use competition between nodes like PoW or PoS. ... so there’s nothing to compete over either
[/quote]

that should be changed then. the IC shouldn't become some sort of node welfare state... distributing ICP to nodes that ain't doing nuthin' or providing much less marginal value vs potentially other new nodes/subnet. 

we are burning money and getting what for it?

I urge you Dom to move us away from a socialistic welfare state funding model to a free-market model as possible.

-------------------------

peterparker | 2023-04-23 11:10:30 UTC | #60

Just found a port of llama.cpp in ... Rust. Has anyone tried to put that in a canister (if that's even possible)?

https://github.com/rustformers/llama-rs

-------------------------

cymqqqq | 2023-04-23 11:18:15 UTC | #61

Yep, I tried and it failed:(

-------------------------

Gamris | 2023-04-24 09:08:14 UTC | #62

I found a GitHub for [Rust bindings for Tensorflow](https://github.com/tensorflow/rust) and one for [Pytorch Rust Bindings](https://github.com/LaurentMazare/tch-rs). Maybe there's hope to run a small HuggingFace model now?

-------------------------

cymqqqq | 2023-04-24 09:30:23 UTC | #63

the two repos I have tried last year, can not work on IC, because the low level of them are written in C++ and can not be compiled into wasm format.
So now, I'm coding my own neural network library based on Rust in my spare time and it can be boxed into the canister.

-------------------------

Severin | 2023-04-24 09:33:26 UTC | #64

`clang` can target wasm, so it shouldn't be impossible to port the C++ libraries. But it may be more effort than it is worth. If you're interested, have a look at a c canister example [here](https://github.com/dfinity/examples/blob/master/c/adventure/build.sh) (not in working state at the moment IIRC)

-------------------------

cymqqqq | 2023-04-24 09:35:09 UTC | #65

thx, I will try it later.

-------------------------

cymqqqq | 2023-04-24 09:42:04 UTC | #66

BTW.
Though it can be compiled into wasm format, how can we generate the candid file?

-------------------------

Severin | 2023-04-24 09:50:13 UTC | #67

By writing a generator yourself I suppose... Or maybe one of the C++ CDKs offers one already. There's a few ones you can discover through forum search.

For the beginning it's probably easiest to write it by hand. That's how I started in my first Rust canistrers too

-------------------------

mnl | 2023-04-24 11:19:55 UTC | #68

what about `burn` https://github.com/burn-rs/burn/tree/main/examples/mnist-inference-web ?

-------------------------

cymqqqq | 2023-04-24 15:54:15 UTC | #69

Hi mnl, I have researched the burn last week, it is also based on tch-rs which low-level is written in C++.

-------------------------

mnl | 2023-04-24 16:33:51 UTC | #70

> also based on tch-rs 

I don't believe that's the case, `tch-rs` is only one of the backends offered by the `burn` crate [see features:](https://github.com/burn-rs/burn#features) 
> * Versatile [Tensor](https://github.com/burn-rs/burn#tensor) crate with pluggable backends 🔧
>   * [Torch](https://github.com/burn-rs/burn/tree/main/burn-tch) backend offering CPU/GPU support 🚀
>   * [NdArray](https://github.com/burn-rs/burn/tree/main/burn-ndarray) backend featuring [`no_std`](https://github.com/burn-rs/burn#no_std-support) compatibility for any platform 👌
>   * [Autodiff](https://github.com/burn-rs/burn/tree/main/burn-autodiff) backend enabling differentiability for all backends 🌟

The `NdArray` backend is used for compiling into WASM and it's the thing used in that MNIST inference web example I linked in my previous post

-------------------------

mnl | 2023-04-24 16:59:48 UTC | #71

There is also [dfdx](https://github.com/coreylowman/dfdx) which compiles to `no_std` therefore it shouldn't be a problem to compile it to `wasm`. I believe It's being used in e.g. https://github.com/Narsil/fast_gpt2 to provide inference

-------------------------

cymqqqq | 2023-04-25 00:32:31 UTC | #72

Ok, so if it's possible to compile it into a wasm file, then the next step is to research how to generate candid file.:)

-------------------------

Gamris | 2023-05-02 02:48:43 UTC | #73

Found a repository of [ML Libraries for Rust](https://github.com/vaaaaanquish/Awesome-Rust-MachineLearning) and some bindings to popular Python frameworks. Huggingface tokenizer and SentenceTransformer would be really cool in the IC!

-------------------------

MCLYang | 2023-05-04 06:12:19 UTC | #74

Hello everyone,

I recently had an idea about incentivizing miners to help machine learning engineers and AI researchers train deep learning models. Instead of using computation power to guess numbers for SHA256, the idea is to switch to computing the loss function threshold, which is a measure of how well an AI model performs on given inputs.

Here's how it would work:

1. A machine learning engineer submits a deep learning model implementation to the blockchain and requests for training. The value of the loss function of the AI is easy to compute given some input.
2. The node providers see the assignment and begin training the models.
3. Suppose a node provider has superior hardware and is able to train the AI faster than other miners. Once they finish training, they start broadcasting the weights of the model to other miners, which is the result of a well-trained AI.
4. Other miners receive the weights and compute the loss function and the Delta of the loss function. If both values are below some threshold that the machine learning engineer required, then the AI is considered good enough and can no longer be significantly optimized.
5. If the other node providers agree that the AI is well-trained, then consensus is reached, and the node provider who trained the AI receives tokens as an incentive.

The main idea is to switch from the number guessing game to computing the loss value, while keeping the rest of the process the same. 

However, there are some unclear parts, such as whether validators who verify the value of the loss function should be incentivized. According to the logic of BTC, they should not. Also, not every machine learning engineer may be able to define a robust AI, and it is possible that the loss can never reach the required threshold. For example, the gradient may not be able to go down because the data or model is not good enough.

What do you think about this idea?

-------------------------

cymqqqq | 2023-05-04 06:32:52 UTC | #75

I have a list of questions: 
1. 

[quote="MCLYang, post:74, topic:14418"]
A machine learning engineer submits a deep learning model implementation to the blockchain and requests for training. The value of the loss function of the AI is easy to compute given some input
[/quote]
What do you mean by "submits a deep learning model implementation"? Is that mean the train source code? And if that can implement on IC, the format do you want to store the code on IC? How do you input the model code?

-------------------------

SwayStar123 | 2023-05-04 07:00:29 UTC | #76

I did actually try to train a NN onchain a few months ago here -> https://github.com/SwayStar123/nn
it worked well and fine, except it was a tiny neural network for recognizing hand written digits (trained on the mnist dataset). Since it was a small neural network, training it on the cpu wasnt a big deal, but itd be impossible to train stable diffusion type models on chain, you need GPUs for that, and yeah since you would need to come to consensus for it, seems like a waste of resources.

Querying for inference seems like an excellent usecase however, since only one node needs to do the calculations for that, and the required hardware is much lower during inference time

-------------------------

MCLYang | 2023-05-04 07:06:51 UTC | #77

Yes, it may be necessary to provide the training source code along with the training data, and on IC it might need to be implemented using Rust or Motoko. Solidity would not work because it does not even have a float data type.

-------------------------

cymqqqq | 2023-05-04 07:20:44 UTC | #78

Great job!
In your code, it seems like for every training step we need to perform consensus. And you store the weights in canisters, right?

-------------------------

SwayStar123 | 2023-05-04 07:27:51 UTC | #79

Yes to both the questions

-------------------------

Maxfinity | 2023-05-04 16:00:08 UTC | #80

[quote="SwayStar123, post:76, topic:14418"]
it worked well and fine, except it was a tiny neural network for recognizing hand written digits (trained on the mnist dataset). Since it was a small neural network, training it on the cpu wasnt a big deal, but itd be impossible to train stable diffusion type models on chain, you need GPUs for that, and yeah since you would need to come to consensus for it, seems like a waste of resources.
[/quote]

I really like the idea of taking market-making to the next level, using AI on the blockchain. There is still a question of which liquidity pools to add liquidity to, how to add more positions when the liquidity price changes, e.g. for Uniswap V3. Imagine having an AI agent, perhaps using outcalls to query for results, that would then be able to add liquidity in a dynamic fashion for users. Then connect this to the #Bitfinity EVM.

-------------------------

junkei-okinawa | 2023-05-07 02:28:40 UTC | #82

Nice effort! I tried to make it possible to execute canister's train and predict using PyScript.
It seems that there are still many challenges, but I am looking forward to the future.
[nn with pyscript](https://github.com/junkei-okinawa/nn/tree/feature_pyscript)

-------------------------

mnl | 2023-05-25 12:35:56 UTC | #83

I've spent a moment gluing the default dfx project together with the example provided by the `burn` project: https://github.com/burn-rs/burn/tree/main/examples/mnist-inference-web :
- code: github.com/smallstepman/ic-mnist
- deployed canister: https://jsi2g-jyaaa-aaaam-abnia-cai.icp0.io/

-------------------------

Gamris | 2023-06-21 08:59:53 UTC | #85

I found this [LLaMA 7b for Rust Implementation](https://github.com/coreylowman/llama-dfdx) using dfdx.

-------------------------

cymqqqq | 2023-06-21 09:28:37 UTC | #86

It consumes a lot of cycles for uploading the model to IC.

-------------------------

mnl | 2023-06-21 13:23:32 UTC | #87

> *LLaMA* -*7b* takes ~12 *GB*

even with 4bit quantization, it probably wouldn't fit in the canister's heap memory

our current best bet is using sth like flan-t5
```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig

line = "answer the following question reasoning step by step: joe has 22 apples, he gave away half and ate 6 apples how many apples does he have"
# model_name = "google/flan-t5-small" # ~850 Mb peak RAM usage, model is ~350 Mb
model_name = "google/flan-t5-base"    # ~2.1 Gb peak RAM usage, model is ~900 Mb

config = GenerationConfig(max_new_tokens=200)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

tokens = tokenizer(line, return_tensors="pt")
outputs = model.generate(**tokens, generation_config=config)

print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```
`He gave away 22 / 2 = 8 apples. He ate 6 apples so he has 8 + 6 = 10 apples. Therefore, the final answer is 10.
`

-------------------------

yvonneanne | 2023-06-21 14:15:29 UTC | #88

[quote="mnl, post:87, topic:14418"]
He gave away 22 / 2 = 8 apples. He ate 6 apples so he has 8 + 6 = 10 apples. Therefore, the final answer is 10.
[/quote]

Absolutely love the convincing reasoning :smile:

-------------------------

mnl | 2023-06-21 14:28:36 UTC | #89

For those who are interested...

## Here's roughly what would need to happen to get the above to work on IC:

LLMs work on a token-by-token basis. To simplify things, think of a token as if it were a syllable. So when you look at this sentence:
```
He gave away 22 / 2 = 8 apples. He ate 6 apples so he has 8 + 6 = 10 apples. Therefore, the final answer is 10.
```
you can imagine splitting the above into syllables. 

The above python code would need to get reimplemented using `burn` and `cdk-rs`:
1. implement two cache structures in the canister:
  a. a cache for the sentence that's currently getting generated
  b. a special cache for embeddings
2. build an update method `fn generate` in a canister that generates one syllable during one update call
3. during the update call, a newly generated "syllable" is appended to a list of generated syllables (stored in `a.` cache)
4. at the very end of that update call, the canister calls itself (calls the same `fn generate` update method) until it generates the stop token (special token used to signify the end of a generated sequence)

It's not trivial work, but also not super complex. It definitely is not going to be useful by any stretch of the imagination (too slow (the above sentence is ~30 syllables * ~4sec = ~2min), and output quality will be terrible cause we can't fit bigger models for now), but it still is a stepping stone.

We can always pick a different idea, but we'll be limited by hardware no matter what we pick because all we have is CPUs without access to vectorized operations (no CUDA, no SIMD) and 4gb or RAM (canister heap). There is only so much stuff that can fit inside 4 GB of RAM, and without vectorized operations, all math for neural nets is bound to be slow. In order to get the quality of bigger models, we would need to figure out how to spread the computation among multiple canisters, but that's a story for another day

-------------------------

yvonneanne | 2023-06-24 15:03:44 UTC | #90

[quote="mnl, post:87, topic:14418"]
> *LLaMA* -*7b* takes ~12 *GB*

even with 4bit quantization, it probably wouldn’t fit in the canister’s heap memory
[/quote]

Is there something that prevents us from using the canister's 48 GB stable memory instead of squeezing everything into the heap?

-------------------------

Icdev2dev | 2023-06-24 15:51:44 UTC | #91

I would also think that you would also need to reimplement how random numbers are generated in that system because of consensus.

 If the above is true, then this is non-trvial because it would likely be a patch in a open-source system that you'd need to carry forward(unless you get that open source system to accept your patch).

-------------------------

mnl | 2023-06-26 15:36:27 UTC | #92

Good point, I haven't thought about it...

From what I understand, stable memory can imitate runtime memory, but in reality, at runtime, we're still limited by 32bit address space which is 4gb (until [this](https://github.com/WebAssembly/memory64)), therefore at any given time, the runtime memory can only hold 4gb worth of data for calculations. Assuming we'd develop an ML library for the inference that natively uses sth like https://crates.io/crates/ic-stable-memory and we would code things up such that instead of loading the whole model at once into memory (as it's currently being done in ML world), this library would load weights and makes calculations layer by layer: loads a layer, does matrix multiplication, clears the runtime memory, and loads another layer and so on and so forth until it provides the output... then yes, it could work I think. 

Tho, I think it would be super duper slow, because if we look at it from a hardware perspective, we get sth like this:
```python
# pseudo code

model_archtecture = nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)
weights_and_biases = LazyLoadFromStableMemory('model.bin')
cache__last_layer_result = initialize_from_input()

for layer in model_archtecture:
    w = load_weights_for_layer(layer, weights_and_biases)
    cache__last_layer_result = w * cache__last_layer_result
```
we would be reading from SSD and putting data into the node's RAM every time a call to `load_weights_for_layer(layer, weights_and_biases)` is made. The size of `w` + `cache__last_layer_result` could not exceed 4gb at any given time.
> The original size of the 65 billion parameter LLaMA model (often referred to as the 65B model) is 120 gigabytes. However, when quantized to 4-bit, the size is reduced to 38.5 gigabytes​[1](https://www.reddit.com/r/LocalLLaMA/wiki/models/)​.

Therefore if we take an example SSD listed in node requirements https://wiki.internetcomputer.org/wiki/Node_Provider_Machine_Hardware_Guide : `6.4 TB NVMe Kioxia SSD 3D-NAND TLC U.3 (Kioxia CM6-V)` we can see it can read `up to 3,500MB/s`, therefore just the loading and unloading parts of the model's weights and biases would take the total of `38500 MB / 3500 MB/s = 11 seconds`

-------------------------

mnl | 2023-06-26 23:03:12 UTC | #93

correct, tho I think this part is not as difficult; we could try talking with some maintainers of Rust ML frameworks and ask them to modify their code such that we can provide a custom implementation of RNG (e.g. based on some trait)

edit: nvm; achievable today with burn https://docs.rs/burn/latest/burn/tensor/backend/trait.Backend.html#tymethod.seed

-------------------------

apotheosis | 2023-06-27 02:01:29 UTC | #94

This is an amazing thread. Kinic Developer Organization is looking into this tech.
There are models that run in constrained environments like phones. I think we can push the edge of what is possible here.


https://www.semianalysis.com/p/google-we-have-no-moat-and-neither

-------------------------

cyberowl | 2023-06-27 11:38:07 UTC | #95

Do you know if we have a small model that can be deployed at the moment that could utilize the 4gb?

-------------------------

mnl | 2023-06-27 11:42:10 UTC | #96

https://huggingface.co/google/flan-t5-small would fit for sure

-------------------------

domwoe | 2023-06-27 12:28:52 UTC | #97

There's essentially no difference between heap and stable memory. They are backed by the same hardware. The only difference is, that currently stable memory needs to be accessed via system API, i.e. there's a context switch from WASM to Rust which adds overhead. Maybe @abk can give a bit more context and what upcoming work on multiple memories and wasm64 bring.

-------------------------

abk | 2023-06-27 12:39:55 UTC | #98

Yes this is correct. [Wasm-native Stable Memory](https://forum.dfinity.org/t/proposal-wasm-native-stable-memory/15966) has recently been approved so this allows us to avoid the API overhead in many cases, but stable memory is still not as fast as the main memory.

One reason is that stable memory uses a 64-bit address space which means we need to insert bounds checks around every access. A second difference is that we need to track which pages have been accessed and limit the total number of accessed pages (if a canister could touch all of stable memory during an execution, then a few canisters running in parallel could use up the entire replica's memory).

These checks can both be skipped on regular memory (because the address space is 32-bits and the total size is only 4 GiB), but if we used the Wasm [memory64 proposal](https://github.com/WebAssembly/memory64) to support a larger main memory then we would need to add all these checks for regular memory operations.

-------------------------

mnl | 2023-06-29 10:28:00 UTC | #99

question to you and @abk 

Let's say I load an array of u8 integers that's total size is 48Gb into stable memory, and write a for loop that will increment each u8 by 1. How ~many times will the node access the SSD in order to transfer the data into RAM? The way I see things, it will access SSD at least `48Gb / 4Gb = 12` times

-------------------------

Gamris | 2023-06-28 17:30:13 UTC | #100

For inferencing proof of concept, [TinyStories](https://arxiv.org/abs/2305.07759) is the smallest model I can think of.

Here's a link to the datasets and the sub 10M parameter models. https://huggingface.co/datasets/roneneldan/TinyStories

But if separate AI compute units are coming to the IC ([as mentioned by Dom in this interview](https://youtu.be/5l_qOEmPnUo?t=415)), would an inferencing engine like [vLLM](https://github.com/vllm-project/vllm) be possible if the AI compute layers do not need to pass through consensus?

-------------------------

abk | 2023-06-29 14:50:02 UTC | #101

Where are you getting the `/ 4GB` from? If you're sequentially copying 1 byte from stable memory to main memory at a time the OS should be pretty good at prefetching the needed memory from disk, so I'd imagine we'd get a pretty high throughput.

-------------------------

lastmjs | 2023-06-29 15:36:07 UTC | #102

FYI at Demergent Labs we're working on [Kybra](https://github.com/demergent-labs/kybra), a Python CDK for the Internet Computer. We already have broad support for the Python language and a limited portion of the stdlib, we'll soon be releasing a much more capable version of the stdlib. The end goal is for Kybra to support C extensions and the C API, which would allow data science and machine learning. This will probably be very difficult to achieve, but I'm hopeful still.

The underlying computational environment of the IC will need a lot of improvement/change to support the kind of computational workload required from the AI world though, in my assessment.

-------------------------

Gamris | 2023-09-06 02:04:53 UTC | #104

Has anyone tried TinyLlama yet? 1.1B parameters, Llama2 architecture and tokenizer, trained on 3 trillion tokens.

Current checkpoint available: https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b
GitHub: https://github.com/jzhang38/TinyLlama

Since the 4-bit quantized model weights of TinyLlama consumes only ~550MB RAM, I'd imagine the Llama.cpp 4-bit quantized version would run nicely inside a canister.

-------------------------

icpp | 2023-09-06 04:39:18 UTC | #105

@Gamris ,
Actively working on it!

- see [icpp-llm](https://github.com/icppWorld/icpp-llm) allows you to run the llama2.c model in a canister. I tested it on the TinyStories model for now, but working on larger models. 
- see also [this forum post](https://forum.dfinity.org/t/llama2-c-llm-running-in-a-canister/21991/8?u=icpp)

Is this what you're looking for?

-------------------------

