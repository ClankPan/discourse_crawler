timo | 2023-10-23 11:44:13 UTC | #1

Today we are announcing the HPL project (high performance ledger). The goal is to implement a ledger on the IC that can receive and process 10,000 transactions per second which are initiated by different users and submitted in individual ingress messages.

To achieve this we have developed a multi-canister architecture for the ledger which operates multiple aggregators deployed on different subnets and a single ledger on another subnet. With every heartbeat, the aggregators forward transaction requests in batches to the ledger where they get processed.

We publish here https://github.com/research-ag/hpl-io :
* did file of the aggregator with comments
* did file of the ledger with comments
* README: a high-level overview of the goals and characteristics of the ledger
* README: description of how external clients (frontends, wallets) submit transactions and track progress by querying both aggregator and ledger

**We would be grateful for review, comments and questions.**

Please note that comments in the .did files and the README complement each other. We currently do not have a single document explaining everything.

The ledger and aggregators have been implemented and are deployed on 12 subnets, one for the ledger and 11 for aggregators.

To play around with the live deployment we have a demo frontend which is rudimentary but useful and much better than the raw candid UI. It is deployed here: https://debug.hpl.live/. Please see the comments in the .did files for an explanation of the functions that are exposed in this demo. 

A first experience shows that the latency from clicking "Send" on a transfer to the final confirmation of processing in the ledger is around 6-8 seconds most of the time. Sometimes it can be less, sometimes more. The latency is due to the extra inter-canister and, cross-subnet hop that is required.

We collect extensive metrics and provide a Grafana dashboard with various panels showing:
* Ledger:
  - transaction rate (tps)
  - batch rate
  - number of registered accounts, owners, assets
  - number of processed transactions
  - heap memory
* Aggregators:
  - high watermark of queue size
  - number of concurrent batches in flight
  - batch return time 

The dashboard can be seen here: http://dashboard.hpl.live/

For example, it can be seen from the dashboard that most of the time there are 5 batches concurrently in flight from the same aggregator. But sometimes this number can peak to several times more.

It also shows that most of the time the response to a batch comes back after 6-7 seconds. But this number can also spike to several times more.

There are various other existing pieces of this project that are not yet ready to be published (but publication will follow):
* Motoko implementation of the aggregator
* Motoko implementation of the ledger
* client library (typescript) for transaction submission
* demo frontend source code

And of course there are many, many more pieces left for which work is either in progress or hasn't started yet.

Have fun with it!

And again, we would be grateful for review and comments on the published interface.

-------------------------

sea-snake | 2023-10-23 09:59:05 UTC | #2

[quote="timo, post:1, topic:23951"]
With every heartbeat, the aggregators forward transaction requests in batches to the ledger where they get processed.
[/quote]

Batching multiple requests into a single request is a rather cool approach, I assume to get this throughput you'd still need to deploy on multiple subnets to not run into the message throughput limit of a specific subnet.

Edit: I see now in the second paragraph that it's indeed multi subnet.

-------------------------

senior.joinu | 2023-10-23 09:59:06 UTC | #3

Great job! I would love to dive into the implementation once it's open source.

[quote="timo, post:1, topic:23951"]
We publish here [https://github.com/research-ag/hpl-io: ](https://github.com/research-ag/hpl-io:)
[/quote]
Please fix the link - it has an extra colon at the end.

-------------------------

stefan.schneider | 2023-10-23 10:57:05 UTC | #4

That's a really cool project.
Is this ledger intended for any particular token, or is it just a general library for now?

Also, how does this effort relate to standardization efforts (ICRC-1/ICRC-2)?

-------------------------

timo | 2023-10-23 11:51:02 UTC | #5

[quote="sea-snake, post:2, topic:23951"]
multiple subnets to not run into the message throughput limit of a specific subnet.
[/quote]

Yes, ingress messages per subnet is the bottleneck. With the 11 deployed aggregators I suspect we can get to 5k tps. We can try that out in the next couple of days.

-------------------------

timo | 2023-10-23 11:57:38 UTC | #6

[quote="stefan.schneider, post:4, topic:23951"]
Is this ledger intended for any particular token, or is it just a general library for now?

Also, how does this effort relate to standardization efforts (ICRC-1/ICRC-2)?
[/quote]

It’s a multi token ledger. Anyone can create a new token on it with a canister call.

It cannot be ICRC-1 compatible for various reasons (explained in the README).

It has a different, more general concept of „virtual accounts“ to achieve the functionality of ICRC-2 and more (also see README).

-------------------------

infu | 2023-10-23 13:58:17 UTC | #7

What was the reason behind virtual accounts? You wanted to make it more restrictive or it has a scalability purpose. I guess if A agrees to accept from B the virtual account is in B's canister making the transaction happen without cross-canister calls.
The restriction may be good for NFTs where you don't want to receive NFT ads in your wallet/gallery.

-------------------------

domwoe | 2023-10-23 16:36:55 UTC | #8

Hi @timo,

This would be a great topic for the [Scalability & Performance WG](https://forum.dfinity.org/t/technical-working-group-scalability-performance/14265). The next session will be Thursday, November 16th, 5:30 pm CET. Would you be able to present there?

(cc @abk )

-------------------------

timo | 2023-10-23 17:00:21 UTC | #9

No, the restriction of not allowing unsolicited deposits is not for scalability reasons. It is just there because I think allowing such deposits is bad because it opens up attacks (spam, tainted funds, unwanted airdrops and taxes, transaction history poisoning, etc.).

-------------------------

Samer | 2023-10-24 19:01:30 UTC | #10

Most interesting DeFi development on the IC since ICRC1!

-------------------------

JaMarco | 2023-10-24 20:56:38 UTC | #11

Why is there 10k tps (20 aggregator subnet) limit? Wouldn't you be able to deploy aggregators on an unlimited amount of subnets? Is it because of the tps limit of the single subnet holding the ledger? If so would you not be able to shard the ledger on different subnets to scale further?

-------------------------

hokosugi | 2023-10-24 21:29:42 UTC | #12

The 20 subnets (10000 tps) is a current assumption, and more will probably be possible in the future. My understanding is that since Ledger is single, the number of processes is bounded by 970/s per one subnet, so the number of processes should vary greatly depending on how much batch processing can be done.
I assume that there will be delays in finality, etc. when HPL exceeds 10000 tps, but what are the tradeoffs in seeking scale?

-------------------------

JaMarco | 2023-10-24 21:35:37 UTC | #13

[quote="hokosugi, post:12, topic:23951"]
I assume that there will be delays in finality, etc. when HPL exceeds 10000 tps, but what are the tradeoffs in seeking scale?
[/quote]
If we want to scale this past 10k tps I guess the finality delays would be inevitable. But once you shard and load balance the ledger the tps should be unbounded then right?

-------------------------

timo | 2023-10-25 04:42:05 UTC | #14

[quote="JaMarco, post:11, topic:23951"]
Why is there 10k tps (20 aggregator subnet) limit? Wouldn’t you be able to deploy aggregators on an unlimited amount of subnets?
[/quote]

The 10k was a _goal_ not a limit. And as a goal it is somewhat arbitrary. It represents a certain order of magnitude that both non-trivial to reach and has practical relevance because it is said that VISAs global transaction throughput is in that order (smaller on average, higher on peak).

To reach the goal 20 subnets with 500 tps ingress throughput each are needed. Currently I cannot even deploy on 20 subnets because when you create a new canister on a random subnet then you end up on one out of only 11 subnets. If someone gives me canister ids on other subnets then I can deploy aggregators there as well. 


[quote="JaMarco, post:11, topic:23951"]
Is it because of the tps limit of the single subnet holding the ledger? If so would you not be able to shard the ledger on different subnets to scale further?
[/quote]

The next limit after horizontally scaling the number of aggregators to 20 and beyond is the 2MB block size limit of the subnet that hosts the single ledger. If a transaction occupies 100 bytes that would create a limit of 20k tps. But we can certainly introduce some compression techniques by shortening reused principals to increase that number.

The next limit after that would be processing time in the single ledger. The code uses around 40k wasm instructions per transaction processed. At an execution round limit of 5 billion instructions that means 125k transactions per round can be processed. But this number has to be taken with a grain of salt. Not all wasm instructions are equal in terms of real-world time. Pushing to actually using 5B instructions per round could significantly increase the round time (from the ~1s it is at lower loads). We haven’t done any experiments measuring real-world time. Moreover, the 40k instructions per transaction were measured with relatively empty data structures. That may increase as well. 

In summary, what these calculations show is that after applying some margins, doing low hanging fruit optimizations, and horizontally scaling the aggregators, 20k tps should be possible. All blockchains combined don’t have that kind of demand. And it’s close to VISAs global peak time demand. Therefore I don’t see a need to shard the ledger. But as you say, sharing the ledger would also be possible. I just don’t see the need at the moment that would justify the added complexity.

What needs to be sharded though is the transaction history archive and indexing canisters. We don’t currently have an archive canister. 10k tps is in the order of 1b per day. The storage would fill up very quickly. If anyone is interested in working on the archive then please let me know! Rust or Motoko, the archive can be in any language.

-------------------------

infu | 2023-10-25 08:03:44 UTC | #15

I like how radically different this is and how it raises interesting questions. Perhaps it will be the next-gen ledger. Anvil is similarly multi-canister multi-token (both NFT and FT) and allows users to create tokens (not in the dapp publicly). Similarities end there. I think the added complexity to clients both on and off-chain hindered adoption and also made me want to change the architecture to something different that didn't require complicated client libraries. Being able to call `r2pvs-tyaaa-aaaar-ajcwq-cai`.`myFunc(...)` is just too good and simple to move away from. 

A btw question: Is it possible to make HPL DoS-resistant? The aggregators accept transactions and store them, but they don't know if these transactions come from accounts with any tokens in them, so a fee can't be taken until the ledger processes them? This will allow someone to flood the aggregators with fake transactions HPL has to process for free?

I wonder what are the benefits of HPL versus this:
We have the same icrc1 compatible ledgers, but imagine there are 20 ledger canisters in different subnets. Each one has its own separate memory and account->balances. When you transfer from one account in ledger A to another account in ledger A - the transaction gets executed without cross-canister calls. When you transfer from account in ledger A to account in ledger B the ledgers securely communicate to execute the transaction.
Each account resolves to a specific ledger - This could be done with a hashmap of the account id, so without any additional queries a function can calculate which ledger is supposed to host a particular account.
The throughput should be ~20x bigger than what one ledger can do. Similar to HPL?
But how do we make this work without requiring additional client libraries which first fetch a routing table, and hashing account IDs to finally connect to the target canister?

I think the best solution would be to have 'routers' which can take the position of another canister, but are not an ordinary canister and are not inside any subnet (or perhaps are in all of them?). They could be a set of functions with the same interface inputs as the target function. They can use the inputs for calculations and then return a canister id. The system then forwards the call to that canister id. Not sure where the router's place is and how it will become secure, but if it's possible to find it a place it will solve a lot of scalability problems without changing everything. 
There will be no changes needed in any canister like dexes or frontends/wallets. We can still use icrc1 and anyone can scale from one canister ledger once it's not enough - to a multi-subnet ledger by replacing the canister with a router and installing more ledgers. Those ledgers will also be the ledgers we use right now, except they have to add a few lines that handle transfers to accounts hosted in other ledgers.
This will also allow other types of canisters to scale, other services, databases, or asset canisters.

-------------------------

timo | 2023-10-25 09:35:05 UTC | #16

[quote="Samer, post:10, topic:23951"]
interesting DeFi development
[/quote]

The relevance for DeFi could be that this is a multi-token ledger.

The two things go hand in hand: with high throughput you make room for multiple tokens, and conversely, if you have a lot of room then how do you fill it other than with multiple tokens?

The benefits of a multi-token ledger that I see for DeFi are:
* atomic swaps between multiple tokens (especially relevant on the asynchronous environment that is the IC)
* unified integrations for wallets, hardware wallets, DEXs, CEXs, etc.
* reduced friction for creating new tokens
* reduced friction for wrapping and bridging (inside IC or cross-chain)

-------------------------

timo | 2023-10-25 09:35:59 UTC | #17

[quote="hokosugi, post:12, topic:23951"]
My understanding is that since Ledger is single, the number of processes is bounded by 970/s per one subnet
[/quote]

What do you mean by processes and where does the number 970/s come from?

-------------------------

timo | 2023-10-25 10:47:46 UTC | #18

[quote="infu, post:15, topic:23951"]
question: Is it possible to make HPL DoS-resistant?
[/quote]

Yes, that's an interesting question. My plan was to create a "credit system". It means that principals hold credits in the ledger and those credits are communicated from the ledger to the aggregators. So the aggregators mirror the credit balance, but with a delay of course. Say I have 1000 credits and there are 10 aggregators. Then each aggregator will allow me to submit 100 concurrent transaction requests. So the space I can occupy in the queue is limited. It is like a quota that is proportional to my credit. When the aggregator knows that a transaction has been processed by the ledger it clears it from the queue, freeing the occupying space. That is happening now. With the credit system it would then also free my quota and allow me to submit more concurrent transaction requests.

There are various variations of this. Credit can refer to an absolute number of transactions, i.e. credit counts down and has to be replenished, or it can refer to a rate, i.e. it doesn't count down and just defines the quota that can be occupied at any point in time.

There are also variations on how credit can be obtained. Either it can be bought outright or it can be earned over time by past successful transactions that lead to a fee payment in the ledger.

There can be a certain quota for "uncredited" transactions that can be used by new users who do not have any credit yet or don't want to bother with it. Then the most a DoS attacker can do is exhaust that quota, i.e. interrupt the service for users who don't have credit, but not for the other ones.

-------------------------

hokosugi | 2023-10-25 11:24:22 UTC | #19

Here is the answer to my past question. My poor understanding is that there is a consensus limit to the tx process.
The finality time of 8 seconds seems a bit long. Is there any advantage over this?

https://forum.dfinity.org/t/icp-lab-storage-scalability-summaries/20242/10

-------------------------

NS01 | 2023-10-26 07:10:04 UTC | #20

Really interesting post. I'll definitely be following along. 

Do you have any thoughts on history (ICRC-3) type functions and how these would work. For example, from the perspective of the 221Bravo dev, how can tokens histories on HPL be searched and how can we automate the process of 'finding' new tokens that are added. 

Ideally anyone tracking the ledger shouldn't have to change structs or enums to pickup new variants of tokens. 

Side thought - I'm not completely sold on the idea of a variable fee depending on transaction size. This seems like it's getting into tokenomics a bit. From a technological standpoint there isn't any cost difference between processing a 1million tx and a 0.0001 tx - why should the 1m pay more?

-------------------------

timo | 2023-10-26 13:11:53 UTC | #21

[quote="NS01, post:20, topic:23951"]
Do you have any thoughts on history (ICRC-3) type functions and how these would work. For example, from the perspective of the 221Bravo dev, how can tokens histories on HPL be searched and how can we automate the process of ‘finding’ new tokens that are added.
[/quote]

No, that is an open topic. I know that we need one or more archive canister and one or more index canisters to allow search for various keys. But work on that hasn't started.

New tokens can be found by querying `ftInfo` for the whole range of asset ids. But the ledger is not a source for what those tokens actually are. You can see the controller of the token and a self-description which of course isn't trust worthy. The ledger does not store the token's symbol. That has to come from somewhere else from a trusted source.

[quote="NS01, post:20, topic:23951"]
Ideally anyone tracking the ledger shouldn’t have to change structs or enums to pickup new variants of tokens.
[/quote]

You mean if a new token type is introduced other than `ft`? For example, a type for NFTs?

[quote="NS01, post:20, topic:23951"]
Side thought - I’m not completely sold on the idea of a variable fee depending on transaction size. This seems like it’s getting into tokenomics a bit. From a technological standpoint there isn’t any cost difference between processing a 1million tx and a 0.0001 tx - why should the 1m pay more?
[/quote]

Fee modes can be changed and new ones introduced later, including one that works with a separate fee token so you don't have to pay in the transferred asset itself (in which case the cost will naturally be independent of the transferred amount and would reflect the actual raw processing cost). But if we pay a fee denominated in the transferred asset itself then my question would be "why not"? Why not make it proportional to the amount? Why not let the ledger make some profit on fees that go beyond the actual cost? Or, put differently, why not take from large transactions and subsidise small ones if it is in an order that large transactions won't care about?

-------------------------

timo | 2023-10-26 13:13:40 UTC | #22

We just did a load test with around 4,000 tps distributed over all 11 aggregators. It can be seen on the dashboard http://dashboard.hpl.live/

![Screenshot 2023-10-26 at 14.44.13|690x196](upload://3OZNQjUVVbggg5MxicW1ZE8Rz2O.png)

I manually submitted some transactions through the frontend while the test was ongoing to check for impact on latency. The time to completion was often unchanged, the normal 6-7s, but I also saw some outliers that took 20s. I guess that's the nature of it when cross-subnet traffic gets busy.

Also visible on the dashboard:

![Screenshot 2023-10-26 at 15.12.56|690x254](upload://nLlnwMbDugAcZsFQAhwtbWMA4O0.png)

-------------------------

NS01 | 2023-10-27 05:07:56 UTC | #23

Thanks for your detailed reply! I'll certainly be following along and giving it a go when I've got a couple of minutes. 

On the topic of fee/ tokenomics - I feel like this sometimes gets in the way of cool tech/ ideas moving the topic away from the core engineering. I've seen too many crypto projects make a mess of cool stuff by getting the tokenomics/ fees wrong (ICP/ ETH are perhaps both good examples). I am interested in this area and I think sustainable ledgers are needed. I think a flat fee (adaptive if needed) based on cycle costs and small % profit would be far better than a fee based on tx value. It seems a bit like income tax and nobody likes that!   

Well done on the 4k TPS test - that's awesome :slight_smile:

-------------------------

timo | 2023-10-27 15:34:14 UTC | #24

[quote="hokosugi, post:19, topic:23951"]
My poor understanding is that there is a consensus limit to the tx process.
[/quote]

Yes, that is exactly the limit that I meant by "ingress messages per subnet is the bottleneck". I was conservatively calculating with 400-500 tps as the limit here. The 970/s was measured by DFINITY in a test environment. I was going with half of that which is also something that I could replicate myself on mainnet with reasonable effort and without having to push it.

There is also a limit imposed by message executions per canister which comes from the time it takes to load the wasm module. That might be somewhere around 700/s. If the ingress limit is indeed as high as 970/s then it would make sense to deploy two aggregators per subnet. Because then we remove the number of wasm invocations as the limit by splitting the ingress message over two canisters.

Anyway, for the time being I will work with a conservative estimate of 400-500/s per subnet and leave tricks to squeeze more out of it for later.

[quote="hokosugi, post:19, topic:23951"]
The finality time of 8 seconds seems a bit long. Is there any advantage over this?
[/quote]

What do you mean by "Is there any advantage over this"? There is simply no alternative. That's what it takes to make canister calls with one-hop cross-subnet calls in the back. That finality is still faster than any competing blockchain. The latency can also be hidden in the frontend. Because the moment a transfer is accepted by an aggregator it is guaranteed to be _processed_ by the ledger. It can still fail because of insufficient funds, but for certain transfers the frontend may know that the funds are there, so it can already show success to the user after the submission to the aggregator is made (after ~3s).

-------------------------

hokosugi | 2023-10-27 21:54:06 UTC | #25

> That finality is still faster than any competing blockchain. 

What exactly is a competing blockchain? I thought finality 8 seconds was in the slow category, Sorana's is about 1 second, Avalanche's about 2 seconds.
My intention is to ask what are the advantages of HPL that would overcome the slow finality. The following immediately come to mind.Is there anything else?

* Unlimited scale is possible.

* Multi-tokens

* Ability to do atomic swaps with transaction execution in Ledger Canister

-------------------------

Sormarler | 2023-10-28 04:20:53 UTC | #26

Transaction finality on Solana is 12 seconds. Avalanche is about 2 seconds.

-------------------------

Sormarler | 2023-10-28 05:06:45 UTC | #27

So was it you guys burning all these cycles doing transactions?

-------------------------

timo | 2023-10-30 08:55:19 UTC | #28

We sustained approx. 5k tps for 6h which burned cycles at a rate of approx. 900 TC der day or 10B cycles per second. That is about 2M cycles per transaction.

![Screenshot 2023-10-28 at 08.24.12|690x161](upload://d7SDjJ7GV98bUQHX9in4vnWRPMh.png)

The IC public dashboard currently shows 28B cycles/s after we stopped this test. That is not from us. During our test the IC dashboard peaked at 40B cycles/s which included our ~10B cycles/s.

According to [this table](https://internetcomputer.org/docs/current/developer-docs/gas-cost), ingress message reception costs 1.2M cycles plus 2k cycles per byte in the message. Not sure which are bytes are counted though (payload only or request envelope). Then x-net byte transmission costs another 1k cycles per byte. So if an ingress message is 200 bytes and the forwarded part to the ledger is 100 bytes then that would make for a base cost of 1.2M + 0.4M + 0.1 M = 1.7M per transaction without counting cost of instructions. That seems to match the observed cost of approx. 2M per transaction.

UPDATE: More precisely we have for the cost in cycles:
* 1.2 M ingress message reception (+ bytes)
* 0.59 M ingress message execution (+ instructions)

We measured the aggregator instructions per submission at 20k instructions, which cost 8k cycles. So that makes 1.2M + 0.59M + 0.008M = ~1.8M cycles. Which leaves ~0.2M cycles for the bytes.

-------------------------

hokosugi | 2023-10-28 07:52:35 UTC | #29

Thanks for pointing that out. I checked several sites and found that the finality time is indicated as ~1s and 12s. It is not a Solana document, but there seems to be an "optimistic confirmation" mechanism on vasa's site that can shorten the finality.

https://usa.visa.com/solutions/crypto/deep-dive-on-solana.html

-------------------------

timo | 2023-10-28 08:18:57 UTC | #30

[quote="hokosugi, post:25, topic:23951"]
Avalanche’s about 2 seconds.
[/quote]

Avalanche has only probabilistic finality. It depends on the number of confirmations. 

This is from [Robinhood](https://robinhood.com/us/en/support/articles/crypto-transfers/):



> * **Avalanche** (AVAX): 20 confirmations (approximately 20 seconds)

-------------------------

Sormarler | 2023-10-29 01:11:46 UTC | #31

Thank you. I did not know that. Another thing that was mentioned is using the front end to improve the user experience by making things feel more snappy. Many projects on ICP do not take advantage of that. There is no need for the user to watch itTransaction load up for 8 seconds. Optimistically executed and then if it feels have an error pop up.

-------------------------

timo | 2023-10-29 09:02:37 UTC | #32

[quote="hokosugi, post:25, topic:23951"]
My intention is to ask what are the advantages of HPL that would overcome the slow finality. The following immediately come to mind.Is there anything else?

* Unlimited scale is possible.
* Multi-tokens
* Ability to do atomic swaps with transaction execution in Ledger Canister
[/quote]

Yes, that sums it up pretty well. We accept one additional hop which seems to be adding about 4 seconds of latency in exchange for the ability to scale horizontally. This in turn unlocks the ability to host multiple tokens without risk of one "noisy" token degrading the user experience of other tokens. Hosting multiple tokens unlocks atomic swaps.

-------------------------

NS01 | 2023-10-29 10:12:35 UTC | #33

I knew the cycle spike would be you testing 😅! So fees on the ledger MUST be set to cover at least 2M in cycles - there is no way that any dev other than Dfinity could sustain 900TC per day.  The cycle burn is mind blowing.. but I suppose that at 5k tps the adoption would be mind blowing as well 🤯

EDIT - It really gives some interesting problems for 221Bravo trying to index this volume of transactions. We'd have to roll out an equally horizontal setup of index canisters working in 'teams'.

EDIT EDIT - Rough maths.. 5k tps is 300,000 tx per minute. If 10,000 txs ~ 2mb (seems about right for ICRC ledgers) This would require 30 calls a minute to be requested from HPL if block size doesn't change. That's not too bad however when taking into account processing time (30 seconds+) and time it takes for messages to travel - a call every 2 second becomes a bit more tricky. 

None of this is an issue with HPL and is stuff for the 221Bravo team to think about... but I'm curious if you've got any thoughts on blockchain explorers and how they would integrate/ keep up with HPL. Say there are a number of blockchain explorers all querying hundreds of thousands of transactions on the ledger (split across 2mb blocks) - would the canister ingress que be a bottle neck?

I suppose getting 100k tps into a ledger is one thing.. but I imagine there could be a requirement to get an equal amount of data back out on the other side - wallets, blockchain explorers etc will all likely be spamming calls to HPL. Do these need to be batched as well?

-------------------------

Berg | 2023-10-29 10:11:39 UTC | #34

How much did the whole test cost in ICP to execute in total?

-------------------------

timo | 2023-10-29 12:07:08 UTC | #35

Yes, what goes in must also be able to come out. But that shouldn't be a problem. If 2 MB per block can come in then 2 MB per block can also come out.

I think that downstream from the ledger must first come an archive canister. It takes in the stream of all ledger transactions from a fire hose, i.e. in large batches. In fact, it must probably be a set of archive canisters because the amount of data is enormous. 10k tps is 0.86 billion per day. That means multiple GB of data every day.

The archive would be part of the HPL which means it would have exclusive access to the fire hose. From the archive onwards the data is then made public. It is provided to indexers and additional mirrors, etc. From the archive onwards the data has to fan out because no single canister can provide all the services.

-------------------------

timo | 2023-10-29 12:08:53 UTC | #36

[quote="Berg, post:34, topic:23951, full:true"]
How much did the whole test cost in ICP to execute in total?
[/quote]

Don't know exactly, ~250 TC is currently ~32 ICP on sonic.

-------------------------

Sormarler | 2023-10-29 20:13:09 UTC | #37

This would allow atomic transactions for tokens built on using this ledger right?

-------------------------

timo | 2023-10-30 07:52:05 UTC | #38

[quote="Sormarler, post:37, topic:23951, full:true"]
This would allow atomic transactions for tokens built on using this ledger right?
[/quote]

Yes.

With a multi-token ledger you can also build a multi-minter. That's a single canister that can wrap all ICRC-1 tokens and mint them on HPL. All that is required to bring an existing ICRC-1 token to HPL is a canister call to the multi-minter that takes the canister id of the token's ledger (the ICRC-1 ledger) as an argument. No need to deploy a new minter every time you want to bridge a new asset to HPL.

-------------------------

timo | 2023-10-31 12:57:15 UTC | #39

There has been some discussion of latency in the thread above. So we decided to run a test which is submitting one transaction every 5s to a random aggregator. There is no other load, so we get the pure latency from the consensus and message routing layers of the IC without any effects from congestion.

![Screenshot 2023-10-31 at 10.28.39|690x329](upload://6sknXXBPxgtIImmDBUYHhwveOMY.png)

We can see the latency, averaged over 10 minutes, starts out somewhere around 7.1 second.

Then we activated a feature designed for improving latency. The problem with heartbeats is that they always run at the _beginning_ of an execution round. Hence, from within a heartbeat you can never forward the transactions that come in in the _same_ execution round. You can only forward the ones that came in in the _previous_ execution round. If the heartbeat were to run at the _end_ of an execution round then that would allow us to forward transactions in the same execution round in which they arrived, reducing overall latency by exactly one execution round. The feature that we activated essentially simulates a heartbeat at the end of each execution round. It does so by using self-calls. Of course it only works under small and medium load, when the execution rounds are not full, because otherwise the self-calls could slip into a different execution round.

We can see the effect in the graph. The latency drops to somewhere around 6.1 seconds. So in summary we can say that the HPL has on average 6s latency under small load.

It should be noted that all aggregators used in this test are indeed on different subnets than the ledger, as would be the case in a deployment at scale. It is of course possible to deploy _one_ aggregator on the _same_ subnet as the ledger. That does not interfere with incoming batches because they live in separate parts of the block space. In each block there are 2MB reserved for cross-subnet traffic (which contains the batches going to the ledger) and another 2MB are reserved for ingress traffic (which contains the transactions going to the aggregator on the same subnet). So the "local" aggregator would not interfere with the ledger. Since the aggregator is "local" the latency then drops from 6s to about 4s. So there is at least room for some transactions at 4s latency which could be made available to everyone at times when the overall load is low (when one aggregator is enough, i.e. when the whole HPL runs under 400 tps) or which could be made available for a higher fee to users who need it.

This image was taken with an aggregator on the same subnet as the ledger. The average is somewhere around 4.2 seconds:
![Screenshot 2023-10-31 at 11.00.54|690x326](upload://9p0ETfd2IeEl2nBcjZAtaPfROGM.png)

-------------------------

rumenov | 2023-11-01 09:30:31 UTC | #40

Hi @timo , I am very excited to see such usage.

[quote="timo, post:5, topic:23951"]
Yes, ingress messages per subnet is the bottleneck
[/quote]

From the networking team side I we can help with the following.

Currently you were hitting the rate limiter on the boundary nodes. Each boundary node allows only 300 ingress messages per second. There is no reason for such conservative limit given the MB/s is actually what matters. Looking at your workload your ingress messages are fairly small. We will increase the limit to 1000 ingress messages per second in the coming days and I will ping the thread when this is done.

Rosti

-------------------------

timo | 2023-11-01 10:30:39 UTC | #41

[quote="rumenov, post:40, topic:23951"]
Currently you were hitting the rate limiter on the boundary nodes. Each boundary node allows only 300 ingress messages per second.
[/quote]

Hi Rosti. Thanks for offering to increase the limit. But I am not currently hitting the rate limit. Each boundary node allows 300 ingress messages per target subnet. My load script distributes the load equally over 10 boundary nodes. That is way below the limit.

Say there are N target subnets (= number of aggregators) and M boundary nodes and I send a total of R tps. Then R/(NM) must be < 300. In my case I have R = 5,000, N = 11, M = 10, so R/(NM) = 45. So I am still very far away from the rate limit.

Yes, if the rate limit was increased then I could simplify my load script and I could send the whole load to one boundary node instead of 10. But it is actually a more realistic simulation of real-world usage if the traffic goes through different boundary nodes. Also, I am not sure if that would help me. It is hard in practice from a networking perspective to get that many requests per second to a single server reliably and at a constant rate. I am afraid that it would result in higher fluctuations in my tps rates. Going through 10 boundary nodes evens out the natural fluctuations in the speed and quality of the network connection.

-------------------------

timo | 2023-11-01 10:41:27 UTC | #42

[quote="rumenov, post:40, topic:23951"]
[quote="timo, post:5, topic:23951"]
Yes, ingress messages per subnet is the bottleneck
[/quote]
[/quote]

This statement refers to a consensus bottleneck which still exists regardless of what the boundary nodes do. This limit comes from how many individual messages can be gossiped per second between the nodes of a subnet. Because they are individual messages there is some limit regardless of how small they are. I believe this limit is < 1,000 but I have not actually tried to push it that far. The most I tried was 450.

-------------------------

rumenov | 2023-11-01 12:13:27 UTC | #43

1. Looking at boundary node metrics there was an increase of 429 responses exactly when you were running your load. That's why I thought you are hitting the boundary node limits. Looking more carefully I do see some 429 responses coming from replicas. More specifically I see ~200k 429 responses starting 10pm on 27th until 6am on 28th.
2. There is no explicit limit on the ingress messages per second we gossip. So the number of messages really depend on bandwidth, latency, finalization rate and if the code actually pipelines/multiplexes messages well.
3. Most likely the replicas were not gossiping fast enough hence the ingress artifact pools got full, hence the 429 but some metrics are inconclusive.

-------------------------

timo | 2023-11-01 12:37:08 UTC | #44

But you have concluded that the 429 are _not_ coming from the boundary nodes? So we are certain that it wasn't a rate limit? Or is that not clear yet?

How large are the ingress artifact pools?

The number of ~200k was for one boundary node that you looked at (if yes which one?) or for all combined?

-------------------------

timo | 2023-11-01 12:44:08 UTC | #45

[quote="timo, post:42, topic:23951"]
This limit comes from how many individual messages can be gossiped per second between the nodes of a subnet.
[/quote]

I will take that statement back. Block making should be independent from gossiping. So even if gossiping completely fails, due to being overloaded or anything else, block making should still work. It just means that ordering isn't fair anymore and the variance in latency goes up significantly. But throughput shouldn't be affected by gossiping. Throughput should indeed only depend on message size. So maybe it is possible to get >1000 ingress messages per second into a subnet, just not in a very fair and responsive way.

-------------------------

memetics | 2023-11-01 13:09:13 UTC | #46

When funded I would like to be an early adopter if the allocation is healthy

-------------------------

rumenov | 2023-11-01 15:52:11 UTC | #47

[quote="timo, post:44, topic:23951"]
But you have concluded that the 429 are *not* coming from the boundary nodes? So we are certain that it wasn’t a rate limit? Or is that not clear yet?
[/quote]

It is the replicas only. In the morning we looked at aggregated logs from the boundary nodes that didn't make the distinction. 

[quote="timo, post:44, topic:23951"]
How large are the ingress artifact pools?
[/quote]

10k number of ingress artifacts in total for [both the validated and unvalidated pools](https://github.com/dfinity/ic/blob/master/rs/artifact_pool/src/ingress_pool.rs#L452-L462). 

[quote="timo, post:44, topic:23951"]
The number of ~200k was for one boundary node that you looked at (if yes which one?) or for all combined?
[/quote]

Those were coming from subnet `o3ow2` (combined across all replicas in that subnet).

-------------------------

hokosugi | 2023-11-01 21:57:27 UTC | #48

Thank you for your response regarding latency.
I would expect enterprise scale transaction processing to be possible if the ledger is scaleable. I don't see any issues compared to the Solana and Layer2 solutions that Visa is testing. What are the advantages over these?


What is the use case for HPL? Are you thinking of adapting it to the enterprise or building a huge ledger system including ckBTC, ckETH, and other ICRC wrapped in HPL?
Also, is it a technical limitation that you can't transfer money to/from other accounts only through virtual accounts? Or is it to solve the disadvantage of not being able to create a common account by creating a separate Principal for Internet Identity authentication?

-------------------------

Sal_Paradise | 2023-11-02 10:08:55 UTC | #49

What? 600k TPS?

![image|690x99](upload://yHCCRF8geXsJbo5lsYHUiNxT6Sf.png)

-------------------------

timo | 2023-11-02 12:25:26 UTC | #50

[quote="Sal_Paradise, post:49, topic:23951"]
What? 600k TPS?
[/quote]

That's only "validation" which means it is inside the execution layer. It doesn't include any networking / consensus. They made a new C++ implementation of the validator and to show how optimised it is they compare the pure validation (aka execution) speed.

But the real bottle neck is always networking which makes the cited number of 600k irrelevant for throughput. It is relevant if validators want to catch up and re-validate the entire history of the chain, which is probably why they optimized it.

For networking / consensus you will see a wide range of throughput numbers thrown around but they are not always comparable. A big differentiator is fairness. For example, if I just go round-robin through a set of block makers in a pre-determined way then the user can directly submit to the next upcoming block maker (or the next N blockmakers as a back-up). I don't need any gossiping of individual transactions between block makers. They can focus on sending entire block proposals, which are essentially large batches of user transactions, instead of individual transactions. But this is susceptible to front-running. If instead you gossip individual transactions between all blockmakers and only determine the next block maker a fraction of a second before the block is made then you have a much harder problem, but are less susceptible to front-running.

Gossiping is expensive and slow and the cost increases significantly with the number of messages. It is a huge difference if I gossip 10k individual messages immediately as they come in, from the one node who received the message from the user to all other nodes, or if I collect them all in one place (a sequencer) and then send all 10k in one batch to other validators. The two problems don't even compare. Throughput with batching is practically unlimited when compared to a protocol based on gossiping individual messages. For the latter 1,000 tps is already an achievement.

That being said, applications are also not always comparable. For a ledger that we are building here fairness of ordering is probably not so important. Because we don't usually double spend our own transactions. Nor would anyone be interested in front-running a payment that I make. But for a DEX it is very important.

-------------------------

timo | 2023-11-02 11:46:41 UTC | #51

[quote="hokosugi, post:48, topic:23951"]
I would expect enterprise scale transaction processing to be possible if the ledger is scaleable. I don’t see any issues compared to the Solana and Layer2 solutions that Visa is testing. What are the advantages over these?
[/quote]

What is the question exactly, you mean what are the advantages of HPL over what Visa is doing? Do you have a link to Visa's projects?

[quote="hokosugi, post:48, topic:23951"]
What is the use case for HPL? Are you thinking of adapting it to the enterprise or building a huge ledger system including ckBTC, ckETH, and other ICRC wrapped in HPL?
[/quote]

The latter. One place where all other tokens can be wrapped plus native ones can live and where they can be atomically swapped. And one place, when integrated into a wallet, hardware wallet, or exchange or other service, then all tokens are immediately integrated as well.

[quote="hokosugi, post:48, topic:23951"]
Also, is it a technical limitation that you can’t transfer money to/from other accounts only through virtual accounts? Or is it to solve the disadvantage of not being able to create a common account by creating a separate Principal for Internet Identity authentication?
[/quote]

It is not a technical limitation, it is a deliberate one. And it has nothing to do with II. I have plans for direct transfers as well without virtual accounts.

-------------------------

JaMarco | 2023-11-02 12:26:43 UTC | #52

[quote="timo, post:51, topic:23951"]
What is the question exactly, you mean what are the advantages of HPL over what Visa is doing? Do you have a link to Visa’s projects?
[/quote]

This twitter thread I think explains https://twitter.com/cuysheffield/status/1699031109080945049

-------------------------

Sal_Paradise | 2023-11-02 13:22:38 UTC | #54

im still quite confused about VISA. what are they seeing in Solana that we dont?

I remember reading @victorshoup's thesis on Solana's consensus mechanism. proof of history. its fundamentally flawed, apparently. Was he wrong?

Is this the issue?

https://twitter.com/dominic_w/status/1719780910982172900?s=20

-------------------------

Sal_Paradise | 2023-11-02 16:10:10 UTC | #55

![image|690x154](upload://jDIWMNJgWmjzz6ExNylJV1RWu8P.png)

https://www.shoup.net/papers/poh.pdf

-------------------------

JaMarco | 2023-11-03 00:10:24 UTC | #56

[quote="Sal_Paradise, post:54, topic:23951"]
im still quite confused about VISA. what are they seeing in Solana that we dont?
[/quote]

One big thing is Solana has USDC, which is what Visa is using to settle transactions in.

-------------------------

timo | 2023-11-03 09:22:29 UTC | #57

We have improved the dashboard on http://dashboard.hpl.live/. It now allows for a detailed analysis of latency which looks like this:

![Screenshot 2023-11-03 at 09.56.43|515x500](upload://whDmkyPQKlKuJwoKh93sMJ66A5C.png)

What is measured here is end-to-end time for making a transfer, i.e. the time from initiating the transfer to receipt of the result, both measured in the frontend. The highest level of detail is given by the heatmap in which even the outliers can be seen. We also show the average and various percentiles (50%, 75%, 90%, 95%). All three together should give quite a complete picture.

We have selected 5 aggregators that we will continue to work with. The dashboard shows a combined number for all of them, i.e. what we see is essentially an average over all aggregators. But we also have a couple of graphs for each individual aggregator including average latency and heatmap (not percentiles). Average latency should be enough to compare the aggregators against each other and the individual heatmaps allow us to see any concentration of outliers which usually point to a communication problem of the subnet hosting the aggregator.

Finally, for comparison, we also measure "ping time" which is a direct update call to each of the canisters (aggregators and ledger). This is the latency that we would experience if the HPL did not use xnet hops, e.g. if it was a standard ICRC1 canister. We see that average ping time is usually around 3.2 seconds whereas average HPL latency is usually around 6 seconds when there is no load on the system.

-------------------------

rumenov | 2023-11-03 12:04:30 UTC | #58

[quote="timo, post:57, topic:23951"]
What is measured here is end-to-end time for making a transfer, i.e. the time from initiating the transfer to receipt of the result, both measured in the frontend
[/quote]

I assume you are using either `agent-rs` or `agent-js`. Both agents continuously poll the state in order to return the result to the user.

In the coming weeks we will propose an addition to the HTTPS interface, that will include something like a synchronous call endpoint. The semantics will be that the user sends a update request and the HTTPS requests returns when the request has been executed by the execution layer, so no polling required.

This low hanging fruit will automatically shave off ~1 sec of the e2e latency.

-------------------------

timo | 2023-11-03 13:24:25 UTC | #59

[quote="rumenov, post:58, topic:23951"]
I assume you are using either `agent-rs` or `agent-js`.
[/quote]

No, we are using a special HPL client library which in turn uses components from agent-js but it's not just a wrapper around it. We submit the transfer to the aggregator and then "poll" the ledger but with query calls. So the polling that you mention isn't happening here or if it is then it is not in the critical path.

But we would still like to use the new feature in the HPL client library when it's available. Even if it won't give us end-to-end gains. Thanks for pointing it out!

EDIT: The supplemental "ping time" test uses agent-js. We have to adjust the polling interval to make for a fairer comparison.

-------------------------

timo | 2023-11-05 08:39:18 UTC | #60

[quote="NS01, post:23, topic:23951"]
I think a flat fee (adaptive if needed) based on cycle costs and small % profit would be far better than a fee based on tx value. It seems a bit like income tax and nobody likes that!
[/quote]

In the context of a multi-token ledger, if the flat fee is to be fair (i.e. equal) across tokens then it requires knowledge of value or exchange rate of all tokens. We would need to know at least the exchange rate of each token to one common token (say for example ICP). Then we can set a constant fee denominated in the common token and convert it to flat fees denominated in each and all of the other tokens. And the flat fees indeed adapt, as you suggested, as the exchange rate moves.

For the time being, without having those mechanisms available, the percentage fee based on tx value is the easiest way to have any kind of fee at all that is fair across tokens.

The next easiest will be to denominate the fee in a dedicated fee token regardless of which token is being transacted. But it has the drawback that users need to hold the fee token.

After that in terms of complexity comes a flat fee denominated in the transacted asset based on knowledge of exchange rates.

-------------------------

memetics | 2023-11-05 09:28:11 UTC | #61

Any way to early adopt HPL?

-------------------------

timo | 2023-11-05 10:04:35 UTC | #62

If you are asking about funding, investment, buying tokens etc then no, nothing like that is planned. This is a research project that can turn into a grassroots project if a community forms around it. The source code will be published under Apache 2.0. There is no entity that takes funding/investment or issues tokens.

If the HPL ever becomes a reality then I believe that a dedicated fee token makes sense that people can hold and use to pay fee. Besides that, another idea for an HPL-related token is to issue reward tokens to the users who pay fees proportional to the amount they paid. The reward token could then be used in governance or it could earn future fees if fees accumulate in excess of cycle cost. Anyway, as a reward token it would be held by users, not investors.

-------------------------

memetics | 2023-11-05 10:15:26 UTC | #63

Oh. :'( damn it kinda disappointed

-------------------------

hokosugi | 2023-11-05 22:41:35 UTC | #64

I understand that this is a research project, but how about making the native Token of HPL Cycle or Stablecoin? By making gas and fees paid in Cycle or Stablecoin, we can more closely resemble a real world ledger system.
Tokens can be used as rewards or points for each project or enterprise.
As web3, there is nothing innovative or novel, but surprisingly, it seems to me that you are looking for certainty, including scale as a ledger.

-------------------------

timo | 2023-11-06 07:41:17 UTC | #65

[quote="hokosugi, post:64, topic:23951"]
By making gas and fees paid in Cycle or Stablecoin, we can more closely resemble a real world ledger system
[/quote]

Yes, that would be desirable.

Some thoughts: The ledger may accrue excess fees over time that aren't entirely spent on cycles for its own operation. Do you really want such funds to be stored in cycles? There are already too many cycles floating around, that's why the price for cycles on exchanges is 20% of what it should be. I wouldn't like to add to this problem by charging fees in cycles.

Re. stablecoin there currently aren't any on the IC. When there are any then which one do you pick? We don't like to pick a "winner" here.

In conclusion, maybe charging fees in ICP is largest common denominator here.

-------------------------

hokosugi | 2023-11-07 05:55:38 UTC | #66

I understand about stablecoin.

Is it really a problem that XTC is priced at a 70-80% discount to Cycle, and as demand for Cycle increases, won't it go back to a pegged price between Cycle and XDR? Isn't this almost the same phenomenon as when the number of transactions in a chain drops, the native tokens as a gas fee goes from a corresponding decrease in importance to a decrease in price?

I'm not sure, but I've never seen any other chain offer a fee that matches the value scale of the real world, so I feel it would be advantageous in resolving legal issues, etc.

-------------------------

Maxfinity | 2023-11-11 00:27:36 UTC | #67

[quote="rumenov, post:40, topic:23951"]
Currently you were hitting the rate limiter on the boundary nodes. Each boundary node allows only 300 ingress messages per second. There is no reason for such conservative limit given the MB/s is actually what matters. Looking at your workload your ingress messages are fairly small. We will increase the limit to 1000 ingress messages per second in the coming days and I will ping the thread when this is done.
[/quote]

Is this on all subnets, that will also make Bitfinity capable of handling close to 1000 TPS without batching.

-------------------------

Forreal | 2023-11-11 01:46:57 UTC | #68

How TPS can Bitfinity do in one subnet?

-------------------------

Maxfinity | 2023-11-11 05:04:29 UTC | #69

if we batch txs about 750 TPS.

-------------------------

timo | 2023-11-11 06:18:00 UTC | #70

What’s the bottleneck here, limiting it to about 750, with batching?

-------------------------

Maxfinity | 2023-11-11 06:20:18 UTC | #71

Merklizing the state roots. Without that like 1500 TPS.

-------------------------

hokosugi | 2023-11-12 02:11:58 UTC | #72

[quote="timo, post:14, topic:23951"]
But as you say, sharing the ledger would also be possible. I just don’t see the need at the moment that would justify the added complexity.
[/quote]

I understand from your explanation that the throughput is such that sharding is not necessary, but I think it is important to be able to do it to differentiate yourself since sharding is an area that each chain is competing to develop. At the very least, it would have a significant marketing impact. 
My understanding is that there is Sharding for multiple Ledger Canisters and Sharding for multiple Archive and Index Canisters. In the former case, would transactions across Ledger Canisters be asynchronous processing? Also, in the latter case, once it starts running, will it run out of capacity and have to be split and stored in Canisters or Subnets. Is there any way to counter these issues?

-------------------------

timo | 2023-11-12 13:43:35 UTC | #73

[quote="hokosugi, post:72, topic:23951"]
In the former case, would transactions across Ledger Canisters be asynchronous processing?
[/quote]

Yes. 

[quote="hokosugi, post:72, topic:23951"]
Also, in the latter case, once it starts running, will it run out of capacity and have to be split and stored in Canisters or Subnets.
[/quote]

The archive (and index) should indeed be designed in a sharded way from the beginning. It will be split across multiple canisters and subnets. If enough subnets are available then it would not run out of capacity. But it would certainly be desirable if cheaper solutions come up in the future such as specific "storage subnets" that don't hold everything on SSD.

[quote="hokosugi, post:72, topic:23951"]
Is there any way to counter these issues?
[/quote]

Which issue exactly do you mean?

[quote="hokosugi, post:72, topic:23951"]
At the very least, it would have a significant marketing impact.
[/quote]

I am not so sure about that. How much has the IC benefitted from being sharded in terms of marketing? I would say 0. If at all it has made it harder to sell because people just used the sharded aspect to doubt the IC's security or degree of decentralization. The IC would have benefitted more from launching earlier but not sharded. In retrospect the market for vertical scaling proved to be very big judging by the chains that captured it. And the market for sharded chains proved to be very small.

For HPL, I think the aggregator interface, which is what end users use, does not need to change when the backend migrates from a non-sharded to a sharded architecture. Therefore, I still see no need to invest time into sharding right now. It would be good to first see how the HPL is even being used and then based on that information look into sharded designs. For example, if the majority of transactions are simple transfers from A to B then that is easier to shard because it doesn't require locking (only refunds if a transfer fails). Atomic swaps are harder in the sharded setting because they may require locking. But we don't know if the more general types of transaction will actually be used to a significant degree.

Also, sharding will introduce additional latency in the transactions. Let's first see if there is any adoption at the current latency before exposing users to even higher latency.

-------------------------

hokosugi | 2023-11-12 21:03:46 UTC | #74

[quote="timo, post:73, topic:23951"]
Which issue exactly do you mean?
[/quote]

We do not know the deeper technical details. But I am wondering, if you save archives with multiple Canisters & Subnets, doesn't the lack of proof attached to Query calls and the fact that Composite queries are not yet possible between Subnets cause delays and make it difficult to use? That's what I was worried about and thought it was an "issue".

As for Sharding, I understand. The market wants vertical scale rather than horizontal scale right now. But that doesn't mean we should ignore horizontal scale, and I hope that the design will take into account that enterprises that will need horizontal scale will enter the market in the future.

-------------------------

timo | 2023-11-17 14:01:30 UTC | #75

We are slowly releasing the HPL source code over time as the different components mature. A couple of Motoko libraries published on mops were actually part of the HPL project. In particular all of these:

* [enumeration](https://mops.one/enumeration) is the data structure that maps user's principals to internal numerical user ids. It is based on a modified RBTree. 
* [vector](https://mops.one/search/vector) is the data structure that holds all user accounts. Looking up a subaccount or virtual account, if the internal user id is known, happens in constant time.
* [swb](https://mops.one/swb) is the data structure that is used in the sliding window buffer protocol for queuing and batching in the aggregator.

Today we are publishing [promtracker](https://mops.one/search/promtracker) which is a library used to collect metrics during canister operation, aggregate and process them, and export them in prometheus-compatible format through an http endpoint. There they can be scraped and written to a prometheus database, for example by the light-weight grafana-agent. We use this to produce all the graphs on the [dashboard](http://dashboard.hpl.live/). For example, the library automatically captures high and low watermarks over a configurable interval as well as histograms over time which can be used to generate heatmaps. 

For example heatmaps on Grafana look like this:
![Screenshot 2023-11-17 at 14.59.26|690x388](upload://5tSzA1lb2AD8qbnvclUp9TD9yyO.png)

-------------------------

hokosugi | 2024-03-08 08:41:42 UTC | #76

Regarding Link, from the R&D demo it looked like it would prevent misdirection. If a link is created with a fictitious address/account or a copy and paste error, wouldn't it be possible for the sender to pull the link if the recipient doesn't exist or doesn't want to touch a link they don't remember? If so, this would be a solution to the blockchain challenge of irreversible transmission.

-------------------------

timo | 2024-03-08 09:21:10 UTC | #77

Correct. A link is personal for you, that means if you can push to it then, by definition, you also have access rights to pull the funds back from it. That is, of course, unless the receiver actively reduces the link's balance (i.e. "accepts" the funds). The key here is that the receiver must be active _after_ your push. This prevents loss of funds in various scenarios, mainly the scenario where the receiver has lost his keys after setting up the link or even after you pushed the funds, or where the receiver has disappeared (e.g. shut down his service). 

Some of the other things you mentioned are indeed prevented as well, but already earlier, before it even comes to any transfer. For example:

- a link with a fictitious account cannot be created in the first place,
- when you make a copy-paste error then the resulting link code is unlikely to 1) exist, 2) have access for you _and_ 3) match the asset id that you are trying to push it,
- the receiver cannot forget a link because a principal can always query the ledger for all its links

HPL tries to prevent loss of funds also in other ways, not only ones related to transfers. For example, it is not possible to forget your "subaccount ids" or to forget which assets you held. A principal can always query the ledger for all its accounts in all assets.

-------------------------

hokosugi | 2024-03-08 10:30:26 UTC | #78

Do you have any concrete plans for utilizing HPL as a ledger in the future? For example, we would like to see ckBTC selected as the reserve currency, forming an ecosystem that includes BTC-related tokens, etc., or partnering with credit companies in stablecoin.

-------------------------

integral_wizard | 2024-04-09 08:30:07 UTC | #79

Hey @timo . How is it going? Are you able to make progress and work on this? I recently daydreamed about the HPL :grinning: With the Solana  transaction failures at 1k TPS, it would be sweet to showcase that you can build a better version of Solana tech on ICP. 10k TPS for a blockchain is getting close or surpassing the web2 equivalent payment rails. So it would be a big deal and great marketing for ICP.

-------------------------

timo | 2024-04-15 08:16:23 UTC | #80

The HPL is usable. It has been presented in the February 2024 Global R&D as a demo. The wallet is accessible at https://wallet.hpl.live. However, the deployed version is still a test ledger. It can be wiped at any point in time. For the real, permanent ledger two things are missing: 1) governance, 2) separation of "owners" from principals.

1) When the permanent ledger is deployed then it is unclear who manages updates and how. That is an open question. Some form of governance has to be developed.

2) Currently, you cannot "rotate" your keys, i.e. all your accounts and virtual accounts are bound to the one principal that you used to create them. Historically, in other ledgers, this has been fine. If you have a Bitcoin address and want to update your keys then you move your funds to a new Bitcoin address. But it's not so easy in a multi-token ledger anymore. If you are holding many different assets, have set up many subaccounts for different purposes, and on top of that have created many virtual accounts (allowances) for use with various services then it is not so easy to re-create the entire setup under a new principal and to move all the funds over. Therefore, we should think about an indirection from principals to owner ids where the principal can be updated without affecting the owner id.

Work on these topics has not started yet.

Re. 2) If you use a wallet frontend hosted on a particular URL and you log in with internet identity your principal is bound to that specific frontend URL. You cannot take that principal with you to a different frontend hosted at a different URL. If you would like to switch to a different frontend or if you are forced to because the old one has disappeared then you are out of luck. That is one of the biggest reasons that an indirection is required. The problem is specific to internet identity and related login methods such as NFID. If you use SiWE or paste a mnemonic phrase directly then you can take your principal with you. So it is not entirely clear that the ledger has to solve this problem vs II has to solve this problem. But currently I am thinking that enabling key rotation has many benefits that it would be worth doing.

-------------------------

icarus | 2024-04-15 10:27:24 UTC | #81

Re 2. If the principal used to identify the owner of HPL account belongs to a canister then the principal used to control the canister can be changed; wouldn't this provide the necessary indirection where required or desired? 
At the expense of deploying and running such an HPL-account-controller canister per user.

-------------------------

hokosugi | 2024-04-15 10:31:18 UTC | #82

You still haven't solved the problem of the capacity to store the Archive, have you?

-------------------------

timo | 2024-04-15 10:31:33 UTC | #83

Yes, that is correct and in principle I like the canister-per-user concept. At least for "power users". For the average user I think it increases onboarding friction, maintenance burden, costs and latency. I am looking for something more lightweight.

-------------------------

timo | 2024-04-15 10:39:13 UTC | #84

[quote="hokosugi, post:82, topic:23951, full:true"]
You still haven’t solved the problem of the capacity to store the Archive, have you?
[/quote]

For the archive we have an on-chain archive (completed, in testing) to store the history of all successful transactions. This should be fine as a minimal solution sufficient for launching a real ledger. It is publicly accessible and would serve as a backup in case the ledger gets bricked. We can have a mix of fees and rate limits in the ledger to make sure that the archive does not get too large (say does not occupy more than one subnet). That way we can push out off-chain archives to be a future task. 

An archive is not an index. It does not provide search by various keys. But I think it's ok to launch a ledger without an index/explorer as long as the fulll history can be read chronologically.

-------------------------

Forreal | 2024-08-03 12:07:50 UTC | #85

Hi @timo and the research team,

I'm very interested in the progress of HPL and its potential to significantly enhance the capabilities of the Internet Computer.  Are there any updates you can share regarding its development or any plans for public testing and deployment?

The prospect of 10,000 TPS is exciting, and I'm eager to see how HPL could empower new applications on the IC.

Thanks for your ongoing work!

-------------------------

timo | 2024-08-09 09:56:51 UTC | #86

There aren't any updates. HPL could be launched if we figured out the governance. Who controls the HPL ledger and who can decide on upgrades? A community has to form who can take on these things and who can do the initial deployment.

Shall we put HPL under NNS control? Then every update requires an NNS proposal.

Shall we create an SNS for controlling HPL? SNS does not seem to be designed for it. It has governance tokens, neurons with a lock up period and is designed for raising money. For HPL it is not clear what the incentive is to hold and lock up governance tokens. It is also not clear why we should raise money or how an SNS can be created without raising money.

Shall we develop a new form of governance for HPL?

-------------------------

