CatPirate | 2022-06-26 08:20:03 UTC | #1

There are 35 Subnets right now.

I saw a tweet from Dom that subnets can process up to 750 update calls per second. I assume he excluded the numbers from the NNS subnet, which does have its own privileges for being the most important Subnet of the protocol.

***Hypothetical calculations***

So assuming there are 20 Subnets that process update calls @ 750 tps/sec - we get 15,000 TPS, which is a 30% increase from Dec 2021 Stress test.

Can someone please help me with the numbers? I do realize that there are different types of Subnet, but it's not clear to a few users such as me, and the documentation is still being developed.

A brief overview would be very helpful. Thanks in advance.

-------------------------

Zane | 2022-06-26 10:11:37 UTC | #2

Where did Dom mention the 750tps number? Back in January it used to be 500.

Would be nice to know what they have done to push it to 750 and realistically speaking how high they think it might be eventually.

-------------------------

Forreal | 2022-06-26 19:13:14 UTC | #3

I have heard someone mention 750 to 1K per subnet before but I can't find the source.

-------------------------

stefan-kaestle | 2022-06-28 07:49:49 UTC | #4

The numbers I have seen in this thread are indeed close to what we have measured, either in mainnet itself or in equivalent testnets that we are running internally. 
In our latest weekly performance runs we are up to about 900 updates/s (with ~6s p90 latency for update perceived by the user in between submitting it until executed). If we stress beyond that, we see failures appearing (e.g 26% failure rate for 1200 updates/s) and higher latency (around 10s)

I wouldn't say there is one single big feature that by itself explains the improvement. I rather think that this is a culmination of many smaller improvement that we have done over time.

I wouldn't call those theoretical numbers, as we have actually measured them with real workloads. Theoretically, the number of txns per second is probably much higher and will depend a lot on the configuration, e.g. the finalization rate and the number of messages we allow in a block. Likely, for this benchmark we would ultimately be network bound, so until we saturate the network, we should be able to increase the number of updates/s. We have not measured it with different network configurations yet, though.

Note that the actual number depends a lot on things like:
 - Geo replication: The more spread out the nodes are across the globe, the more conservative we have to be in terms of block rate and message count per block etc.
 - Application: If it's a compute intensive application that we are benchmarking, the expected bottleneck would move from the network to the CPUs and the expected update rate would be lower.
- The number of nodes in the subnetwork. The more nodes, the longer it takes to reach agreement and therefore the expected update rate would be lower with increasing subnet size.
- Subnet type and configuration
- There is likely more :-)

Finally, the replica software is still mostly developed for correctness, and not for performance. With time, we will hopefully be able to push the performance further.

**EDIT**: Just to emphasize. I'm referring here to the performance of a single subnetwork. For the absolute numbers, those have to be multiplied by the number of subnetworks.

-------------------------

Zane | 2022-06-27 17:10:03 UTC | #5

That's great news! Didn't expect almost 2x improvements in such a short amount of time. 
One thing I don't understand though is how come other chains can achieve more than 1k tps despite having higher replication factor and non permissioned nodes? The average ICP subnet has 13 nodes and their specs are very high, so I assumed the TPS to be in the thousands already.

-------------------------

stefan-kaestle | 2022-06-28 07:07:15 UTC | #6

> One thing I don’t understand though is how come other chains can achieve more than 1k tps despite having higher replication factor and non permissioned nodes?

Which blockchain are you referring to and do they count read-requests as transactions? 
I'm specifically trying to avoid this word, since it's a bit contentious and different people understand different things when they talk about transactions.

The IC can currently (and again, this will improve over time) support around 900 updates/s and more than 36k queries/s *per subnet*.

So with the current 32 application subnetworks that's >28k updates/s + 1.2M queries/s.

-------------------------

Zane | 2022-06-28 08:54:31 UTC | #7

AVAX claims a 4.5k theoretical tps and registered 870tps 8 months ago, while the number is similar to the IC consider that was achieved with hundreds of nodes running on cheaper hardware, not sure how they measure it though. Their consensus model is quite interesting and seems to scale better than IC's as node count grows, that's cause nodes don't have to query all individual nodes in a subnet to reach consensus, each nodes queries a randomized subset for multiple rounds, this apparently allows very quick finality and higher replication factor.

-------------------------

stefan-kaestle | 2022-06-28 09:16:06 UTC | #8

[quote="Zane, post:7, topic:14039"]
that’s cause nodes don’t have to query all individual nodes in a subnet to reach consensus, each nodes queries a randomized subset for multiple rounds,
[/quote]

This seems to be quite similar to some of the work we are planning in the transport layer to replace the current all-to-all communication pattern.

-------------------------

Zane | 2022-06-28 09:17:55 UTC | #9

Would that be the "Next gen P2P network" mentioned in the roadmap?

-------------------------

stefan-kaestle | 2022-06-28 09:34:07 UTC | #10

Yes, I believe that would be one of the improvements (not a P2P expert, though)

-------------------------

CatPirate | 2022-06-28 11:36:34 UTC | #11

Thank You for the detailed answer.

[quote="stefan-kaestle, post:4, topic:14039"]
Geo-replication: The more spread out the nodes are across the globe, the more conservative we have to be in terms of block rate and message count per block etc.
[/quote]

I read somewhere that the research is being done with Starlink for Remote Nodes. That would be fun to see.

[quote="stefan-kaestle, post:6, topic:14039"]
with the current 32 application subnetworks that’s >28k updates/s + 1.2M queries/s
[/quote]

What about the NNS and BTC Subnet? I'm curious about their numbers

-------------------------

Sormarler | 2022-06-28 12:02:38 UTC | #12

Until I see those numbers from AVAX on a dashboard I will not trust theoretical TPS on whitepapers. Even #ICP I want to see it.

-------------------------

Zane | 2022-06-28 12:11:22 UTC | #13

Afaik the highest registered was around 1k, which is still a feat considering Avax has hundreds of nodes vs IC's average of 13 and nodes can be run on very cheap hardware.

-------------------------

stefan-kaestle | 2022-06-28 12:22:44 UTC | #14

[quote="CatPirate, post:11, topic:14039"]
What about the NNS and BTC Subnet? I’m curious about their numbers
[/quote]

We haven't measured that, but my guess would be:

* updates/s: lower proportionally to the lower finalization rate, i.e. around half of that of an application subnetwork
* query/s: higher proportionally to the higher number of nodes (so around 3x higher)

-------------------------

stefan-kaestle | 2022-06-28 12:25:56 UTC | #15

[quote="Sormarler, post:12, topic:14039, full:true"]
Until I see those numbers from AVAX on a dashboard I will not trust theoretical TPS on whitepapers. Even #ICP I want to see it.
[/quote]

If you don't trust Dashboards, you can always run your own benchmark. If you run against mainnet, you will eventually be rate-limited by the boundary nodes. 
You can also benchmark against the replica software running on your local machine (via `dfx`). You can check out https://forum.dfinity.org/t/errors-when-stress-testing-locally/13455 for example.

-------------------------

Sormarler | 2022-06-29 20:45:20 UTC | #16

I trust the dashboard. I am talking about internal testing numbers.

-------------------------

Sormarler | 2022-11-06 21:51:52 UTC | #17

Do y'all keep official records of these tests?

-------------------------

stefan-kaestle | 2022-11-07 10:33:08 UTC | #18

Hi @Sormarler, we keep some internal records of those tests, but are not planning to release them externally, as this would mean quite significant extra overhead and we don't currently have the engineering capacity for that.

-------------------------

Zane | 2022-11-16 17:27:40 UTC | #19

[quote="stefan-kaestle, post:6, topic:14039"]
around 900 updates/s
[/quote]

Is this number capped by how fast a canister can process each message or amount of updates that can happen in a single round? E.g if a message executes a function which sends 1 ICP to n canisters would that result in higher tps than n messages to send 1 ICP to one canister? 

It is my understanding execution is usually the fastest step in the IC stack, message ordering and achieving finality is what takes time due to network latency. 

Also is time to finality influenced by how much of the state has been changed since previous round? E.g Does it take longer to reach finality on 900 updates vs 0?

-------------------------

stefan-kaestle | 2022-11-17 10:54:20 UTC | #20

In the case of this benchmark, the bottleneck is most likely Consensus and we are reaching limits on the maximum number of ingress messages that can be stored in a single block.

There are also limits in the execution environment (and elsewhere). For very compute heavy canister, you can easily move the bottleneck to the execution environment, which can (currently) only execute one concurrent update call per canister. 

If you do messaging to other canisters, you could of course also hit bottlenecks there.

It entirely depends on the messages you send and the complexity of the canister code that you want to run.

Sorry if this isn't very concrete, but it really depends :slight_smile:

re finality:
I don't think the amount of state should have a large impact on finality, since more changes just make calculating checksum a bit more expensive, but that shouldn't not dominate the cost of reaching finality. I'm not an expert here, though.

-------------------------

lastmjs | 2023-02-16 22:15:25 UTC | #21

[quote="stefan-kaestle, post:6, topic:14039"]
The IC can currently (and again, this will improve over time) support around 900 updates/s and more than 36k queries/s *per subnet*.
[/quote]

How would these numbers translate to an individual canister? I'm trying to figure out the practical performance limits in queries/sec and updates/sec for individual canisters.

-------------------------

stefan-kaestle | 2023-02-17 08:43:28 UTC | #22

Hi @lastmjs,

This question is hard to answer, as it depends mostly on your canister code.

In the experiments we have been running, there is actually only *one* single canister per subnetwork. So the numbers we have reported can be achieved just with a single canister. 

The reason for this is that the bottleneck in our experiment is the consensus throughput, which doesn't depend on where ingress messages go.

Depending on your canister, the bottleneck might also be the canister code itself. There can only be one update call to a single canister at the same time. So an upper bound on the number of updates/s naturally is given by that. 

For example, if a single update call takes 10ms to execute, you will not be able to execute more than 100 updates/s on that canister (100 * 10ms = 1s). 

I just checked our internal metrics and it appears that the large majority of update calls during the last couple of days have completed within 1ms, suggesting that for many canister is should be possible to achieve close to the above mentioned 900 updates/s when the subnetwork is otherwise idle.

Hope that helps!

-------------------------

timo | 2023-02-17 09:11:25 UTC | #23

[quote="stefan-kaestle, post:22, topic:14039"]
I just checked our internal metrics and it appears that the large majority of update calls during the last couple of days have completed within 1ms
[/quote]

What is the time for a no-op update message, i.e. how long does it take just to spin up the wasm VM?

-------------------------

timo | 2023-02-17 11:23:37 UTC | #24

[quote="lastmjs, post:21, topic:14039"]
I’m trying to figure out the practical performance limits in queries/sec and updates/sec for individual canisters.
[/quote]

I have done tests on mainnet and gotten 450 tps through to a single canister. It's tricky though to submit them at this high rate, and at a constant rate and to multiple boundary nodes in parallel. Above that I saw timeout errors. But at 450 tps they all got processed, no errors. The updates were short though, maybe around 10k cycles each.

And they were anonymous calls. I couldn’t sign fast enough to make signed requests on the fly.

-------------------------

stefan-kaestle | 2023-02-17 09:30:38 UTC | #25

I cannot give you precise numbers right now, but we do see a lot of update calls with a duration of less than 100 µs on mainnet.

I know this is not exactly what you asked, but that should be an indication that system overheads are rather low.

-------------------------

stefan-kaestle | 2023-02-17 09:33:54 UTC | #26

Re creating load at a given request rate, you might want to look at:

 - https://github.com/dfinity/ic/tree/master/rs/workload_generator
 - https://github.com/dfinity/ic/tree/master/scalability

A lot of what you want to achieve should be possible out of the box. The only (somewhat) difficult part is to run against multiple boundary nodes, as you would need to use multiple workload generator and hope that they get different IP addresses for the boundary node DNS entries, which is only really the case if you deploy them on different geographic locations.

Happy to help you with concrete benchmarking requirements :slight_smile:

-------------------------

Zane | 2023-02-17 14:49:13 UTC | #28

[quote="stefan-kaestle, post:22, topic:14039"]
I just checked our internal metrics and it appears that the large majority of update calls during the last couple of days have completed within 1ms, suggesting that for many canister is should be possible to achieve close to the above mentioned 900 updates/s when the subnetwork is otherwise idle.
[/quote]

Realistically by how many orders of magnitude do you expect that number to increase? While 900 updates/s per subnet might currently be considered as performant in the blockchain world, it really isn't that much, especially taking in account the tradeoffs made to achieve it. 

If the IC truly aims to be a viable alternative to centralized cloud and eventually lead to a "blockchain singularity", it needs to provide much higher throughput and while it can already be potentially achieved by spreading the load on different subnets, scaling horizontally is limited by data dependency and cross subnet messaging latency and comes with an increased cognitive load for the devs, which is the opposite of what Dfinity initially advertised the IC would do. 

Financial exchanges process hundreds of thousands of updates/s per trading pair due to HFTs and arbitrageurs, that is more than the tps of all existing subnets combined! Am I missing something or are there improvements planned which will substantially increase throughput?

-------------------------

stefan-kaestle | 2023-02-17 15:25:57 UTC | #29

One obvious thing to do is to have subnets that are more localized. 

The rate of ingress messages that can be processed largely depends on how fast we can create blocks and that in turn depends on the geographic distance between nodes (i.e. latency). That's because we need to give nodes enough time (multiple round trips) until we can be sure all nodes have seen all artifacts that go in the block.

If we would spin up subnets that are say in the US only, or Europe only, we could probably with the current technology already configure a much higher block rate, since less time is required until all nodes have seen all artifacts.

The IC way of doing consensus is not really that much slower theoretically vs "web2" (3f + 1 instead of 2f +1), but our system is much more decentralized.

Those this make sense?

-------------------------

lastmjs | 2023-02-17 17:10:45 UTC | #30

Very helpful thank you!

-------------------------

lastmjs | 2023-02-17 17:12:24 UTC | #31

Where are these results published?

-------------------------

Zane | 2023-02-17 17:19:51 UTC | #32

But wouldn't that partially defeat the purpose of deterministic decentralization and reduce the "sovereignty" of those subnets? If they are hosted on nodes in the same jurisdiction that would negatively impact the degree of decentralization of the dApps on those subnets, even more so than the low node count. 
This would be rather problematic for financial exchanges, on top of it there would need to be sync systems between regional shards or users in far away countries will have increased latency resulting in a worse UX. In terms of updates what are the potential gains such a solution could bring? 

All in all I'm not a huge fan of this solution, it seems to compromise even more on the initial vision proposed by Dfinity for performance gains which might not even bring it significantly closer to our end goal.

-------------------------

stefan-kaestle | 2023-02-20 07:49:24 UTC | #33

> Where are these results published?

We don't publish our weekly performance numbers yet, but are planning to do so once we fine some time to brush up the dashboards in a way that makes sense externally. A lot of the benchmarks we are running are quite complex, and while their results make sense to the team internally (who developed them) a lot of extra explanation needs to be added so that they make sense without that context for external consumption.

-------------------------

stefan-kaestle | 2023-02-20 07:54:29 UTC | #34

[quote="Zane, post:32, topic:14039, full:true"]
But wouldn’t that partially defeat the purpose of deterministic decentralization and reduce the “sovereignty” of those subnets? If they are hosted on nodes in the same jurisdiction that would negatively impact the degree of decentralization of the dApps on those subnets, even more so than the low node count.
This would be rather problematic for financial exchanges, on top of it there would need to be sync systems between regional shards or users in far away countries will have increased latency resulting in a worse UX. In terms of updates what are the potential gains such a solution could bring?

All in all I’m not a huge fan of this solution, it seems to compromise even more on the initial vision proposed by Dfinity for performance gains which might not even bring it significantly closer to our end goal.
[/quote]

Yes, such subnets would definitely be less decentralized and you would not want to use them for financial transactions.
They might make sense for a lot of other applications though.

My point was, that in general, if you want to get close to the speed of web2, you can do so relatively straight forward on the IC as well. However, you will loose some (but not all) of the benefits of the IC. Also, that subnet will still be less centralized than a web2 cloud provider, since close-by data center could still be operated by different entities.

There are probably also optimizations we can do in the protocol to increase the throughput of ingress messages without sacrificing decentralization, but those are less straight forward and therefore likely a little bit further in the future.

-------------------------

